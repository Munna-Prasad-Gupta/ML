{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2086655,"sourceType":"datasetVersion","datasetId":1251121}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:39:04.227376Z","iopub.execute_input":"2024-11-19T06:39:04.228609Z","iopub.status.idle":"2024-11-19T06:39:05.518569Z","shell.execute_reply.started":"2024-11-19T06:39:04.228552Z","shell.execute_reply":"2024-11-19T06:39:05.517577Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/hinditoeng/train.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install evaluate\nimport re\nimport warnings\nimport os\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    TrainerCallback,\n)\nfrom datasets import Dataset\nimport evaluate\nimport torch\nimport pandas as pd\nimport numpy as np\nimport json\n\n# Disable WANDB\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device.type == \"cuda\":\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"GPU not available. Using CPU instead.\")\n\n# Load dataset\nFILENAME = \"/kaggle/input/hinditoeng/train.csv\"  # Replace with your dataset file name\ntranslation_data = pd.read_csv(FILENAME)\ntranslation_data.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:43:41.841742Z","iopub.execute_input":"2024-11-19T06:43:41.842905Z","iopub.status.idle":"2024-11-19T06:43:52.757102Z","shell.execute_reply.started":"2024-11-19T06:43:41.842845Z","shell.execute_reply":"2024-11-19T06:43:52.755842Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nGPU not available. Using CPU instead.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Index(['Unnamed: 0', 'hindi', 'english'], dtype='object')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"!pip install evaluate\n!pip install transformers datasets evaluate torch\n!pip install sacrebleu\nimport re\nimport warnings\nimport os\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    TrainerCallback,\n)\nfrom datasets import Dataset\nimport evaluate\nimport torch\nimport pandas as pd\nimport numpy as np\nimport json\n\n# Disable WANDB\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device.type == \"cuda\":\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"GPU not available. Using CPU instead.\")\n\n# Load dataset\nFILENAME = \"/kaggle/input/hinditoeng/train.csv\"  # Replace with your dataset file name\ntranslation_data = pd.read_csv(FILENAME)\n\n# Rename columns for consistency\ntranslation_data.rename(columns={\"english_sentence\": \"english\", \"hindi_sentence\": \"hindi\"}, inplace=True)\n\n# Preprocessing function\ndef preprocess_text(text):\n    if not isinstance(text, str):\n        text = str(text)\n    text = re.sub(r\"[^a-zA-Z\\u0900-\\u097F\\s]\", \"\", text)  # Keep English and Hindi characters\n    return text.strip()\n\n# Preprocess text\ntranslation_data[\"english\"] = translation_data[\"english\"].apply(preprocess_text)\ntranslation_data[\"hindi\"] = translation_data[\"hindi\"].apply(preprocess_text)\n\n# Split dataset into train and test\ntrain_size = 0.8\ntrain_data = translation_data.sample(frac=train_size, random_state=42)\ntest_data = translation_data.drop(train_data.index)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_data)\ntest_dataset = Dataset.from_pandas(test_data)\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\").to(device)\n\n# Tokenize function\ndef tokenize_function(examples):\n    model_inputs = tokenizer(\n        examples[\"english\"], max_length=64, truncation=True, padding=\"max_length\"\n    )\n    labels = tokenizer(\n        examples[\"hindi\"], max_length=64, truncation=True, padding=\"max_length\"\n    ).input_ids\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Tokenize datasets\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Set dataset format for PyTorch\ntrain_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntest_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results_english_hindi\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,  # Increased for GPU\n    per_device_eval_batch_size=16,  # Increased for GPU\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=10,\n    predict_with_generate=True,\n    logging_dir=\"./logs_english_hindi\",\n    logging_steps=10,\n    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch size\n    fp16=True,  # Enable mixed precision\n    dataloader_num_workers=2,\n    remove_unused_columns=False,\n    save_strategy=\"epoch\",\n)\n\n# Evaluation metric (BLEU)\nmetric = evaluate.load(\"sacrebleu\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    decoded_labels = [[label] for label in decoded_labels]  # BLEU expects nested lists\n    bleu_result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    accuracy = np.mean(\n        [pred.strip() == ref[0].strip() for pred, ref in zip(decoded_preds, decoded_labels)]\n    )\n    return {\"bleu\": bleu_result[\"score\"], \"accuracy\": accuracy}\n\n# Custom logging callback\nclass CustomTrainerCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs:\n            print(f\"Epoch: {state.epoch}, Logs: {logs}\")\n\n# Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[CustomTrainerCallback()],\n)\n\n# Train the model\nprint(\"Training the model on the English-Hindi dataset...\")\ntrain_result = trainer.train()\n\n# Save training history\nhistory_file = \"./results_english_hindi/training_history.json\"\nwith open(history_file, \"w\") as f:\n    json.dump(train_result.metrics, f)\nprint(f\"Training history saved to {history_file}\")\n\n# Save the model\nmodel.save_pretrained(\"./results_english_hindi/model\")\ntokenizer.save_pretrained(\"./results_english_hindi/tokenizer\")\nprint(\"Model and tokenizer saved to './results_english_hindi/'\")\n\n# Evaluate the model\nresults = trainer.evaluate()\nprint(\"Evaluation Results:\", results)\n\n# Save evaluation results\nevaluation_file = \"./results_english_hindi/evaluation_results.json\"\nwith open(evaluation_file, \"w\") as f:\n    json.dump(results, f)\nprint(f\"Evaluation results saved to {evaluation_file}\")\n\n# Translate function\ndef translate_english_to_hindi(english_sentence):\n    inputs = tokenizer(\n        english_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=64\n    ).to(device)\n    outputs = model.generate(inputs.input_ids, max_length=128, num_beams=4, early_stopping=True)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Example translation\nenglish_input = \"This is a test sentence for translation.\"\nhindi_translation = translate_english_to_hindi(english_input)\nprint(\"English:\", english_input)\nprint(\"Hindi:\", hindi_translation)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:48:27.623305Z","iopub.execute_input":"2024-11-19T06:48:27.623726Z","iopub.status.idle":"2024-11-19T09:16:37.174366Z","shell.execute_reply.started":"2024-11-19T06:48:27.623674Z","shell.execute_reply":"2024-11-19T09:16:37.173355Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.0.0 sacrebleu-2.4.3\nUsing GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6cf373ecb394e44828cf20262228734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e72f3ad8e9a4be89fb3dbd2173c0992"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f77e87b6e8740bc9be0533e72a24bd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c83528069e442aba3f1ff0bbd17eac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38ec21c39d8c4ee586047325b94f20d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"143af825ec154ce9994b6daad8bf6671"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/81858 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8104c821547454ca9c894d39a1e37eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20464 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3f7d150c1fb421f813e4599222c4d66"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab825472e6f9435cba1cd0a41b2f5387"}},"metadata":{}},{"name":"stdout","text":"Training the model on the English-Hindi dataset...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25580' max='25580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25580/25580 2:21:18, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.050200</td>\n      <td>0.046635</td>\n      <td>0.000000</td>\n      <td>0.943413</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.041700</td>\n      <td>0.044428</td>\n      <td>0.000000</td>\n      <td>0.943999</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.047300</td>\n      <td>0.042740</td>\n      <td>0.000003</td>\n      <td>0.944830</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.049200</td>\n      <td>0.041021</td>\n      <td>0.000000</td>\n      <td>0.945563</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.041700</td>\n      <td>0.041171</td>\n      <td>0.000000</td>\n      <td>0.945612</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.038300</td>\n      <td>0.041471</td>\n      <td>0.000000</td>\n      <td>0.945758</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0.003908540160250147, Logs: {'loss': 9.6568, 'grad_norm': 32.969661712646484, 'learning_rate': 1.999374511336982e-05, 'epoch': 0.003908540160250147}\nEpoch: 0.007817080320500294, Logs: {'loss': 8.266, 'grad_norm': 32.89570236206055, 'learning_rate': 1.998670836591087e-05, 'epoch': 0.007817080320500294}\nEpoch: 0.01172562048075044, Logs: {'loss': 6.7904, 'grad_norm': 91.50592041015625, 'learning_rate': 1.9979671618451916e-05, 'epoch': 0.01172562048075044}\nEpoch: 0.015634160641000587, Logs: {'loss': 5.8871, 'grad_norm': 39.18257141113281, 'learning_rate': 1.997185301016419e-05, 'epoch': 0.015634160641000587}\nEpoch: 0.019542700801250732, Logs: {'loss': 4.6761, 'grad_norm': 37.833370208740234, 'learning_rate': 1.996403440187647e-05, 'epoch': 0.019542700801250732}\nEpoch: 0.02345124096150088, Logs: {'loss': 3.8901, 'grad_norm': 70.52739715576172, 'learning_rate': 1.995621579358874e-05, 'epoch': 0.02345124096150088}\nEpoch: 0.027359781121751026, Logs: {'loss': 3.2961, 'grad_norm': 35.20824432373047, 'learning_rate': 1.9949179046129788e-05, 'epoch': 0.027359781121751026}\nEpoch: 0.031268321282001174, Logs: {'loss': 2.7611, 'grad_norm': 25.243911743164062, 'learning_rate': 1.9941360437842067e-05, 'epoch': 0.031268321282001174}\nEpoch: 0.035176861442251316, Logs: {'loss': 1.9311, 'grad_norm': 35.46476745605469, 'learning_rate': 1.993354182955434e-05, 'epoch': 0.035176861442251316}\nEpoch: 0.039085401602501464, Logs: {'loss': 1.6941, 'grad_norm': 14.501616477966309, 'learning_rate': 1.9925723221266616e-05, 'epoch': 0.039085401602501464}\nEpoch: 0.04299394176275161, Logs: {'loss': 1.4257, 'grad_norm': 3.787623405456543, 'learning_rate': 1.991790461297889e-05, 'epoch': 0.04299394176275161}\nEpoch: 0.04690248192300176, Logs: {'loss': 1.1489, 'grad_norm': 4.579604148864746, 'learning_rate': 1.9910086004691166e-05, 'epoch': 0.04690248192300176}\nEpoch: 0.0508110220832519, Logs: {'loss': 1.0843, 'grad_norm': 7.18254280090332, 'learning_rate': 1.990226739640344e-05, 'epoch': 0.0508110220832519}\nEpoch: 0.05471956224350205, Logs: {'loss': 0.8361, 'grad_norm': 7.486725330352783, 'learning_rate': 1.9894448788115716e-05, 'epoch': 0.05471956224350205}\nEpoch: 0.0586281024037522, Logs: {'loss': 0.7937, 'grad_norm': 2.80198335647583, 'learning_rate': 1.9886630179827994e-05, 'epoch': 0.0586281024037522}\nEpoch: 0.06253664256400235, Logs: {'loss': 0.6745, 'grad_norm': 3.4340977668762207, 'learning_rate': 1.987881157154027e-05, 'epoch': 0.06253664256400235}\nEpoch: 0.0664451827242525, Logs: {'loss': 0.5253, 'grad_norm': 2.8228201866149902, 'learning_rate': 1.9870992963252544e-05, 'epoch': 0.0664451827242525}\nEpoch: 0.07035372288450263, Logs: {'loss': 0.4709, 'grad_norm': 3.5799643993377686, 'learning_rate': 1.986317435496482e-05, 'epoch': 0.07035372288450263}\nEpoch: 0.07426226304475278, Logs: {'loss': 0.3978, 'grad_norm': 1.8038164377212524, 'learning_rate': 1.9855355746677093e-05, 'epoch': 0.07426226304475278}\nEpoch: 0.07817080320500293, Logs: {'loss': 0.3356, 'grad_norm': 2.271247148513794, 'learning_rate': 1.9847537138389368e-05, 'epoch': 0.07817080320500293}\nEpoch: 0.08207934336525308, Logs: {'loss': 0.3072, 'grad_norm': 1.2152526378631592, 'learning_rate': 1.9839718530101643e-05, 'epoch': 0.08207934336525308}\nEpoch: 0.08598788352550323, Logs: {'loss': 0.275, 'grad_norm': 0.9844598174095154, 'learning_rate': 1.9831899921813918e-05, 'epoch': 0.08598788352550323}\nEpoch: 0.08989642368575337, Logs: {'loss': 0.2436, 'grad_norm': 98.90372467041016, 'learning_rate': 1.9824081313526193e-05, 'epoch': 0.08989642368575337}\nEpoch: 0.09380496384600352, Logs: {'loss': 0.2078, 'grad_norm': 0.6499581933021545, 'learning_rate': 1.9816262705238468e-05, 'epoch': 0.09380496384600352}\nEpoch: 0.09771350400625367, Logs: {'loss': 0.1901, 'grad_norm': 0.8790692090988159, 'learning_rate': 1.9808444096950746e-05, 'epoch': 0.09771350400625367}\nEpoch: 0.1016220441665038, Logs: {'loss': 0.1792, 'grad_norm': 1.5563340187072754, 'learning_rate': 1.9800625488663017e-05, 'epoch': 0.1016220441665038}\nEpoch: 0.10553058432675395, Logs: {'loss': 0.1513, 'grad_norm': 0.8846457600593567, 'learning_rate': 1.9792806880375296e-05, 'epoch': 0.10553058432675395}\nEpoch: 0.1094391244870041, Logs: {'loss': 0.1605, 'grad_norm': 0.6821264028549194, 'learning_rate': 1.978498827208757e-05, 'epoch': 0.1094391244870041}\nEpoch: 0.11334766464725425, Logs: {'loss': 0.1359, 'grad_norm': 0.3833064138889313, 'learning_rate': 1.9777169663799845e-05, 'epoch': 0.11334766464725425}\nEpoch: 0.1172562048075044, Logs: {'loss': 0.1494, 'grad_norm': 0.4462767541408539, 'learning_rate': 1.976935105551212e-05, 'epoch': 0.1172562048075044}\nEpoch: 0.12116474496775455, Logs: {'loss': 0.1584, 'grad_norm': 0.6992499828338623, 'learning_rate': 1.9761532447224395e-05, 'epoch': 0.12116474496775455}\nEpoch: 0.1250732851280047, Logs: {'loss': 0.1348, 'grad_norm': 0.8085712790489197, 'learning_rate': 1.975371383893667e-05, 'epoch': 0.1250732851280047}\nEpoch: 0.12898182528825483, Logs: {'loss': 0.1431, 'grad_norm': 0.6736743450164795, 'learning_rate': 1.9745895230648945e-05, 'epoch': 0.12898182528825483}\nEpoch: 0.132890365448505, Logs: {'loss': 0.13, 'grad_norm': 0.5173050761222839, 'learning_rate': 1.9738076622361223e-05, 'epoch': 0.132890365448505}\nEpoch: 0.13679890560875513, Logs: {'loss': 0.1033, 'grad_norm': 0.5602893829345703, 'learning_rate': 1.9730258014073495e-05, 'epoch': 0.13679890560875513}\nEpoch: 0.14070744576900526, Logs: {'loss': 0.1046, 'grad_norm': 0.32104402780532837, 'learning_rate': 1.9722439405785773e-05, 'epoch': 0.14070744576900526}\nEpoch: 0.14461598592925543, Logs: {'loss': 0.1132, 'grad_norm': 1.1624090671539307, 'learning_rate': 1.9714620797498048e-05, 'epoch': 0.14461598592925543}\nEpoch: 0.14852452608950556, Logs: {'loss': 0.1061, 'grad_norm': 0.3324664533138275, 'learning_rate': 1.9706802189210323e-05, 'epoch': 0.14852452608950556}\nEpoch: 0.15243306624975572, Logs: {'loss': 0.1125, 'grad_norm': 0.2501479685306549, 'learning_rate': 1.9698983580922598e-05, 'epoch': 0.15243306624975572}\nEpoch: 0.15634160641000586, Logs: {'loss': 0.0976, 'grad_norm': 0.9349585175514221, 'learning_rate': 1.9691164972634872e-05, 'epoch': 0.15634160641000586}\nEpoch: 0.16025014657025602, Logs: {'loss': 0.1008, 'grad_norm': 2.1670353412628174, 'learning_rate': 1.9683346364347147e-05, 'epoch': 0.16025014657025602}\nEpoch: 0.16415868673050615, Logs: {'loss': 0.0921, 'grad_norm': 0.34099671244621277, 'learning_rate': 1.9675527756059422e-05, 'epoch': 0.16415868673050615}\nEpoch: 0.16806722689075632, Logs: {'loss': 0.0939, 'grad_norm': 0.2956390380859375, 'learning_rate': 1.9667709147771697e-05, 'epoch': 0.16806722689075632}\nEpoch: 0.17197576705100645, Logs: {'loss': 0.1126, 'grad_norm': 0.31624990701675415, 'learning_rate': 1.9659890539483975e-05, 'epoch': 0.17197576705100645}\nEpoch: 0.17588430721125659, Logs: {'loss': 0.1059, 'grad_norm': 0.34082141518592834, 'learning_rate': 1.9652071931196247e-05, 'epoch': 0.17588430721125659}\nEpoch: 0.17979284737150675, Logs: {'loss': 0.1042, 'grad_norm': 0.7013537883758545, 'learning_rate': 1.9644253322908525e-05, 'epoch': 0.17979284737150675}\nEpoch: 0.18370138753175688, Logs: {'loss': 0.0835, 'grad_norm': 0.25212979316711426, 'learning_rate': 1.96364347146208e-05, 'epoch': 0.18370138753175688}\nEpoch: 0.18760992769200704, Logs: {'loss': 0.1119, 'grad_norm': 0.3686469495296478, 'learning_rate': 1.9628616106333075e-05, 'epoch': 0.18760992769200704}\nEpoch: 0.19151846785225718, Logs: {'loss': 0.1042, 'grad_norm': 1.7194292545318604, 'learning_rate': 1.962079749804535e-05, 'epoch': 0.19151846785225718}\nEpoch: 0.19542700801250734, Logs: {'loss': 0.0902, 'grad_norm': 0.30810898542404175, 'learning_rate': 1.9612978889757624e-05, 'epoch': 0.19542700801250734}\nEpoch: 0.19933554817275748, Logs: {'loss': 0.1236, 'grad_norm': 1.3150622844696045, 'learning_rate': 1.96051602814699e-05, 'epoch': 0.19933554817275748}\nEpoch: 0.2032440883330076, Logs: {'loss': 0.0879, 'grad_norm': 0.44111254811286926, 'learning_rate': 1.9597341673182174e-05, 'epoch': 0.2032440883330076}\nEpoch: 0.20715262849325777, Logs: {'loss': 0.1246, 'grad_norm': 0.19536778330802917, 'learning_rate': 1.9589523064894452e-05, 'epoch': 0.20715262849325777}\nEpoch: 0.2110611686535079, Logs: {'loss': 0.0866, 'grad_norm': 1.5040909051895142, 'learning_rate': 1.9581704456606724e-05, 'epoch': 0.2110611686535079}\nEpoch: 0.21496970881375807, Logs: {'loss': 0.0839, 'grad_norm': 0.26162034273147583, 'learning_rate': 1.9573885848319002e-05, 'epoch': 0.21496970881375807}\nEpoch: 0.2188782489740082, Logs: {'loss': 0.0888, 'grad_norm': 0.35656073689460754, 'learning_rate': 1.9566067240031277e-05, 'epoch': 0.2188782489740082}\nEpoch: 0.22278678913425837, Logs: {'loss': 0.1022, 'grad_norm': 0.6054224967956543, 'learning_rate': 1.9558248631743552e-05, 'epoch': 0.22278678913425837}\nEpoch: 0.2266953292945085, Logs: {'loss': 0.1021, 'grad_norm': 0.28563225269317627, 'learning_rate': 1.9550430023455827e-05, 'epoch': 0.2266953292945085}\nEpoch: 0.23060386945475864, Logs: {'loss': 0.0762, 'grad_norm': 0.32988637685775757, 'learning_rate': 1.95426114151681e-05, 'epoch': 0.23060386945475864}\nEpoch: 0.2345124096150088, Logs: {'loss': 0.0763, 'grad_norm': 0.3340173363685608, 'learning_rate': 1.9534792806880376e-05, 'epoch': 0.2345124096150088}\nEpoch: 0.23842094977525893, Logs: {'loss': 0.0773, 'grad_norm': 0.15697570145130157, 'learning_rate': 1.952697419859265e-05, 'epoch': 0.23842094977525893}\nEpoch: 0.2423294899355091, Logs: {'loss': 0.0944, 'grad_norm': 0.41958823800086975, 'learning_rate': 1.9519155590304926e-05, 'epoch': 0.2423294899355091}\nEpoch: 0.24623803009575923, Logs: {'loss': 0.078, 'grad_norm': 0.22618485987186432, 'learning_rate': 1.95113369820172e-05, 'epoch': 0.24623803009575923}\nEpoch: 0.2501465702560094, Logs: {'loss': 0.0987, 'grad_norm': 0.1808711737394333, 'learning_rate': 1.9503518373729476e-05, 'epoch': 0.2501465702560094}\nEpoch: 0.2540551104162595, Logs: {'loss': 0.0708, 'grad_norm': 0.18476292490959167, 'learning_rate': 1.9495699765441754e-05, 'epoch': 0.2540551104162595}\nEpoch: 0.25796365057650966, Logs: {'loss': 0.0733, 'grad_norm': 0.37416431307792664, 'learning_rate': 1.9487881157154026e-05, 'epoch': 0.25796365057650966}\nEpoch: 0.2618721907367598, Logs: {'loss': 0.0801, 'grad_norm': 2.785356283187866, 'learning_rate': 1.9480062548866304e-05, 'epoch': 0.2618721907367598}\nEpoch: 0.26578073089701, Logs: {'loss': 0.0708, 'grad_norm': 0.26328474283218384, 'learning_rate': 1.947224394057858e-05, 'epoch': 0.26578073089701}\nEpoch: 0.2696892710572601, Logs: {'loss': 0.1078, 'grad_norm': 0.14483901858329773, 'learning_rate': 1.9464425332290854e-05, 'epoch': 0.2696892710572601}\nEpoch: 0.27359781121751026, Logs: {'loss': 0.0753, 'grad_norm': 0.18636159598827362, 'learning_rate': 1.945660672400313e-05, 'epoch': 0.27359781121751026}\nEpoch: 0.2775063513777604, Logs: {'loss': 0.0627, 'grad_norm': 0.27005043625831604, 'learning_rate': 1.9448788115715403e-05, 'epoch': 0.2775063513777604}\nEpoch: 0.2814148915380105, Logs: {'loss': 0.0725, 'grad_norm': 0.2660049498081207, 'learning_rate': 1.944096950742768e-05, 'epoch': 0.2814148915380105}\nEpoch: 0.2853234316982607, Logs: {'loss': 0.0728, 'grad_norm': 0.40372031927108765, 'learning_rate': 1.9433150899139953e-05, 'epoch': 0.2853234316982607}\nEpoch: 0.28923197185851085, Logs: {'loss': 0.0762, 'grad_norm': 0.24981893599033356, 'learning_rate': 1.942533229085223e-05, 'epoch': 0.28923197185851085}\nEpoch: 0.293140512018761, Logs: {'loss': 0.0686, 'grad_norm': 0.27124881744384766, 'learning_rate': 1.9417513682564506e-05, 'epoch': 0.293140512018761}\nEpoch: 0.2970490521790111, Logs: {'loss': 0.0764, 'grad_norm': 0.8829721212387085, 'learning_rate': 1.940969507427678e-05, 'epoch': 0.2970490521790111}\nEpoch: 0.3009575923392613, Logs: {'loss': 0.0748, 'grad_norm': 0.2131795585155487, 'learning_rate': 1.9401876465989056e-05, 'epoch': 0.3009575923392613}\nEpoch: 0.30486613249951144, Logs: {'loss': 0.0626, 'grad_norm': 0.1829233318567276, 'learning_rate': 1.939405785770133e-05, 'epoch': 0.30486613249951144}\nEpoch: 0.3087746726597616, Logs: {'loss': 0.0708, 'grad_norm': 0.1959461271762848, 'learning_rate': 1.9386239249413606e-05, 'epoch': 0.3087746726597616}\nEpoch: 0.3126832128200117, Logs: {'loss': 0.0651, 'grad_norm': 0.17548620700836182, 'learning_rate': 1.937842064112588e-05, 'epoch': 0.3126832128200117}\nEpoch: 0.3165917529802619, Logs: {'loss': 0.0657, 'grad_norm': 1.5088454484939575, 'learning_rate': 1.937060203283816e-05, 'epoch': 0.3165917529802619}\nEpoch: 0.32050029314051204, Logs: {'loss': 0.064, 'grad_norm': 0.2839696705341339, 'learning_rate': 1.936278342455043e-05, 'epoch': 0.32050029314051204}\nEpoch: 0.32440883330076214, Logs: {'loss': 0.0817, 'grad_norm': 0.17757833003997803, 'learning_rate': 1.9354964816262705e-05, 'epoch': 0.32440883330076214}\nEpoch: 0.3283173734610123, Logs: {'loss': 0.0763, 'grad_norm': 0.24643918871879578, 'learning_rate': 1.9347146207974983e-05, 'epoch': 0.3283173734610123}\nEpoch: 0.33222591362126247, Logs: {'loss': 0.0789, 'grad_norm': 0.1534067541360855, 'learning_rate': 1.9339327599687255e-05, 'epoch': 0.33222591362126247}\nEpoch: 0.33613445378151263, Logs: {'loss': 0.0664, 'grad_norm': 0.8078960180282593, 'learning_rate': 1.9331508991399533e-05, 'epoch': 0.33613445378151263}\nEpoch: 0.34004299394176274, Logs: {'loss': 0.0624, 'grad_norm': 0.2550842761993408, 'learning_rate': 1.9323690383111808e-05, 'epoch': 0.34004299394176274}\nEpoch: 0.3439515341020129, Logs: {'loss': 0.0799, 'grad_norm': 0.23685216903686523, 'learning_rate': 1.9315871774824083e-05, 'epoch': 0.3439515341020129}\nEpoch: 0.34786007426226306, Logs: {'loss': 0.0685, 'grad_norm': 2.359084367752075, 'learning_rate': 1.9308053166536358e-05, 'epoch': 0.34786007426226306}\nEpoch: 0.35176861442251317, Logs: {'loss': 0.0655, 'grad_norm': 0.22834935784339905, 'learning_rate': 1.9300234558248633e-05, 'epoch': 0.35176861442251317}\nEpoch: 0.35567715458276333, Logs: {'loss': 0.0654, 'grad_norm': 0.17481030523777008, 'learning_rate': 1.9292415949960908e-05, 'epoch': 0.35567715458276333}\nEpoch: 0.3595856947430135, Logs: {'loss': 0.0595, 'grad_norm': 0.14201904833316803, 'learning_rate': 1.9284597341673182e-05, 'epoch': 0.3595856947430135}\nEpoch: 0.36349423490326366, Logs: {'loss': 0.0631, 'grad_norm': 0.2254810929298401, 'learning_rate': 1.927677873338546e-05, 'epoch': 0.36349423490326366}\nEpoch: 0.36740277506351376, Logs: {'loss': 0.0679, 'grad_norm': 0.3568738102912903, 'learning_rate': 1.9268960125097732e-05, 'epoch': 0.36740277506351376}\nEpoch: 0.3713113152237639, Logs: {'loss': 0.0679, 'grad_norm': 0.3093762695789337, 'learning_rate': 1.926114151681001e-05, 'epoch': 0.3713113152237639}\nEpoch: 0.3752198553840141, Logs: {'loss': 0.0674, 'grad_norm': 0.18786606192588806, 'learning_rate': 1.9253322908522285e-05, 'epoch': 0.3752198553840141}\nEpoch: 0.3791283955442642, Logs: {'loss': 0.0635, 'grad_norm': 0.16949938237667084, 'learning_rate': 1.924550430023456e-05, 'epoch': 0.3791283955442642}\nEpoch: 0.38303693570451436, Logs: {'loss': 0.0722, 'grad_norm': 0.24411386251449585, 'learning_rate': 1.9237685691946835e-05, 'epoch': 0.38303693570451436}\nEpoch: 0.3869454758647645, Logs: {'loss': 0.0666, 'grad_norm': 0.4523850083351135, 'learning_rate': 1.922986708365911e-05, 'epoch': 0.3869454758647645}\nEpoch: 0.3908540160250147, Logs: {'loss': 0.0568, 'grad_norm': 0.15084053575992584, 'learning_rate': 1.9222048475371388e-05, 'epoch': 0.3908540160250147}\nEpoch: 0.3947625561852648, Logs: {'loss': 0.0587, 'grad_norm': 0.31544730067253113, 'learning_rate': 1.921422986708366e-05, 'epoch': 0.3947625561852648}\nEpoch: 0.39867109634551495, Logs: {'loss': 0.0653, 'grad_norm': 0.18861934542655945, 'learning_rate': 1.9206411258795938e-05, 'epoch': 0.39867109634551495}\nEpoch: 0.4025796365057651, Logs: {'loss': 0.0744, 'grad_norm': 0.17860525846481323, 'learning_rate': 1.9198592650508213e-05, 'epoch': 0.4025796365057651}\nEpoch: 0.4064881766660152, Logs: {'loss': 0.0573, 'grad_norm': 0.3776995837688446, 'learning_rate': 1.9190774042220484e-05, 'epoch': 0.4064881766660152}\nEpoch: 0.4103967168262654, Logs: {'loss': 0.0658, 'grad_norm': 0.22431185841560364, 'learning_rate': 1.9182955433932762e-05, 'epoch': 0.4103967168262654}\nEpoch: 0.41430525698651555, Logs: {'loss': 0.0577, 'grad_norm': 0.12757311761379242, 'learning_rate': 1.9175136825645037e-05, 'epoch': 0.41430525698651555}\nEpoch: 0.4182137971467657, Logs: {'loss': 0.0559, 'grad_norm': 0.09894388914108276, 'learning_rate': 1.9167318217357312e-05, 'epoch': 0.4182137971467657}\nEpoch: 0.4221223373070158, Logs: {'loss': 0.0565, 'grad_norm': 0.15352901816368103, 'learning_rate': 1.9159499609069587e-05, 'epoch': 0.4221223373070158}\nEpoch: 0.426030877467266, Logs: {'loss': 0.0621, 'grad_norm': 0.18124622106552124, 'learning_rate': 1.9151681000781862e-05, 'epoch': 0.426030877467266}\nEpoch: 0.42993941762751614, Logs: {'loss': 0.0575, 'grad_norm': 0.14832952618598938, 'learning_rate': 1.9143862392494137e-05, 'epoch': 0.42993941762751614}\nEpoch: 0.43384795778776625, Logs: {'loss': 0.0587, 'grad_norm': 0.5284817814826965, 'learning_rate': 1.913604378420641e-05, 'epoch': 0.43384795778776625}\nEpoch: 0.4377564979480164, Logs: {'loss': 0.0547, 'grad_norm': 0.20921921730041504, 'learning_rate': 1.912822517591869e-05, 'epoch': 0.4377564979480164}\nEpoch: 0.44166503810826657, Logs: {'loss': 0.0633, 'grad_norm': 0.11637821793556213, 'learning_rate': 1.912040656763096e-05, 'epoch': 0.44166503810826657}\nEpoch: 0.44557357826851673, Logs: {'loss': 0.0584, 'grad_norm': 0.16729168593883514, 'learning_rate': 1.911258795934324e-05, 'epoch': 0.44557357826851673}\nEpoch: 0.44948211842876684, Logs: {'loss': 0.0527, 'grad_norm': 0.1428176611661911, 'learning_rate': 1.9104769351055514e-05, 'epoch': 0.44948211842876684}\nEpoch: 0.453390658589017, Logs: {'loss': 0.0585, 'grad_norm': 0.1196904107928276, 'learning_rate': 1.909695074276779e-05, 'epoch': 0.453390658589017}\nEpoch: 0.45729919874926717, Logs: {'loss': 0.063, 'grad_norm': 0.09149256348609924, 'learning_rate': 1.9089132134480064e-05, 'epoch': 0.45729919874926717}\nEpoch: 0.4612077389095173, Logs: {'loss': 0.0649, 'grad_norm': 0.18804815411567688, 'learning_rate': 1.908131352619234e-05, 'epoch': 0.4612077389095173}\nEpoch: 0.46511627906976744, Logs: {'loss': 0.0641, 'grad_norm': 0.23899266123771667, 'learning_rate': 1.9073494917904614e-05, 'epoch': 0.46511627906976744}\nEpoch: 0.4690248192300176, Logs: {'loss': 0.0604, 'grad_norm': 0.21629683673381805, 'learning_rate': 1.906567630961689e-05, 'epoch': 0.4690248192300176}\nEpoch: 0.47293335939026776, Logs: {'loss': 0.0604, 'grad_norm': 0.11877520382404327, 'learning_rate': 1.9057857701329167e-05, 'epoch': 0.47293335939026776}\nEpoch: 0.47684189955051787, Logs: {'loss': 0.0645, 'grad_norm': 0.15847907960414886, 'learning_rate': 1.905003909304144e-05, 'epoch': 0.47684189955051787}\nEpoch: 0.48075043971076803, Logs: {'loss': 0.0529, 'grad_norm': 0.10532207041978836, 'learning_rate': 1.9042220484753717e-05, 'epoch': 0.48075043971076803}\nEpoch: 0.4846589798710182, Logs: {'loss': 0.0518, 'grad_norm': 0.5683825016021729, 'learning_rate': 1.903440187646599e-05, 'epoch': 0.4846589798710182}\nEpoch: 0.4885675200312683, Logs: {'loss': 0.0593, 'grad_norm': 0.18063543736934662, 'learning_rate': 1.9026583268178263e-05, 'epoch': 0.4885675200312683}\nEpoch: 0.49247606019151846, Logs: {'loss': 0.0654, 'grad_norm': 1.3676166534423828, 'learning_rate': 1.901876465989054e-05, 'epoch': 0.49247606019151846}\nEpoch: 0.4963846003517686, Logs: {'loss': 0.0603, 'grad_norm': 0.3384553790092468, 'learning_rate': 1.9010946051602816e-05, 'epoch': 0.4963846003517686}\nEpoch: 0.5002931405120188, Logs: {'loss': 0.0615, 'grad_norm': 0.29466086626052856, 'learning_rate': 1.900312744331509e-05, 'epoch': 0.5002931405120188}\nEpoch: 0.5042016806722689, Logs: {'loss': 0.0624, 'grad_norm': 0.21993376314640045, 'learning_rate': 1.8995308835027366e-05, 'epoch': 0.5042016806722689}\nEpoch: 0.508110220832519, Logs: {'loss': 0.0614, 'grad_norm': 0.3369949758052826, 'learning_rate': 1.898749022673964e-05, 'epoch': 0.508110220832519}\nEpoch: 0.5120187609927692, Logs: {'loss': 0.0643, 'grad_norm': 0.43596789240837097, 'learning_rate': 1.897967161845192e-05, 'epoch': 0.5120187609927692}\nEpoch: 0.5159273011530193, Logs: {'loss': 0.0655, 'grad_norm': 0.25980710983276367, 'learning_rate': 1.897185301016419e-05, 'epoch': 0.5159273011530193}\nEpoch: 0.5198358413132695, Logs: {'loss': 0.0571, 'grad_norm': 0.21225696802139282, 'learning_rate': 1.896403440187647e-05, 'epoch': 0.5198358413132695}\nEpoch: 0.5237443814735196, Logs: {'loss': 0.0537, 'grad_norm': 0.289326548576355, 'learning_rate': 1.8956215793588744e-05, 'epoch': 0.5237443814735196}\nEpoch: 0.5276529216337698, Logs: {'loss': 0.0617, 'grad_norm': 0.17096330225467682, 'learning_rate': 1.894839718530102e-05, 'epoch': 0.5276529216337698}\nEpoch: 0.53156146179402, Logs: {'loss': 0.0572, 'grad_norm': 0.25590676069259644, 'learning_rate': 1.8940578577013293e-05, 'epoch': 0.53156146179402}\nEpoch: 0.5354700019542701, Logs: {'loss': 0.0621, 'grad_norm': 0.1720997393131256, 'learning_rate': 1.8932759968725568e-05, 'epoch': 0.5354700019542701}\nEpoch: 0.5393785421145202, Logs: {'loss': 0.0584, 'grad_norm': 0.2087957113981247, 'learning_rate': 1.8924941360437843e-05, 'epoch': 0.5393785421145202}\nEpoch: 0.5432870822747704, Logs: {'loss': 0.0533, 'grad_norm': 0.393268883228302, 'learning_rate': 1.8917122752150118e-05, 'epoch': 0.5432870822747704}\nEpoch: 0.5471956224350205, Logs: {'loss': 0.0594, 'grad_norm': 0.14898236095905304, 'learning_rate': 1.8909304143862396e-05, 'epoch': 0.5471956224350205}\nEpoch: 0.5511041625952706, Logs: {'loss': 0.0513, 'grad_norm': 0.198377788066864, 'learning_rate': 1.8901485535574668e-05, 'epoch': 0.5511041625952706}\nEpoch: 0.5550127027555208, Logs: {'loss': 0.0539, 'grad_norm': 0.32111266255378723, 'learning_rate': 1.8893666927286946e-05, 'epoch': 0.5550127027555208}\nEpoch: 0.5589212429157709, Logs: {'loss': 0.0584, 'grad_norm': 0.28879907727241516, 'learning_rate': 1.888584831899922e-05, 'epoch': 0.5589212429157709}\nEpoch: 0.562829783076021, Logs: {'loss': 0.0557, 'grad_norm': 0.14481084048748016, 'learning_rate': 1.8878029710711496e-05, 'epoch': 0.562829783076021}\nEpoch: 0.5667383232362713, Logs: {'loss': 0.0521, 'grad_norm': 0.21258926391601562, 'learning_rate': 1.887021110242377e-05, 'epoch': 0.5667383232362713}\nEpoch: 0.5706468633965214, Logs: {'loss': 0.0561, 'grad_norm': 0.1762194186449051, 'learning_rate': 1.8862392494136045e-05, 'epoch': 0.5706468633965214}\nEpoch: 0.5745554035567716, Logs: {'loss': 0.0549, 'grad_norm': 0.18714483082294464, 'learning_rate': 1.885457388584832e-05, 'epoch': 0.5745554035567716}\nEpoch: 0.5784639437170217, Logs: {'loss': 0.0508, 'grad_norm': 0.29012617468833923, 'learning_rate': 1.8846755277560595e-05, 'epoch': 0.5784639437170217}\nEpoch: 0.5823724838772718, Logs: {'loss': 0.0611, 'grad_norm': 0.13774867355823517, 'learning_rate': 1.883893666927287e-05, 'epoch': 0.5823724838772718}\nEpoch: 0.586281024037522, Logs: {'loss': 0.0559, 'grad_norm': 0.14349853992462158, 'learning_rate': 1.8831118060985145e-05, 'epoch': 0.586281024037522}\nEpoch: 0.5901895641977721, Logs: {'loss': 0.0563, 'grad_norm': 0.18023620545864105, 'learning_rate': 1.882329945269742e-05, 'epoch': 0.5901895641977721}\nEpoch: 0.5940981043580222, Logs: {'loss': 0.0651, 'grad_norm': 0.14984934031963348, 'learning_rate': 1.8815480844409698e-05, 'epoch': 0.5940981043580222}\nEpoch: 0.5980066445182725, Logs: {'loss': 0.0585, 'grad_norm': 0.17451632022857666, 'learning_rate': 1.880766223612197e-05, 'epoch': 0.5980066445182725}\nEpoch: 0.6019151846785226, Logs: {'loss': 0.0589, 'grad_norm': 0.3610214293003082, 'learning_rate': 1.8799843627834248e-05, 'epoch': 0.6019151846785226}\nEpoch: 0.6058237248387727, Logs: {'loss': 0.0563, 'grad_norm': 0.1425773799419403, 'learning_rate': 1.8792025019546523e-05, 'epoch': 0.6058237248387727}\nEpoch: 0.6097322649990229, Logs: {'loss': 0.0587, 'grad_norm': 0.3881482779979706, 'learning_rate': 1.8784206411258798e-05, 'epoch': 0.6097322649990229}\nEpoch: 0.613640805159273, Logs: {'loss': 0.0609, 'grad_norm': 0.3419175148010254, 'learning_rate': 1.8776387802971072e-05, 'epoch': 0.613640805159273}\nEpoch: 0.6175493453195232, Logs: {'loss': 0.057, 'grad_norm': 0.25240960717201233, 'learning_rate': 1.8768569194683347e-05, 'epoch': 0.6175493453195232}\nEpoch: 0.6214578854797733, Logs: {'loss': 0.0548, 'grad_norm': 0.18148161470890045, 'learning_rate': 1.8760750586395625e-05, 'epoch': 0.6214578854797733}\nEpoch: 0.6253664256400234, Logs: {'loss': 0.0506, 'grad_norm': 0.2646191418170929, 'learning_rate': 1.8752931978107897e-05, 'epoch': 0.6253664256400234}\nEpoch: 0.6292749658002736, Logs: {'loss': 0.0625, 'grad_norm': 0.2889006435871124, 'learning_rate': 1.8745113369820175e-05, 'epoch': 0.6292749658002736}\nEpoch: 0.6331835059605238, Logs: {'loss': 0.0519, 'grad_norm': 0.16274473071098328, 'learning_rate': 1.873729476153245e-05, 'epoch': 0.6331835059605238}\nEpoch: 0.6370920461207739, Logs: {'loss': 0.0563, 'grad_norm': 0.1611247956752777, 'learning_rate': 1.8729476153244725e-05, 'epoch': 0.6370920461207739}\nEpoch: 0.6410005862810241, Logs: {'loss': 0.0587, 'grad_norm': 0.19309887290000916, 'learning_rate': 1.8721657544957e-05, 'epoch': 0.6410005862810241}\nEpoch: 0.6449091264412742, Logs: {'loss': 0.055, 'grad_norm': 0.9047468304634094, 'learning_rate': 1.8713838936669275e-05, 'epoch': 0.6449091264412742}\nEpoch: 0.6488176666015243, Logs: {'loss': 0.0521, 'grad_norm': 0.17782148718833923, 'learning_rate': 1.870602032838155e-05, 'epoch': 0.6488176666015243}\nEpoch: 0.6527262067617745, Logs: {'loss': 0.0536, 'grad_norm': 0.1485482007265091, 'learning_rate': 1.8698201720093824e-05, 'epoch': 0.6527262067617745}\nEpoch: 0.6566347469220246, Logs: {'loss': 0.0545, 'grad_norm': 0.17505498230457306, 'learning_rate': 1.86903831118061e-05, 'epoch': 0.6566347469220246}\nEpoch: 0.6605432870822747, Logs: {'loss': 0.0521, 'grad_norm': 0.17506369948387146, 'learning_rate': 1.8682564503518374e-05, 'epoch': 0.6605432870822747}\nEpoch: 0.6644518272425249, Logs: {'loss': 0.0499, 'grad_norm': 0.16364561021327972, 'learning_rate': 1.867474589523065e-05, 'epoch': 0.6644518272425249}\nEpoch: 0.668360367402775, Logs: {'loss': 0.0493, 'grad_norm': 0.12449520826339722, 'learning_rate': 1.8666927286942927e-05, 'epoch': 0.668360367402775}\nEpoch: 0.6722689075630253, Logs: {'loss': 0.0576, 'grad_norm': 0.13308335840702057, 'learning_rate': 1.86591086786552e-05, 'epoch': 0.6722689075630253}\nEpoch: 0.6761774477232754, Logs: {'loss': 0.0495, 'grad_norm': 0.3376491367816925, 'learning_rate': 1.8651290070367477e-05, 'epoch': 0.6761774477232754}\nEpoch: 0.6800859878835255, Logs: {'loss': 0.0487, 'grad_norm': 0.13859662413597107, 'learning_rate': 1.8643471462079752e-05, 'epoch': 0.6800859878835255}\nEpoch: 0.6839945280437757, Logs: {'loss': 0.0527, 'grad_norm': 0.2482958734035492, 'learning_rate': 1.8635652853792027e-05, 'epoch': 0.6839945280437757}\nEpoch: 0.6879030682040258, Logs: {'loss': 0.0492, 'grad_norm': 0.15296602249145508, 'learning_rate': 1.86278342455043e-05, 'epoch': 0.6879030682040258}\nEpoch: 0.6918116083642759, Logs: {'loss': 0.053, 'grad_norm': 0.09187339246273041, 'learning_rate': 1.8620015637216576e-05, 'epoch': 0.6918116083642759}\nEpoch: 0.6957201485245261, Logs: {'loss': 0.0549, 'grad_norm': 0.1958165317773819, 'learning_rate': 1.861219702892885e-05, 'epoch': 0.6957201485245261}\nEpoch: 0.6996286886847762, Logs: {'loss': 0.0537, 'grad_norm': 0.2501961290836334, 'learning_rate': 1.8604378420641126e-05, 'epoch': 0.6996286886847762}\nEpoch: 0.7035372288450263, Logs: {'loss': 0.0536, 'grad_norm': 1.1970984935760498, 'learning_rate': 1.8596559812353404e-05, 'epoch': 0.7035372288450263}\nEpoch: 0.7074457690052766, Logs: {'loss': 0.0646, 'grad_norm': 0.12033659219741821, 'learning_rate': 1.8588741204065676e-05, 'epoch': 0.7074457690052766}\nEpoch: 0.7113543091655267, Logs: {'loss': 0.0476, 'grad_norm': 0.12735804915428162, 'learning_rate': 1.8580922595777954e-05, 'epoch': 0.7113543091655267}\nEpoch: 0.7152628493257768, Logs: {'loss': 0.054, 'grad_norm': 0.1343458890914917, 'learning_rate': 1.857310398749023e-05, 'epoch': 0.7152628493257768}\nEpoch: 0.719171389486027, Logs: {'loss': 0.0551, 'grad_norm': 0.28249675035476685, 'learning_rate': 1.8565285379202504e-05, 'epoch': 0.719171389486027}\nEpoch: 0.7230799296462771, Logs: {'loss': 0.0563, 'grad_norm': 0.14476118981838226, 'learning_rate': 1.855746677091478e-05, 'epoch': 0.7230799296462771}\nEpoch: 0.7269884698065273, Logs: {'loss': 0.0528, 'grad_norm': 0.2925013601779938, 'learning_rate': 1.8549648162627054e-05, 'epoch': 0.7269884698065273}\nEpoch: 0.7308970099667774, Logs: {'loss': 0.0525, 'grad_norm': 0.3039970397949219, 'learning_rate': 1.854182955433933e-05, 'epoch': 0.7308970099667774}\nEpoch: 0.7348055501270275, Logs: {'loss': 0.0606, 'grad_norm': 1.3361515998840332, 'learning_rate': 1.8534010946051603e-05, 'epoch': 0.7348055501270275}\nEpoch: 0.7387140902872777, Logs: {'loss': 0.059, 'grad_norm': 0.19215275347232819, 'learning_rate': 1.8526192337763878e-05, 'epoch': 0.7387140902872777}\nEpoch: 0.7426226304475279, Logs: {'loss': 0.0517, 'grad_norm': 0.15292274951934814, 'learning_rate': 1.8518373729476157e-05, 'epoch': 0.7426226304475279}\nEpoch: 0.746531170607778, Logs: {'loss': 0.0509, 'grad_norm': 0.13086630403995514, 'learning_rate': 1.8510555121188428e-05, 'epoch': 0.746531170607778}\nEpoch: 0.7504397107680282, Logs: {'loss': 0.0571, 'grad_norm': 0.3074050843715668, 'learning_rate': 1.8502736512900706e-05, 'epoch': 0.7504397107680282}\nEpoch: 0.7543482509282783, Logs: {'loss': 0.061, 'grad_norm': 0.142946258187294, 'learning_rate': 1.849491790461298e-05, 'epoch': 0.7543482509282783}\nEpoch: 0.7582567910885284, Logs: {'loss': 0.0515, 'grad_norm': 0.296502023935318, 'learning_rate': 1.8487099296325256e-05, 'epoch': 0.7582567910885284}\nEpoch: 0.7621653312487786, Logs: {'loss': 0.0572, 'grad_norm': 0.293935090303421, 'learning_rate': 1.847928068803753e-05, 'epoch': 0.7621653312487786}\nEpoch: 0.7660738714090287, Logs: {'loss': 0.0587, 'grad_norm': 0.18138451874256134, 'learning_rate': 1.8471462079749806e-05, 'epoch': 0.7660738714090287}\nEpoch: 0.7699824115692788, Logs: {'loss': 0.0488, 'grad_norm': 0.12614645063877106, 'learning_rate': 1.846364347146208e-05, 'epoch': 0.7699824115692788}\nEpoch: 0.773890951729529, Logs: {'loss': 0.0504, 'grad_norm': 0.1727868765592575, 'learning_rate': 1.8455824863174355e-05, 'epoch': 0.773890951729529}\nEpoch: 0.7777994918897791, Logs: {'loss': 0.0564, 'grad_norm': 0.1052507758140564, 'learning_rate': 1.8448006254886634e-05, 'epoch': 0.7777994918897791}\nEpoch: 0.7817080320500294, Logs: {'loss': 0.056, 'grad_norm': 0.15926410257816315, 'learning_rate': 1.8440187646598905e-05, 'epoch': 0.7817080320500294}\nEpoch: 0.7856165722102795, Logs: {'loss': 0.065, 'grad_norm': 0.19291463494300842, 'learning_rate': 1.8432369038311183e-05, 'epoch': 0.7856165722102795}\nEpoch: 0.7895251123705296, Logs: {'loss': 0.0501, 'grad_norm': 0.1413259357213974, 'learning_rate': 1.8424550430023458e-05, 'epoch': 0.7895251123705296}\nEpoch: 0.7934336525307798, Logs: {'loss': 0.0533, 'grad_norm': 0.13189385831356049, 'learning_rate': 1.8416731821735733e-05, 'epoch': 0.7934336525307798}\nEpoch: 0.7973421926910299, Logs: {'loss': 0.0576, 'grad_norm': 0.17439962923526764, 'learning_rate': 1.8408913213448008e-05, 'epoch': 0.7973421926910299}\nEpoch: 0.80125073285128, Logs: {'loss': 0.0505, 'grad_norm': 0.26402223110198975, 'learning_rate': 1.8401094605160283e-05, 'epoch': 0.80125073285128}\nEpoch: 0.8051592730115302, Logs: {'loss': 0.0511, 'grad_norm': 0.13568788766860962, 'learning_rate': 1.8393275996872558e-05, 'epoch': 0.8051592730115302}\nEpoch: 0.8090678131717803, Logs: {'loss': 0.0522, 'grad_norm': 0.12738271057605743, 'learning_rate': 1.8385457388584833e-05, 'epoch': 0.8090678131717803}\nEpoch: 0.8129763533320304, Logs: {'loss': 0.0541, 'grad_norm': 1.1147723197937012, 'learning_rate': 1.8377638780297107e-05, 'epoch': 0.8129763533320304}\nEpoch: 0.8168848934922807, Logs: {'loss': 0.0527, 'grad_norm': 0.4033663272857666, 'learning_rate': 1.8369820172009382e-05, 'epoch': 0.8168848934922807}\nEpoch: 0.8207934336525308, Logs: {'loss': 0.0597, 'grad_norm': 0.22408929467201233, 'learning_rate': 1.8362001563721657e-05, 'epoch': 0.8207934336525308}\nEpoch: 0.8247019738127809, Logs: {'loss': 0.053, 'grad_norm': 0.10128912329673767, 'learning_rate': 1.8354182955433935e-05, 'epoch': 0.8247019738127809}\nEpoch: 0.8286105139730311, Logs: {'loss': 0.0567, 'grad_norm': 0.16079655289649963, 'learning_rate': 1.834636434714621e-05, 'epoch': 0.8286105139730311}\nEpoch: 0.8325190541332812, Logs: {'loss': 0.0603, 'grad_norm': 0.0952255055308342, 'learning_rate': 1.8338545738858485e-05, 'epoch': 0.8325190541332812}\nEpoch: 0.8364275942935314, Logs: {'loss': 0.0591, 'grad_norm': 0.13886189460754395, 'learning_rate': 1.833072713057076e-05, 'epoch': 0.8364275942935314}\nEpoch: 0.8403361344537815, Logs: {'loss': 0.0554, 'grad_norm': 0.3368971049785614, 'learning_rate': 1.8322908522283035e-05, 'epoch': 0.8403361344537815}\nEpoch: 0.8442446746140316, Logs: {'loss': 0.0618, 'grad_norm': 0.27653831243515015, 'learning_rate': 1.831508991399531e-05, 'epoch': 0.8442446746140316}\nEpoch: 0.8481532147742818, Logs: {'loss': 0.0657, 'grad_norm': 0.17924262583255768, 'learning_rate': 1.8307271305707585e-05, 'epoch': 0.8481532147742818}\nEpoch: 0.852061754934532, Logs: {'loss': 0.0527, 'grad_norm': 0.19709938764572144, 'learning_rate': 1.8299452697419863e-05, 'epoch': 0.852061754934532}\nEpoch: 0.8559702950947821, Logs: {'loss': 0.055, 'grad_norm': 0.09872207790613174, 'learning_rate': 1.8291634089132134e-05, 'epoch': 0.8559702950947821}\nEpoch: 0.8598788352550323, Logs: {'loss': 0.0515, 'grad_norm': 0.09124535322189331, 'learning_rate': 1.8283815480844413e-05, 'epoch': 0.8598788352550323}\nEpoch: 0.8637873754152824, Logs: {'loss': 0.0499, 'grad_norm': 0.24439601600170135, 'learning_rate': 1.8275996872556688e-05, 'epoch': 0.8637873754152824}\nEpoch: 0.8676959155755325, Logs: {'loss': 0.0558, 'grad_norm': 0.12019483745098114, 'learning_rate': 1.8268178264268962e-05, 'epoch': 0.8676959155755325}\nEpoch: 0.8716044557357827, Logs: {'loss': 0.0511, 'grad_norm': 0.15751266479492188, 'learning_rate': 1.8260359655981237e-05, 'epoch': 0.8716044557357827}\nEpoch: 0.8755129958960328, Logs: {'loss': 0.049, 'grad_norm': 0.17290543019771576, 'learning_rate': 1.8252541047693512e-05, 'epoch': 0.8755129958960328}\nEpoch: 0.8794215360562829, Logs: {'loss': 0.0514, 'grad_norm': 0.31319504976272583, 'learning_rate': 1.8244722439405787e-05, 'epoch': 0.8794215360562829}\nEpoch: 0.8833300762165331, Logs: {'loss': 0.0451, 'grad_norm': 0.1464979648590088, 'learning_rate': 1.8236903831118062e-05, 'epoch': 0.8833300762165331}\nEpoch: 0.8872386163767833, Logs: {'loss': 0.0488, 'grad_norm': 0.16380706429481506, 'learning_rate': 1.8229085222830337e-05, 'epoch': 0.8872386163767833}\nEpoch: 0.8911471565370335, Logs: {'loss': 0.0449, 'grad_norm': 0.23578988015651703, 'learning_rate': 1.822126661454261e-05, 'epoch': 0.8911471565370335}\nEpoch: 0.8950556966972836, Logs: {'loss': 0.0522, 'grad_norm': 0.2808806300163269, 'learning_rate': 1.8213448006254886e-05, 'epoch': 0.8950556966972836}\nEpoch: 0.8989642368575337, Logs: {'loss': 0.055, 'grad_norm': 0.1129593551158905, 'learning_rate': 1.8205629397967165e-05, 'epoch': 0.8989642368575337}\nEpoch: 0.9028727770177839, Logs: {'loss': 0.0526, 'grad_norm': 0.30763566493988037, 'learning_rate': 1.8197810789679436e-05, 'epoch': 0.9028727770177839}\nEpoch: 0.906781317178034, Logs: {'loss': 0.0642, 'grad_norm': 0.940009593963623, 'learning_rate': 1.8189992181391714e-05, 'epoch': 0.906781317178034}\nEpoch: 0.9106898573382841, Logs: {'loss': 0.0508, 'grad_norm': 0.1985567957162857, 'learning_rate': 1.818217357310399e-05, 'epoch': 0.9106898573382841}\nEpoch: 0.9145983974985343, Logs: {'loss': 0.0502, 'grad_norm': 0.15928499400615692, 'learning_rate': 1.8174354964816264e-05, 'epoch': 0.9145983974985343}\nEpoch: 0.9185069376587844, Logs: {'loss': 0.0495, 'grad_norm': 0.17869369685649872, 'learning_rate': 1.816653635652854e-05, 'epoch': 0.9185069376587844}\nEpoch: 0.9224154778190345, Logs: {'loss': 0.0507, 'grad_norm': 0.15138188004493713, 'learning_rate': 1.8158717748240814e-05, 'epoch': 0.9224154778190345}\nEpoch: 0.9263240179792848, Logs: {'loss': 0.0497, 'grad_norm': 0.2537382245063782, 'learning_rate': 1.8150899139953092e-05, 'epoch': 0.9263240179792848}\nEpoch: 0.9302325581395349, Logs: {'loss': 0.0544, 'grad_norm': 0.11836135387420654, 'learning_rate': 1.8143080531665364e-05, 'epoch': 0.9302325581395349}\nEpoch: 0.934141098299785, Logs: {'loss': 0.0502, 'grad_norm': 0.1521960347890854, 'learning_rate': 1.8135261923377642e-05, 'epoch': 0.934141098299785}\nEpoch: 0.9380496384600352, Logs: {'loss': 0.0603, 'grad_norm': 0.17851009964942932, 'learning_rate': 1.8127443315089917e-05, 'epoch': 0.9380496384600352}\nEpoch: 0.9419581786202853, Logs: {'loss': 0.0535, 'grad_norm': 0.12825888395309448, 'learning_rate': 1.811962470680219e-05, 'epoch': 0.9419581786202853}\nEpoch: 0.9458667187805355, Logs: {'loss': 0.051, 'grad_norm': 0.14903897047042847, 'learning_rate': 1.8111806098514466e-05, 'epoch': 0.9458667187805355}\nEpoch: 0.9497752589407856, Logs: {'loss': 0.0527, 'grad_norm': 0.20032866299152374, 'learning_rate': 1.810398749022674e-05, 'epoch': 0.9497752589407856}\nEpoch: 0.9536837991010357, Logs: {'loss': 0.0525, 'grad_norm': 0.21742647886276245, 'learning_rate': 1.8096168881939016e-05, 'epoch': 0.9536837991010357}\nEpoch: 0.957592339261286, Logs: {'loss': 0.0596, 'grad_norm': 0.12366113066673279, 'learning_rate': 1.808835027365129e-05, 'epoch': 0.957592339261286}\nEpoch: 0.9615008794215361, Logs: {'loss': 0.0528, 'grad_norm': 0.17795054614543915, 'learning_rate': 1.8080531665363566e-05, 'epoch': 0.9615008794215361}\nEpoch: 0.9654094195817862, Logs: {'loss': 0.0563, 'grad_norm': 0.144095316529274, 'learning_rate': 1.807271305707584e-05, 'epoch': 0.9654094195817862}\nEpoch: 0.9693179597420364, Logs: {'loss': 0.0477, 'grad_norm': 0.08442382514476776, 'learning_rate': 1.8064894448788116e-05, 'epoch': 0.9693179597420364}\nEpoch: 0.9732264999022865, Logs: {'loss': 0.0563, 'grad_norm': 0.18478237092494965, 'learning_rate': 1.8057075840500394e-05, 'epoch': 0.9732264999022865}\nEpoch: 0.9771350400625366, Logs: {'loss': 0.048, 'grad_norm': 0.10629711300134659, 'learning_rate': 1.8049257232212665e-05, 'epoch': 0.9771350400625366}\nEpoch: 0.9810435802227868, Logs: {'loss': 0.0539, 'grad_norm': 0.35042935609817505, 'learning_rate': 1.8041438623924944e-05, 'epoch': 0.9810435802227868}\nEpoch: 0.9849521203830369, Logs: {'loss': 0.0509, 'grad_norm': 0.28677991032600403, 'learning_rate': 1.803362001563722e-05, 'epoch': 0.9849521203830369}\nEpoch: 0.988860660543287, Logs: {'loss': 0.0543, 'grad_norm': 0.5960797071456909, 'learning_rate': 1.8025801407349493e-05, 'epoch': 0.988860660543287}\nEpoch: 0.9927692007035372, Logs: {'loss': 0.0483, 'grad_norm': 0.15129756927490234, 'learning_rate': 1.8017982799061768e-05, 'epoch': 0.9927692007035372}\nEpoch: 0.9966777408637874, Logs: {'loss': 0.0502, 'grad_norm': 0.22217273712158203, 'learning_rate': 1.8010164190774043e-05, 'epoch': 0.9966777408637874}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0.9998045729919874, Logs: {'eval_loss': 0.046634890139102936, 'eval_bleu': 1.0519919183420577e-10, 'eval_accuracy': 0.9434128225175918, 'eval_runtime': 350.4248, 'eval_samples_per_second': 58.398, 'eval_steps_per_second': 3.65, 'epoch': 0.9998045729919874}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1.0005862810240376, Logs: {'loss': 0.0519, 'grad_norm': 0.15123414993286133, 'learning_rate': 1.8002345582486318e-05, 'epoch': 1.0005862810240376}\nEpoch: 1.0044948211842877, Logs: {'loss': 0.05, 'grad_norm': 0.10865063965320587, 'learning_rate': 1.7994526974198593e-05, 'epoch': 1.0044948211842877}\nEpoch: 1.0084033613445378, Logs: {'loss': 0.0476, 'grad_norm': 0.14457353949546814, 'learning_rate': 1.798670836591087e-05, 'epoch': 1.0084033613445378}\nEpoch: 1.012311901504788, Logs: {'loss': 0.0574, 'grad_norm': 0.2486591786146164, 'learning_rate': 1.7978889757623143e-05, 'epoch': 1.012311901504788}\nEpoch: 1.016220441665038, Logs: {'loss': 0.0472, 'grad_norm': 0.13794267177581787, 'learning_rate': 1.797107114933542e-05, 'epoch': 1.016220441665038}\nEpoch: 1.0201289818252883, Logs: {'loss': 0.0486, 'grad_norm': 0.22600610554218292, 'learning_rate': 1.7963252541047696e-05, 'epoch': 1.0201289818252883}\nEpoch: 1.0240375219855384, Logs: {'loss': 0.0498, 'grad_norm': 0.12839552760124207, 'learning_rate': 1.795543393275997e-05, 'epoch': 1.0240375219855384}\nEpoch: 1.0279460621457885, Logs: {'loss': 0.0467, 'grad_norm': 0.1475924551486969, 'learning_rate': 1.7947615324472245e-05, 'epoch': 1.0279460621457885}\nEpoch: 1.0318546023060386, Logs: {'loss': 0.0481, 'grad_norm': 0.11223965883255005, 'learning_rate': 1.793979671618452e-05, 'epoch': 1.0318546023060386}\nEpoch: 1.0357631424662888, Logs: {'loss': 0.0562, 'grad_norm': 0.2711449861526489, 'learning_rate': 1.79319781078968e-05, 'epoch': 1.0357631424662888}\nEpoch: 1.039671682626539, Logs: {'loss': 0.053, 'grad_norm': 0.16928139328956604, 'learning_rate': 1.792415949960907e-05, 'epoch': 1.039671682626539}\nEpoch: 1.0435802227867892, Logs: {'loss': 0.0547, 'grad_norm': 0.20939385890960693, 'learning_rate': 1.7916340891321345e-05, 'epoch': 1.0435802227867892}\nEpoch: 1.0474887629470393, Logs: {'loss': 0.048, 'grad_norm': 0.18817317485809326, 'learning_rate': 1.7908522283033623e-05, 'epoch': 1.0474887629470393}\nEpoch: 1.0513973031072894, Logs: {'loss': 0.0489, 'grad_norm': 0.11800600588321686, 'learning_rate': 1.7900703674745895e-05, 'epoch': 1.0513973031072894}\nEpoch: 1.0553058432675395, Logs: {'loss': 0.0468, 'grad_norm': 0.12426212430000305, 'learning_rate': 1.7892885066458173e-05, 'epoch': 1.0553058432675395}\nEpoch: 1.0592143834277896, Logs: {'loss': 0.0445, 'grad_norm': 0.2503669559955597, 'learning_rate': 1.7885066458170448e-05, 'epoch': 1.0592143834277896}\nEpoch: 1.06312292358804, Logs: {'loss': 0.0524, 'grad_norm': 0.20950163900852203, 'learning_rate': 1.7877247849882723e-05, 'epoch': 1.06312292358804}\nEpoch: 1.06703146374829, Logs: {'loss': 0.0461, 'grad_norm': 0.144275963306427, 'learning_rate': 1.7869429241594997e-05, 'epoch': 1.06703146374829}\nEpoch: 1.0709400039085402, Logs: {'loss': 0.0438, 'grad_norm': 0.10669473558664322, 'learning_rate': 1.7861610633307272e-05, 'epoch': 1.0709400039085402}\nEpoch: 1.0748485440687903, Logs: {'loss': 0.0459, 'grad_norm': 0.10433447360992432, 'learning_rate': 1.7853792025019547e-05, 'epoch': 1.0748485440687903}\nEpoch: 1.0787570842290404, Logs: {'loss': 0.0558, 'grad_norm': 0.1908527910709381, 'learning_rate': 1.7845973416731822e-05, 'epoch': 1.0787570842290404}\nEpoch: 1.0826656243892907, Logs: {'loss': 0.0568, 'grad_norm': 0.17957162857055664, 'learning_rate': 1.78381548084441e-05, 'epoch': 1.0826656243892907}\nEpoch: 1.0865741645495408, Logs: {'loss': 0.052, 'grad_norm': 0.1495734602212906, 'learning_rate': 1.7830336200156372e-05, 'epoch': 1.0865741645495408}\nEpoch: 1.090482704709791, Logs: {'loss': 0.0476, 'grad_norm': 0.18896779417991638, 'learning_rate': 1.782251759186865e-05, 'epoch': 1.090482704709791}\nEpoch: 1.094391244870041, Logs: {'loss': 0.0525, 'grad_norm': 0.1376670002937317, 'learning_rate': 1.7814698983580925e-05, 'epoch': 1.094391244870041}\nEpoch: 1.0982997850302911, Logs: {'loss': 0.0543, 'grad_norm': 0.1745636910200119, 'learning_rate': 1.78068803752932e-05, 'epoch': 1.0982997850302911}\nEpoch: 1.1022083251905412, Logs: {'loss': 0.0496, 'grad_norm': 0.13526932895183563, 'learning_rate': 1.7799061767005475e-05, 'epoch': 1.1022083251905412}\nEpoch: 1.1061168653507916, Logs: {'loss': 0.0517, 'grad_norm': 0.1897117793560028, 'learning_rate': 1.779124315871775e-05, 'epoch': 1.1061168653507916}\nEpoch: 1.1100254055110417, Logs: {'loss': 0.0513, 'grad_norm': 0.12758056819438934, 'learning_rate': 1.7783424550430024e-05, 'epoch': 1.1100254055110417}\nEpoch: 1.1139339456712918, Logs: {'loss': 0.0495, 'grad_norm': 0.10873008519411087, 'learning_rate': 1.77756059421423e-05, 'epoch': 1.1139339456712918}\nEpoch: 1.1178424858315419, Logs: {'loss': 0.0599, 'grad_norm': 0.1765369474887848, 'learning_rate': 1.7767787333854578e-05, 'epoch': 1.1178424858315419}\nEpoch: 1.121751025991792, Logs: {'loss': 0.0518, 'grad_norm': 0.20888511836528778, 'learning_rate': 1.775996872556685e-05, 'epoch': 1.121751025991792}\nEpoch: 1.1256595661520423, Logs: {'loss': 0.0494, 'grad_norm': 0.21333099901676178, 'learning_rate': 1.7752150117279124e-05, 'epoch': 1.1256595661520423}\nEpoch: 1.1295681063122924, Logs: {'loss': 0.0462, 'grad_norm': 0.25000157952308655, 'learning_rate': 1.7744331508991402e-05, 'epoch': 1.1295681063122924}\nEpoch: 1.1334766464725425, Logs: {'loss': 0.0525, 'grad_norm': 0.42871353030204773, 'learning_rate': 1.7736512900703674e-05, 'epoch': 1.1334766464725425}\nEpoch: 1.1373851866327926, Logs: {'loss': 0.0531, 'grad_norm': 0.11241903156042099, 'learning_rate': 1.7728694292415952e-05, 'epoch': 1.1373851866327926}\nEpoch: 1.1412937267930428, Logs: {'loss': 0.0489, 'grad_norm': 0.21088095009326935, 'learning_rate': 1.7720875684128227e-05, 'epoch': 1.1412937267930428}\nEpoch: 1.1452022669532929, Logs: {'loss': 0.0542, 'grad_norm': 0.17458771169185638, 'learning_rate': 1.77130570758405e-05, 'epoch': 1.1452022669532929}\nEpoch: 1.1491108071135432, Logs: {'loss': 0.0497, 'grad_norm': 0.4137939214706421, 'learning_rate': 1.7705238467552776e-05, 'epoch': 1.1491108071135432}\nEpoch: 1.1530193472737933, Logs: {'loss': 0.0508, 'grad_norm': 0.16581717133522034, 'learning_rate': 1.769741985926505e-05, 'epoch': 1.1530193472737933}\nEpoch: 1.1569278874340434, Logs: {'loss': 0.053, 'grad_norm': 0.12279456853866577, 'learning_rate': 1.768960125097733e-05, 'epoch': 1.1569278874340434}\nEpoch: 1.1608364275942935, Logs: {'loss': 0.0526, 'grad_norm': 0.1559862494468689, 'learning_rate': 1.76817826426896e-05, 'epoch': 1.1608364275942935}\nEpoch: 1.1647449677545436, Logs: {'loss': 0.0544, 'grad_norm': 0.15601058304309845, 'learning_rate': 1.767396403440188e-05, 'epoch': 1.1647449677545436}\nEpoch: 1.1686535079147937, Logs: {'loss': 0.0494, 'grad_norm': 0.13469257950782776, 'learning_rate': 1.7666145426114154e-05, 'epoch': 1.1686535079147937}\nEpoch: 1.172562048075044, Logs: {'loss': 0.0517, 'grad_norm': 0.5852122902870178, 'learning_rate': 1.765832681782643e-05, 'epoch': 1.172562048075044}\nEpoch: 1.1764705882352942, Logs: {'loss': 0.0547, 'grad_norm': 0.25854840874671936, 'learning_rate': 1.7650508209538704e-05, 'epoch': 1.1764705882352942}\nEpoch: 1.1803791283955443, Logs: {'loss': 0.0463, 'grad_norm': 0.10820130258798599, 'learning_rate': 1.764268960125098e-05, 'epoch': 1.1803791283955443}\nEpoch: 1.1842876685557944, Logs: {'loss': 0.0524, 'grad_norm': 0.14171230792999268, 'learning_rate': 1.7634870992963254e-05, 'epoch': 1.1842876685557944}\nEpoch: 1.1881962087160445, Logs: {'loss': 0.0535, 'grad_norm': 0.11317329853773117, 'learning_rate': 1.762705238467553e-05, 'epoch': 1.1881962087160445}\nEpoch: 1.1921047488762948, Logs: {'loss': 0.0474, 'grad_norm': 0.13606728613376617, 'learning_rate': 1.7619233776387807e-05, 'epoch': 1.1921047488762948}\nEpoch: 1.196013289036545, Logs: {'loss': 0.0515, 'grad_norm': 0.1833164542913437, 'learning_rate': 1.7611415168100078e-05, 'epoch': 1.196013289036545}\nEpoch: 1.199921829196795, Logs: {'loss': 0.047, 'grad_norm': 0.16471023857593536, 'learning_rate': 1.7603596559812356e-05, 'epoch': 1.199921829196795}\nEpoch: 1.2038303693570451, Logs: {'loss': 0.0561, 'grad_norm': 0.32382461428642273, 'learning_rate': 1.759577795152463e-05, 'epoch': 1.2038303693570451}\nEpoch: 1.2077389095172952, Logs: {'loss': 0.0441, 'grad_norm': 0.14529088139533997, 'learning_rate': 1.7587959343236903e-05, 'epoch': 1.2077389095172952}\nEpoch: 1.2116474496775453, Logs: {'loss': 0.0506, 'grad_norm': 0.1535293459892273, 'learning_rate': 1.758014073494918e-05, 'epoch': 1.2116474496775453}\nEpoch: 1.2155559898377957, Logs: {'loss': 0.0493, 'grad_norm': 0.2634589672088623, 'learning_rate': 1.7572322126661456e-05, 'epoch': 1.2155559898377957}\nEpoch: 1.2194645299980458, Logs: {'loss': 0.0553, 'grad_norm': 0.18960176408290863, 'learning_rate': 1.756450351837373e-05, 'epoch': 1.2194645299980458}\nEpoch: 1.2233730701582959, Logs: {'loss': 0.0567, 'grad_norm': 0.21119867265224457, 'learning_rate': 1.7556684910086006e-05, 'epoch': 1.2233730701582959}\nEpoch: 1.227281610318546, Logs: {'loss': 0.0437, 'grad_norm': 0.08738908916711807, 'learning_rate': 1.754886630179828e-05, 'epoch': 1.227281610318546}\nEpoch: 1.231190150478796, Logs: {'loss': 0.0507, 'grad_norm': 0.20726361870765686, 'learning_rate': 1.7541047693510555e-05, 'epoch': 1.231190150478796}\nEpoch: 1.2350986906390462, Logs: {'loss': 0.0492, 'grad_norm': 0.11378785222768784, 'learning_rate': 1.753322908522283e-05, 'epoch': 1.2350986906390462}\nEpoch: 1.2390072307992965, Logs: {'loss': 0.0524, 'grad_norm': 0.1920509785413742, 'learning_rate': 1.752541047693511e-05, 'epoch': 1.2390072307992965}\nEpoch: 1.2429157709595466, Logs: {'loss': 0.0497, 'grad_norm': 0.12556645274162292, 'learning_rate': 1.751759186864738e-05, 'epoch': 1.2429157709595466}\nEpoch: 1.2468243111197967, Logs: {'loss': 0.0536, 'grad_norm': 0.1685650646686554, 'learning_rate': 1.7509773260359658e-05, 'epoch': 1.2468243111197967}\nEpoch: 1.2507328512800469, Logs: {'loss': 0.0527, 'grad_norm': 0.11543403565883636, 'learning_rate': 1.7501954652071933e-05, 'epoch': 1.2507328512800469}\nEpoch: 1.254641391440297, Logs: {'loss': 0.0485, 'grad_norm': 0.235906183719635, 'learning_rate': 1.7494136043784208e-05, 'epoch': 1.254641391440297}\nEpoch: 1.2585499316005473, Logs: {'loss': 0.0473, 'grad_norm': 0.25068750977516174, 'learning_rate': 1.7486317435496483e-05, 'epoch': 1.2585499316005473}\nEpoch: 1.2624584717607974, Logs: {'loss': 0.0505, 'grad_norm': 0.19856144487857819, 'learning_rate': 1.7478498827208758e-05, 'epoch': 1.2624584717607974}\nEpoch: 1.2663670119210475, Logs: {'loss': 0.048, 'grad_norm': 0.13093018531799316, 'learning_rate': 1.7470680218921036e-05, 'epoch': 1.2663670119210475}\nEpoch: 1.2702755520812976, Logs: {'loss': 0.049, 'grad_norm': 0.30265820026397705, 'learning_rate': 1.7462861610633307e-05, 'epoch': 1.2702755520812976}\nEpoch: 1.2741840922415477, Logs: {'loss': 0.0495, 'grad_norm': 0.7602198719978333, 'learning_rate': 1.7455043002345586e-05, 'epoch': 1.2741840922415477}\nEpoch: 1.278092632401798, Logs: {'loss': 0.0494, 'grad_norm': 0.22616392374038696, 'learning_rate': 1.744722439405786e-05, 'epoch': 1.278092632401798}\nEpoch: 1.2820011725620482, Logs: {'loss': 0.0487, 'grad_norm': 0.11151161789894104, 'learning_rate': 1.7439405785770135e-05, 'epoch': 1.2820011725620482}\nEpoch: 1.2859097127222983, Logs: {'loss': 0.0514, 'grad_norm': 0.20071564614772797, 'learning_rate': 1.743158717748241e-05, 'epoch': 1.2859097127222983}\nEpoch: 1.2898182528825484, Logs: {'loss': 0.0478, 'grad_norm': 0.1335384100675583, 'learning_rate': 1.7423768569194685e-05, 'epoch': 1.2898182528825484}\nEpoch: 1.2937267930427985, Logs: {'loss': 0.0546, 'grad_norm': 0.15830416977405548, 'learning_rate': 1.741594996090696e-05, 'epoch': 1.2937267930427985}\nEpoch: 1.2976353332030486, Logs: {'loss': 0.0511, 'grad_norm': 0.1342906504869461, 'learning_rate': 1.7408131352619235e-05, 'epoch': 1.2976353332030486}\nEpoch: 1.3015438733632987, Logs: {'loss': 0.0512, 'grad_norm': 0.13659992814064026, 'learning_rate': 1.740031274433151e-05, 'epoch': 1.3015438733632987}\nEpoch: 1.305452413523549, Logs: {'loss': 0.0496, 'grad_norm': 0.17872847616672516, 'learning_rate': 1.7392494136043785e-05, 'epoch': 1.305452413523549}\nEpoch: 1.3093609536837991, Logs: {'loss': 0.0486, 'grad_norm': 0.17599502205848694, 'learning_rate': 1.738467552775606e-05, 'epoch': 1.3093609536837991}\nEpoch: 1.3132694938440492, Logs: {'loss': 0.0446, 'grad_norm': 0.12643155455589294, 'learning_rate': 1.7376856919468338e-05, 'epoch': 1.3132694938440492}\nEpoch: 1.3171780340042993, Logs: {'loss': 0.0472, 'grad_norm': 0.12670885026454926, 'learning_rate': 1.736903831118061e-05, 'epoch': 1.3171780340042993}\nEpoch: 1.3210865741645494, Logs: {'loss': 0.0541, 'grad_norm': 0.14111678302288055, 'learning_rate': 1.7361219702892888e-05, 'epoch': 1.3210865741645494}\nEpoch: 1.3249951143247998, Logs: {'loss': 0.0479, 'grad_norm': 0.1750110238790512, 'learning_rate': 1.7353401094605162e-05, 'epoch': 1.3249951143247998}\nEpoch: 1.3289036544850499, Logs: {'loss': 0.0484, 'grad_norm': 0.4356369376182556, 'learning_rate': 1.7345582486317437e-05, 'epoch': 1.3289036544850499}\nEpoch: 1.3328121946453, Logs: {'loss': 0.0505, 'grad_norm': 0.1540519893169403, 'learning_rate': 1.7337763878029712e-05, 'epoch': 1.3328121946453}\nEpoch: 1.33672073480555, Logs: {'loss': 0.0466, 'grad_norm': 0.28485679626464844, 'learning_rate': 1.7329945269741987e-05, 'epoch': 1.33672073480555}\nEpoch: 1.3406292749658002, Logs: {'loss': 0.0406, 'grad_norm': 0.10733067989349365, 'learning_rate': 1.7322126661454262e-05, 'epoch': 1.3406292749658002}\nEpoch: 1.3445378151260505, Logs: {'loss': 0.0538, 'grad_norm': 0.31041261553764343, 'learning_rate': 1.7314308053166537e-05, 'epoch': 1.3445378151260505}\nEpoch: 1.3484463552863006, Logs: {'loss': 0.0455, 'grad_norm': 0.14396154880523682, 'learning_rate': 1.7306489444878815e-05, 'epoch': 1.3484463552863006}\nEpoch: 1.3523548954465507, Logs: {'loss': 0.0488, 'grad_norm': 0.21477054059505463, 'learning_rate': 1.7298670836591086e-05, 'epoch': 1.3523548954465507}\nEpoch: 1.3562634356068008, Logs: {'loss': 0.0489, 'grad_norm': 0.0661550834774971, 'learning_rate': 1.7290852228303365e-05, 'epoch': 1.3562634356068008}\nEpoch: 1.360171975767051, Logs: {'loss': 0.0496, 'grad_norm': 0.14555352926254272, 'learning_rate': 1.728303362001564e-05, 'epoch': 1.360171975767051}\nEpoch: 1.3640805159273013, Logs: {'loss': 0.0496, 'grad_norm': 0.12165117263793945, 'learning_rate': 1.7275215011727914e-05, 'epoch': 1.3640805159273013}\nEpoch: 1.3679890560875512, Logs: {'loss': 0.0476, 'grad_norm': 0.23537491261959076, 'learning_rate': 1.726739640344019e-05, 'epoch': 1.3679890560875512}\nEpoch: 1.3718975962478015, Logs: {'loss': 0.045, 'grad_norm': 0.2284214347600937, 'learning_rate': 1.7259577795152464e-05, 'epoch': 1.3718975962478015}\nEpoch: 1.3758061364080516, Logs: {'loss': 0.0506, 'grad_norm': 0.248910054564476, 'learning_rate': 1.725175918686474e-05, 'epoch': 1.3758061364080516}\nEpoch: 1.3797146765683017, Logs: {'loss': 0.0544, 'grad_norm': 0.17670494318008423, 'learning_rate': 1.7243940578577014e-05, 'epoch': 1.3797146765683017}\nEpoch: 1.3836232167285518, Logs: {'loss': 0.0488, 'grad_norm': 0.12453354895114899, 'learning_rate': 1.723612197028929e-05, 'epoch': 1.3836232167285518}\nEpoch: 1.387531756888802, Logs: {'loss': 0.0528, 'grad_norm': 0.2859886884689331, 'learning_rate': 1.7228303362001567e-05, 'epoch': 1.387531756888802}\nEpoch: 1.3914402970490523, Logs: {'loss': 0.0529, 'grad_norm': 0.3305644690990448, 'learning_rate': 1.722048475371384e-05, 'epoch': 1.3914402970490523}\nEpoch: 1.3953488372093024, Logs: {'loss': 0.0521, 'grad_norm': 0.1576155722141266, 'learning_rate': 1.7212666145426117e-05, 'epoch': 1.3953488372093024}\nEpoch: 1.3992573773695525, Logs: {'loss': 0.0574, 'grad_norm': 0.17566660046577454, 'learning_rate': 1.720484753713839e-05, 'epoch': 1.3992573773695525}\nEpoch: 1.4031659175298026, Logs: {'loss': 0.0496, 'grad_norm': 0.1733648180961609, 'learning_rate': 1.7197028928850666e-05, 'epoch': 1.4031659175298026}\nEpoch: 1.4070744576900527, Logs: {'loss': 0.0451, 'grad_norm': 0.2078823447227478, 'learning_rate': 1.718921032056294e-05, 'epoch': 1.4070744576900527}\nEpoch: 1.410982997850303, Logs: {'loss': 0.055, 'grad_norm': 0.362232506275177, 'learning_rate': 1.7181391712275216e-05, 'epoch': 1.410982997850303}\nEpoch: 1.4148915380105531, Logs: {'loss': 0.048, 'grad_norm': 0.30692794919013977, 'learning_rate': 1.717357310398749e-05, 'epoch': 1.4148915380105531}\nEpoch: 1.4188000781708032, Logs: {'loss': 0.0556, 'grad_norm': 0.37426435947418213, 'learning_rate': 1.7165754495699766e-05, 'epoch': 1.4188000781708032}\nEpoch: 1.4227086183310533, Logs: {'loss': 0.0467, 'grad_norm': 0.1859111487865448, 'learning_rate': 1.7157935887412044e-05, 'epoch': 1.4227086183310533}\nEpoch: 1.4266171584913034, Logs: {'loss': 0.0513, 'grad_norm': 0.19238083064556122, 'learning_rate': 1.7150117279124316e-05, 'epoch': 1.4266171584913034}\nEpoch: 1.4305256986515538, Logs: {'loss': 0.0529, 'grad_norm': 0.21862316131591797, 'learning_rate': 1.7142298670836594e-05, 'epoch': 1.4305256986515538}\nEpoch: 1.4344342388118039, Logs: {'loss': 0.0475, 'grad_norm': 0.1961611658334732, 'learning_rate': 1.713448006254887e-05, 'epoch': 1.4344342388118039}\nEpoch: 1.438342778972054, Logs: {'loss': 0.0555, 'grad_norm': 0.25251394510269165, 'learning_rate': 1.7126661454261144e-05, 'epoch': 1.438342778972054}\nEpoch: 1.442251319132304, Logs: {'loss': 0.0488, 'grad_norm': 0.12882308661937714, 'learning_rate': 1.711884284597342e-05, 'epoch': 1.442251319132304}\nEpoch: 1.4461598592925542, Logs: {'loss': 0.0457, 'grad_norm': 0.16114352643489838, 'learning_rate': 1.7111024237685693e-05, 'epoch': 1.4461598592925542}\nEpoch: 1.4500683994528043, Logs: {'loss': 0.0462, 'grad_norm': 0.2156335711479187, 'learning_rate': 1.7103205629397968e-05, 'epoch': 1.4500683994528043}\nEpoch: 1.4539769396130544, Logs: {'loss': 0.052, 'grad_norm': 0.13599245250225067, 'learning_rate': 1.7095387021110243e-05, 'epoch': 1.4539769396130544}\nEpoch: 1.4578854797733047, Logs: {'loss': 0.0489, 'grad_norm': 0.19039738178253174, 'learning_rate': 1.7087568412822518e-05, 'epoch': 1.4578854797733047}\nEpoch: 1.4617940199335548, Logs: {'loss': 0.0457, 'grad_norm': 0.15744300186634064, 'learning_rate': 1.7079749804534793e-05, 'epoch': 1.4617940199335548}\nEpoch: 1.465702560093805, Logs: {'loss': 0.0446, 'grad_norm': 0.1016198918223381, 'learning_rate': 1.7071931196247068e-05, 'epoch': 1.465702560093805}\nEpoch: 1.469611100254055, Logs: {'loss': 0.0513, 'grad_norm': 0.1074393093585968, 'learning_rate': 1.7064112587959346e-05, 'epoch': 1.469611100254055}\nEpoch: 1.4735196404143052, Logs: {'loss': 0.0494, 'grad_norm': 0.21124394237995148, 'learning_rate': 1.705629397967162e-05, 'epoch': 1.4735196404143052}\nEpoch: 1.4774281805745555, Logs: {'loss': 0.0467, 'grad_norm': 0.1632375568151474, 'learning_rate': 1.7048475371383896e-05, 'epoch': 1.4774281805745555}\nEpoch: 1.4813367207348056, Logs: {'loss': 0.0488, 'grad_norm': 0.1692156344652176, 'learning_rate': 1.704065676309617e-05, 'epoch': 1.4813367207348056}\nEpoch: 1.4852452608950557, Logs: {'loss': 0.0507, 'grad_norm': 3.1069693565368652, 'learning_rate': 1.7032838154808445e-05, 'epoch': 1.4852452608950557}\nEpoch: 1.4891538010553058, Logs: {'loss': 0.0584, 'grad_norm': 0.16199494898319244, 'learning_rate': 1.702501954652072e-05, 'epoch': 1.4891538010553058}\nEpoch: 1.493062341215556, Logs: {'loss': 0.0499, 'grad_norm': 0.15011870861053467, 'learning_rate': 1.7017200938232995e-05, 'epoch': 1.493062341215556}\nEpoch: 1.4969708813758063, Logs: {'loss': 0.0447, 'grad_norm': 0.2598731517791748, 'learning_rate': 1.7009382329945273e-05, 'epoch': 1.4969708813758063}\nEpoch: 1.5008794215360561, Logs: {'loss': 0.0467, 'grad_norm': 0.2260986715555191, 'learning_rate': 1.7001563721657545e-05, 'epoch': 1.5008794215360561}\nEpoch: 1.5047879616963065, Logs: {'loss': 0.0519, 'grad_norm': 0.3065667152404785, 'learning_rate': 1.6993745113369823e-05, 'epoch': 1.5047879616963065}\nEpoch: 1.5086965018565566, Logs: {'loss': 0.0494, 'grad_norm': 0.15580639243125916, 'learning_rate': 1.6985926505082098e-05, 'epoch': 1.5086965018565566}\nEpoch: 1.5126050420168067, Logs: {'loss': 0.0503, 'grad_norm': 0.1577935516834259, 'learning_rate': 1.6978107896794373e-05, 'epoch': 1.5126050420168067}\nEpoch: 1.516513582177057, Logs: {'loss': 0.045, 'grad_norm': 0.12240886688232422, 'learning_rate': 1.6970289288506648e-05, 'epoch': 1.516513582177057}\nEpoch: 1.520422122337307, Logs: {'loss': 0.0509, 'grad_norm': 0.13272306323051453, 'learning_rate': 1.6962470680218923e-05, 'epoch': 1.520422122337307}\nEpoch: 1.5243306624975572, Logs: {'loss': 0.0462, 'grad_norm': 0.11379334330558777, 'learning_rate': 1.6954652071931197e-05, 'epoch': 1.5243306624975572}\nEpoch: 1.5282392026578073, Logs: {'loss': 0.0434, 'grad_norm': 0.18010123074054718, 'learning_rate': 1.6946833463643472e-05, 'epoch': 1.5282392026578073}\nEpoch: 1.5321477428180574, Logs: {'loss': 0.0471, 'grad_norm': 0.1287480592727661, 'learning_rate': 1.6939014855355747e-05, 'epoch': 1.5321477428180574}\nEpoch: 1.5360562829783078, Logs: {'loss': 0.0469, 'grad_norm': 0.09305600076913834, 'learning_rate': 1.6931196247068022e-05, 'epoch': 1.5360562829783078}\nEpoch: 1.5399648231385576, Logs: {'loss': 0.0477, 'grad_norm': 0.10008293390274048, 'learning_rate': 1.6923377638780297e-05, 'epoch': 1.5399648231385576}\nEpoch: 1.543873363298808, Logs: {'loss': 0.0483, 'grad_norm': 0.1330479085445404, 'learning_rate': 1.6915559030492575e-05, 'epoch': 1.543873363298808}\nEpoch: 1.547781903459058, Logs: {'loss': 0.0536, 'grad_norm': 0.30359524488449097, 'learning_rate': 1.6907740422204847e-05, 'epoch': 1.547781903459058}\nEpoch: 1.5516904436193082, Logs: {'loss': 0.05, 'grad_norm': 0.14197804033756256, 'learning_rate': 1.6899921813917125e-05, 'epoch': 1.5516904436193082}\nEpoch: 1.5555989837795583, Logs: {'loss': 0.0472, 'grad_norm': 0.26129379868507385, 'learning_rate': 1.68921032056294e-05, 'epoch': 1.5555989837795583}\nEpoch: 1.5595075239398084, Logs: {'loss': 0.0434, 'grad_norm': 0.1634104698896408, 'learning_rate': 1.6884284597341675e-05, 'epoch': 1.5595075239398084}\nEpoch: 1.5634160641000587, Logs: {'loss': 0.0463, 'grad_norm': 0.18823261559009552, 'learning_rate': 1.687646598905395e-05, 'epoch': 1.5634160641000587}\nEpoch: 1.5673246042603086, Logs: {'loss': 0.046, 'grad_norm': 0.12687230110168457, 'learning_rate': 1.6868647380766224e-05, 'epoch': 1.5673246042603086}\nEpoch: 1.571233144420559, Logs: {'loss': 0.0499, 'grad_norm': 0.30245956778526306, 'learning_rate': 1.68608287724785e-05, 'epoch': 1.571233144420559}\nEpoch: 1.575141684580809, Logs: {'loss': 0.049, 'grad_norm': 0.20966674387454987, 'learning_rate': 1.6853010164190774e-05, 'epoch': 1.575141684580809}\nEpoch: 1.5790502247410592, Logs: {'loss': 0.0536, 'grad_norm': 0.16532185673713684, 'learning_rate': 1.6845191555903052e-05, 'epoch': 1.5790502247410592}\nEpoch: 1.5829587649013095, Logs: {'loss': 0.0497, 'grad_norm': 0.35405850410461426, 'learning_rate': 1.6837372947615327e-05, 'epoch': 1.5829587649013095}\nEpoch: 1.5868673050615594, Logs: {'loss': 0.0502, 'grad_norm': 0.1577635556459427, 'learning_rate': 1.6829554339327602e-05, 'epoch': 1.5868673050615594}\nEpoch: 1.5907758452218097, Logs: {'loss': 0.0484, 'grad_norm': 0.1539630889892578, 'learning_rate': 1.6821735731039877e-05, 'epoch': 1.5907758452218097}\nEpoch: 1.5946843853820598, Logs: {'loss': 0.0515, 'grad_norm': 0.24892793595790863, 'learning_rate': 1.6813917122752152e-05, 'epoch': 1.5946843853820598}\nEpoch: 1.59859292554231, Logs: {'loss': 0.051, 'grad_norm': 0.223503977060318, 'learning_rate': 1.6806098514464427e-05, 'epoch': 1.59859292554231}\nEpoch: 1.6025014657025602, Logs: {'loss': 0.0508, 'grad_norm': 0.16544345021247864, 'learning_rate': 1.67982799061767e-05, 'epoch': 1.6025014657025602}\nEpoch: 1.6064100058628101, Logs: {'loss': 0.0447, 'grad_norm': 0.1666676551103592, 'learning_rate': 1.6790461297888976e-05, 'epoch': 1.6064100058628101}\nEpoch: 1.6103185460230605, Logs: {'loss': 0.0473, 'grad_norm': 0.25860467553138733, 'learning_rate': 1.678264268960125e-05, 'epoch': 1.6103185460230605}\nEpoch: 1.6142270861833106, Logs: {'loss': 0.049, 'grad_norm': 0.19156749546527863, 'learning_rate': 1.6774824081313526e-05, 'epoch': 1.6142270861833106}\nEpoch: 1.6181356263435607, Logs: {'loss': 0.0478, 'grad_norm': 0.2644869387149811, 'learning_rate': 1.6767005473025804e-05, 'epoch': 1.6181356263435607}\nEpoch: 1.6220441665038108, Logs: {'loss': 0.0499, 'grad_norm': 0.7293642163276672, 'learning_rate': 1.6759186864738076e-05, 'epoch': 1.6220441665038108}\nEpoch: 1.6259527066640609, Logs: {'loss': 0.047, 'grad_norm': 0.16965889930725098, 'learning_rate': 1.6751368256450354e-05, 'epoch': 1.6259527066640609}\nEpoch: 1.6298612468243112, Logs: {'loss': 0.0489, 'grad_norm': 0.24036164581775665, 'learning_rate': 1.674354964816263e-05, 'epoch': 1.6298612468243112}\nEpoch: 1.633769786984561, Logs: {'loss': 0.047, 'grad_norm': 0.16515271365642548, 'learning_rate': 1.6735731039874904e-05, 'epoch': 1.633769786984561}\nEpoch: 1.6376783271448114, Logs: {'loss': 0.0444, 'grad_norm': 0.22857016324996948, 'learning_rate': 1.672791243158718e-05, 'epoch': 1.6376783271448114}\nEpoch: 1.6415868673050615, Logs: {'loss': 0.0489, 'grad_norm': 0.1843014359474182, 'learning_rate': 1.6720093823299454e-05, 'epoch': 1.6415868673050615}\nEpoch: 1.6454954074653116, Logs: {'loss': 0.0436, 'grad_norm': 0.14753417670726776, 'learning_rate': 1.671227521501173e-05, 'epoch': 1.6454954074653116}\nEpoch: 1.649403947625562, Logs: {'loss': 0.0437, 'grad_norm': 0.14412829279899597, 'learning_rate': 1.6704456606724003e-05, 'epoch': 1.649403947625562}\nEpoch: 1.6533124877858119, Logs: {'loss': 0.0478, 'grad_norm': 0.1308411806821823, 'learning_rate': 1.669663799843628e-05, 'epoch': 1.6533124877858119}\nEpoch: 1.6572210279460622, Logs: {'loss': 0.0459, 'grad_norm': 0.1549071967601776, 'learning_rate': 1.6688819390148553e-05, 'epoch': 1.6572210279460622}\nEpoch: 1.6611295681063123, Logs: {'loss': 0.051, 'grad_norm': 0.13886681199073792, 'learning_rate': 1.668100078186083e-05, 'epoch': 1.6611295681063123}\nEpoch: 1.6650381082665624, Logs: {'loss': 0.0447, 'grad_norm': 0.2240951806306839, 'learning_rate': 1.6673182173573106e-05, 'epoch': 1.6650381082665624}\nEpoch: 1.6689466484268127, Logs: {'loss': 0.0451, 'grad_norm': 0.1235668957233429, 'learning_rate': 1.666536356528538e-05, 'epoch': 1.6689466484268127}\nEpoch: 1.6728551885870626, Logs: {'loss': 0.0531, 'grad_norm': 0.2927749752998352, 'learning_rate': 1.6657544956997656e-05, 'epoch': 1.6728551885870626}\nEpoch: 1.676763728747313, Logs: {'loss': 0.0476, 'grad_norm': 0.13405215740203857, 'learning_rate': 1.664972634870993e-05, 'epoch': 1.676763728747313}\nEpoch: 1.680672268907563, Logs: {'loss': 0.0492, 'grad_norm': 0.2103651463985443, 'learning_rate': 1.6641907740422206e-05, 'epoch': 1.680672268907563}\nEpoch: 1.6845808090678132, Logs: {'loss': 0.0493, 'grad_norm': 0.3831319808959961, 'learning_rate': 1.663408913213448e-05, 'epoch': 1.6845808090678132}\nEpoch: 1.6884893492280633, Logs: {'loss': 0.0452, 'grad_norm': 0.3555000126361847, 'learning_rate': 1.6626270523846755e-05, 'epoch': 1.6884893492280633}\nEpoch: 1.6923978893883134, Logs: {'loss': 0.044, 'grad_norm': 0.20664365589618683, 'learning_rate': 1.6618451915559034e-05, 'epoch': 1.6923978893883134}\nEpoch: 1.6963064295485637, Logs: {'loss': 0.05, 'grad_norm': 0.10667885839939117, 'learning_rate': 1.6610633307271305e-05, 'epoch': 1.6963064295485637}\nEpoch: 1.7002149697088138, Logs: {'loss': 0.0485, 'grad_norm': 0.19762718677520752, 'learning_rate': 1.6602814698983583e-05, 'epoch': 1.7002149697088138}\nEpoch: 1.704123509869064, Logs: {'loss': 0.0461, 'grad_norm': 0.31521445512771606, 'learning_rate': 1.6594996090695858e-05, 'epoch': 1.704123509869064}\nEpoch: 1.708032050029314, Logs: {'loss': 0.045, 'grad_norm': 0.21907559037208557, 'learning_rate': 1.6587177482408133e-05, 'epoch': 1.708032050029314}\nEpoch: 1.7119405901895641, Logs: {'loss': 0.0475, 'grad_norm': 0.08073779195547104, 'learning_rate': 1.6579358874120408e-05, 'epoch': 1.7119405901895641}\nEpoch: 1.7158491303498145, Logs: {'loss': 0.0507, 'grad_norm': 0.2058311104774475, 'learning_rate': 1.6571540265832683e-05, 'epoch': 1.7158491303498145}\nEpoch: 1.7197576705100643, Logs: {'loss': 0.0491, 'grad_norm': 0.3041594922542572, 'learning_rate': 1.6563721657544958e-05, 'epoch': 1.7197576705100643}\nEpoch: 1.7236662106703147, Logs: {'loss': 0.0461, 'grad_norm': 0.1519266664981842, 'learning_rate': 1.6555903049257233e-05, 'epoch': 1.7236662106703147}\nEpoch: 1.7275747508305648, Logs: {'loss': 0.0568, 'grad_norm': 0.3734934329986572, 'learning_rate': 1.654808444096951e-05, 'epoch': 1.7275747508305648}\nEpoch: 1.7314832909908149, Logs: {'loss': 0.0447, 'grad_norm': 0.29077115654945374, 'learning_rate': 1.6540265832681782e-05, 'epoch': 1.7314832909908149}\nEpoch: 1.7353918311510652, Logs: {'loss': 0.0514, 'grad_norm': 0.29824888706207275, 'learning_rate': 1.653244722439406e-05, 'epoch': 1.7353918311510652}\nEpoch: 1.739300371311315, Logs: {'loss': 0.0524, 'grad_norm': 0.17470130324363708, 'learning_rate': 1.6524628616106335e-05, 'epoch': 1.739300371311315}\nEpoch: 1.7432089114715654, Logs: {'loss': 0.0485, 'grad_norm': 0.11989018321037292, 'learning_rate': 1.651681000781861e-05, 'epoch': 1.7432089114715654}\nEpoch: 1.7471174516318155, Logs: {'loss': 0.048, 'grad_norm': 0.24338020384311676, 'learning_rate': 1.6508991399530885e-05, 'epoch': 1.7471174516318155}\nEpoch: 1.7510259917920656, Logs: {'loss': 0.0455, 'grad_norm': 0.17314687371253967, 'learning_rate': 1.650117279124316e-05, 'epoch': 1.7510259917920656}\nEpoch: 1.754934531952316, Logs: {'loss': 0.0515, 'grad_norm': 0.241906076669693, 'learning_rate': 1.6493354182955435e-05, 'epoch': 1.754934531952316}\nEpoch: 1.7588430721125659, Logs: {'loss': 0.0515, 'grad_norm': 0.1352098137140274, 'learning_rate': 1.648553557466771e-05, 'epoch': 1.7588430721125659}\nEpoch: 1.7627516122728162, Logs: {'loss': 0.0433, 'grad_norm': 0.13109737634658813, 'learning_rate': 1.6477716966379985e-05, 'epoch': 1.7627516122728162}\nEpoch: 1.7666601524330663, Logs: {'loss': 0.0456, 'grad_norm': 0.22122891247272491, 'learning_rate': 1.646989835809226e-05, 'epoch': 1.7666601524330663}\nEpoch: 1.7705686925933164, Logs: {'loss': 0.0471, 'grad_norm': 0.3654952645301819, 'learning_rate': 1.6462079749804534e-05, 'epoch': 1.7705686925933164}\nEpoch: 1.7744772327535665, Logs: {'loss': 0.0466, 'grad_norm': 0.192213773727417, 'learning_rate': 1.6454261141516813e-05, 'epoch': 1.7744772327535665}\nEpoch: 1.7783857729138166, Logs: {'loss': 0.0494, 'grad_norm': 0.2634766697883606, 'learning_rate': 1.6446442533229084e-05, 'epoch': 1.7783857729138166}\nEpoch: 1.782294313074067, Logs: {'loss': 0.0505, 'grad_norm': 0.15431073307991028, 'learning_rate': 1.6438623924941362e-05, 'epoch': 1.782294313074067}\nEpoch: 1.7862028532343168, Logs: {'loss': 0.048, 'grad_norm': 0.12657073140144348, 'learning_rate': 1.6430805316653637e-05, 'epoch': 1.7862028532343168}\nEpoch: 1.7901113933945672, Logs: {'loss': 0.0471, 'grad_norm': 0.19087205827236176, 'learning_rate': 1.6422986708365912e-05, 'epoch': 1.7901113933945672}\nEpoch: 1.7940199335548173, Logs: {'loss': 0.0483, 'grad_norm': 0.14037740230560303, 'learning_rate': 1.6415168100078187e-05, 'epoch': 1.7940199335548173}\nEpoch: 1.7979284737150674, Logs: {'loss': 0.0509, 'grad_norm': 0.21994413435459137, 'learning_rate': 1.6407349491790462e-05, 'epoch': 1.7979284737150674}\nEpoch: 1.8018370138753177, Logs: {'loss': 0.0502, 'grad_norm': 0.19326557219028473, 'learning_rate': 1.639953088350274e-05, 'epoch': 1.8018370138753177}\nEpoch: 1.8057455540355676, Logs: {'loss': 0.0471, 'grad_norm': 0.1621413379907608, 'learning_rate': 1.639171227521501e-05, 'epoch': 1.8057455540355676}\nEpoch: 1.809654094195818, Logs: {'loss': 0.0516, 'grad_norm': 0.1854260265827179, 'learning_rate': 1.638389366692729e-05, 'epoch': 1.809654094195818}\nEpoch: 1.813562634356068, Logs: {'loss': 0.0438, 'grad_norm': 0.12800747156143188, 'learning_rate': 1.6376075058639565e-05, 'epoch': 1.813562634356068}\nEpoch: 1.8174711745163181, Logs: {'loss': 0.0473, 'grad_norm': 0.08490458130836487, 'learning_rate': 1.636825645035184e-05, 'epoch': 1.8174711745163181}\nEpoch: 1.8213797146765685, Logs: {'loss': 0.0465, 'grad_norm': 0.1405174285173416, 'learning_rate': 1.6360437842064114e-05, 'epoch': 1.8213797146765685}\nEpoch: 1.8252882548368183, Logs: {'loss': 0.0462, 'grad_norm': 0.09959588944911957, 'learning_rate': 1.635261923377639e-05, 'epoch': 1.8252882548368183}\nEpoch: 1.8291967949970687, Logs: {'loss': 0.0539, 'grad_norm': 0.24037782847881317, 'learning_rate': 1.6344800625488664e-05, 'epoch': 1.8291967949970687}\nEpoch: 1.8331053351573188, Logs: {'loss': 0.049, 'grad_norm': 0.1813385784626007, 'learning_rate': 1.633698201720094e-05, 'epoch': 1.8331053351573188}\nEpoch: 1.8370138753175689, Logs: {'loss': 0.0453, 'grad_norm': 0.12817613780498505, 'learning_rate': 1.6329163408913217e-05, 'epoch': 1.8370138753175689}\nEpoch: 1.840922415477819, Logs: {'loss': 0.05, 'grad_norm': 0.11751142889261246, 'learning_rate': 1.632134480062549e-05, 'epoch': 1.840922415477819}\nEpoch: 1.844830955638069, Logs: {'loss': 0.0508, 'grad_norm': 0.14621585607528687, 'learning_rate': 1.6313526192337764e-05, 'epoch': 1.844830955638069}\nEpoch: 1.8487394957983194, Logs: {'loss': 0.0451, 'grad_norm': 0.22084885835647583, 'learning_rate': 1.6305707584050042e-05, 'epoch': 1.8487394957983194}\nEpoch: 1.8526480359585695, Logs: {'loss': 0.0441, 'grad_norm': 0.30477210879325867, 'learning_rate': 1.6297888975762313e-05, 'epoch': 1.8526480359585695}\nEpoch: 1.8565565761188196, Logs: {'loss': 0.0529, 'grad_norm': 0.10892746597528458, 'learning_rate': 1.629007036747459e-05, 'epoch': 1.8565565761188196}\nEpoch: 1.8604651162790697, Logs: {'loss': 0.0469, 'grad_norm': 0.16289569437503815, 'learning_rate': 1.6282251759186866e-05, 'epoch': 1.8604651162790697}\nEpoch: 1.8643736564393198, Logs: {'loss': 0.0493, 'grad_norm': 0.21328610181808472, 'learning_rate': 1.627443315089914e-05, 'epoch': 1.8643736564393198}\nEpoch: 1.8682821965995702, Logs: {'loss': 0.049, 'grad_norm': 0.18324518203735352, 'learning_rate': 1.6266614542611416e-05, 'epoch': 1.8682821965995702}\nEpoch: 1.87219073675982, Logs: {'loss': 0.0434, 'grad_norm': 0.14849156141281128, 'learning_rate': 1.625879593432369e-05, 'epoch': 1.87219073675982}\nEpoch: 1.8760992769200704, Logs: {'loss': 0.0463, 'grad_norm': 0.15628822147846222, 'learning_rate': 1.6250977326035966e-05, 'epoch': 1.8760992769200704}\nEpoch: 1.8800078170803205, Logs: {'loss': 0.0474, 'grad_norm': 0.13825805485248566, 'learning_rate': 1.624315871774824e-05, 'epoch': 1.8800078170803205}\nEpoch: 1.8839163572405706, Logs: {'loss': 0.0523, 'grad_norm': 0.0886201336979866, 'learning_rate': 1.623534010946052e-05, 'epoch': 1.8839163572405706}\nEpoch: 1.887824897400821, Logs: {'loss': 0.0461, 'grad_norm': 0.20518624782562256, 'learning_rate': 1.622752150117279e-05, 'epoch': 1.887824897400821}\nEpoch: 1.8917334375610708, Logs: {'loss': 0.0459, 'grad_norm': 0.1312674582004547, 'learning_rate': 1.621970289288507e-05, 'epoch': 1.8917334375610708}\nEpoch: 1.8956419777213211, Logs: {'loss': 0.0451, 'grad_norm': 0.20124760270118713, 'learning_rate': 1.6211884284597344e-05, 'epoch': 1.8956419777213211}\nEpoch: 1.8995505178815713, Logs: {'loss': 0.0451, 'grad_norm': 0.23386195302009583, 'learning_rate': 1.620406567630962e-05, 'epoch': 1.8995505178815713}\nEpoch: 1.9034590580418214, Logs: {'loss': 0.0482, 'grad_norm': 0.12874583899974823, 'learning_rate': 1.6196247068021893e-05, 'epoch': 1.9034590580418214}\nEpoch: 1.9073675982020717, Logs: {'loss': 0.0474, 'grad_norm': 0.24095657467842102, 'learning_rate': 1.6188428459734168e-05, 'epoch': 1.9073675982020717}\nEpoch: 1.9112761383623216, Logs: {'loss': 0.0488, 'grad_norm': 0.09247244149446487, 'learning_rate': 1.6180609851446446e-05, 'epoch': 1.9112761383623216}\nEpoch: 1.915184678522572, Logs: {'loss': 0.0509, 'grad_norm': 0.12232329696416855, 'learning_rate': 1.6172791243158718e-05, 'epoch': 1.915184678522572}\nEpoch: 1.919093218682822, Logs: {'loss': 0.0451, 'grad_norm': 0.11434879153966904, 'learning_rate': 1.6164972634870996e-05, 'epoch': 1.919093218682822}\nEpoch: 1.9230017588430721, Logs: {'loss': 0.049, 'grad_norm': 0.16670207679271698, 'learning_rate': 1.615715402658327e-05, 'epoch': 1.9230017588430721}\nEpoch: 1.9269102990033222, Logs: {'loss': 0.0495, 'grad_norm': 0.14607109129428864, 'learning_rate': 1.6149335418295543e-05, 'epoch': 1.9269102990033222}\nEpoch: 1.9308188391635723, Logs: {'loss': 0.0423, 'grad_norm': 0.21762660145759583, 'learning_rate': 1.614151681000782e-05, 'epoch': 1.9308188391635723}\nEpoch: 1.9347273793238227, Logs: {'loss': 0.0475, 'grad_norm': 0.17842452228069305, 'learning_rate': 1.6133698201720096e-05, 'epoch': 1.9347273793238227}\nEpoch: 1.9386359194840725, Logs: {'loss': 0.0439, 'grad_norm': 0.17665426433086395, 'learning_rate': 1.612587959343237e-05, 'epoch': 1.9386359194840725}\nEpoch: 1.9425444596443229, Logs: {'loss': 0.0478, 'grad_norm': 0.1068093553185463, 'learning_rate': 1.6118060985144645e-05, 'epoch': 1.9425444596443229}\nEpoch: 1.946452999804573, Logs: {'loss': 0.0438, 'grad_norm': 0.17077620327472687, 'learning_rate': 1.611024237685692e-05, 'epoch': 1.946452999804573}\nEpoch: 1.950361539964823, Logs: {'loss': 0.049, 'grad_norm': 0.4227737784385681, 'learning_rate': 1.6102423768569195e-05, 'epoch': 1.950361539964823}\nEpoch: 1.9542700801250734, Logs: {'loss': 0.0482, 'grad_norm': 0.23926156759262085, 'learning_rate': 1.609460516028147e-05, 'epoch': 1.9542700801250734}\nEpoch: 1.9581786202853233, Logs: {'loss': 0.0487, 'grad_norm': 0.1271193027496338, 'learning_rate': 1.6086786551993748e-05, 'epoch': 1.9581786202853233}\nEpoch: 1.9620871604455736, Logs: {'loss': 0.0482, 'grad_norm': 0.15115255117416382, 'learning_rate': 1.607896794370602e-05, 'epoch': 1.9620871604455736}\nEpoch: 1.9659957006058237, Logs: {'loss': 0.0474, 'grad_norm': 0.33987122774124146, 'learning_rate': 1.6071149335418298e-05, 'epoch': 1.9659957006058237}\nEpoch: 1.9699042407660738, Logs: {'loss': 0.0446, 'grad_norm': 0.20671352744102478, 'learning_rate': 1.6063330727130573e-05, 'epoch': 1.9699042407660738}\nEpoch: 1.9738127809263242, Logs: {'loss': 0.0436, 'grad_norm': 0.23887448012828827, 'learning_rate': 1.6055512118842848e-05, 'epoch': 1.9738127809263242}\nEpoch: 1.977721321086574, Logs: {'loss': 0.0494, 'grad_norm': 0.17799361050128937, 'learning_rate': 1.6047693510555123e-05, 'epoch': 1.977721321086574}\nEpoch: 1.9816298612468244, Logs: {'loss': 0.0449, 'grad_norm': 0.19248344004154205, 'learning_rate': 1.6039874902267397e-05, 'epoch': 1.9816298612468244}\nEpoch: 1.9855384014070745, Logs: {'loss': 0.0466, 'grad_norm': 0.2741895318031311, 'learning_rate': 1.6032056293979672e-05, 'epoch': 1.9855384014070745}\nEpoch: 1.9894469415673246, Logs: {'loss': 0.0478, 'grad_norm': 0.3685642182826996, 'learning_rate': 1.6024237685691947e-05, 'epoch': 1.9894469415673246}\nEpoch: 1.9933554817275747, Logs: {'loss': 0.0495, 'grad_norm': 0.3897703289985657, 'learning_rate': 1.6016419077404225e-05, 'epoch': 1.9933554817275747}\nEpoch: 1.9972640218878248, Logs: {'loss': 0.0541, 'grad_norm': 0.24198880791664124, 'learning_rate': 1.6008600469116497e-05, 'epoch': 1.9972640218878248}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2.0, Logs: {'eval_loss': 0.04548591747879982, 'eval_bleu': 2.906606336236866e-12, 'eval_accuracy': 0.943950351837373, 'eval_runtime': 349.7972, 'eval_samples_per_second': 58.502, 'eval_steps_per_second': 3.656, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2.001172562048075, Logs: {'loss': 0.0572, 'grad_norm': 0.2159370630979538, 'learning_rate': 1.6000781860828775e-05, 'epoch': 2.001172562048075}\nEpoch: 2.005081102208325, Logs: {'loss': 0.0488, 'grad_norm': 0.11778285354375839, 'learning_rate': 1.599296325254105e-05, 'epoch': 2.005081102208325}\nEpoch: 2.0089896423685754, Logs: {'loss': 0.0451, 'grad_norm': 0.17659012973308563, 'learning_rate': 1.598514464425332e-05, 'epoch': 2.0089896423685754}\nEpoch: 2.0128981825288257, Logs: {'loss': 0.0447, 'grad_norm': 0.15343321859836578, 'learning_rate': 1.59773260359656e-05, 'epoch': 2.0128981825288257}\nEpoch: 2.0168067226890756, Logs: {'loss': 0.0478, 'grad_norm': 0.12053137272596359, 'learning_rate': 1.5969507427677875e-05, 'epoch': 2.0168067226890756}\nEpoch: 2.020715262849326, Logs: {'loss': 0.0487, 'grad_norm': 0.13575226068496704, 'learning_rate': 1.596168881939015e-05, 'epoch': 2.020715262849326}\nEpoch: 2.024623803009576, Logs: {'loss': 0.0495, 'grad_norm': 0.13997161388397217, 'learning_rate': 1.5953870211102424e-05, 'epoch': 2.024623803009576}\nEpoch: 2.028532343169826, Logs: {'loss': 0.0461, 'grad_norm': 0.24984805285930634, 'learning_rate': 1.59460516028147e-05, 'epoch': 2.028532343169826}\nEpoch: 2.032440883330076, Logs: {'loss': 0.0448, 'grad_norm': 0.11323874443769455, 'learning_rate': 1.5938232994526977e-05, 'epoch': 2.032440883330076}\nEpoch: 2.0363494234903263, Logs: {'loss': 0.0442, 'grad_norm': 0.34789910912513733, 'learning_rate': 1.593041438623925e-05, 'epoch': 2.0363494234903263}\nEpoch: 2.0402579636505767, Logs: {'loss': 0.0517, 'grad_norm': 0.3711719512939453, 'learning_rate': 1.5922595777951527e-05, 'epoch': 2.0402579636505767}\nEpoch: 2.0441665038108265, Logs: {'loss': 0.0533, 'grad_norm': 0.16272428631782532, 'learning_rate': 1.5914777169663802e-05, 'epoch': 2.0441665038108265}\nEpoch: 2.048075043971077, Logs: {'loss': 0.0451, 'grad_norm': 0.12240488082170486, 'learning_rate': 1.5906958561376077e-05, 'epoch': 2.048075043971077}\nEpoch: 2.0519835841313268, Logs: {'loss': 0.0495, 'grad_norm': 0.12760566174983978, 'learning_rate': 1.5899139953088352e-05, 'epoch': 2.0519835841313268}\nEpoch: 2.055892124291577, Logs: {'loss': 0.0465, 'grad_norm': 0.5933309197425842, 'learning_rate': 1.5891321344800627e-05, 'epoch': 2.055892124291577}\nEpoch: 2.0598006644518274, Logs: {'loss': 0.0492, 'grad_norm': 0.19717828929424286, 'learning_rate': 1.58835027365129e-05, 'epoch': 2.0598006644518274}\nEpoch: 2.0637092046120773, Logs: {'loss': 0.0431, 'grad_norm': 0.17952078580856323, 'learning_rate': 1.5875684128225176e-05, 'epoch': 2.0637092046120773}\nEpoch: 2.0676177447723276, Logs: {'loss': 0.0475, 'grad_norm': 0.19199435412883759, 'learning_rate': 1.5867865519937455e-05, 'epoch': 2.0676177447723276}\nEpoch: 2.0715262849325775, Logs: {'loss': 0.0442, 'grad_norm': 0.1639445424079895, 'learning_rate': 1.5860046911649726e-05, 'epoch': 2.0715262849325775}\nEpoch: 2.075434825092828, Logs: {'loss': 0.0505, 'grad_norm': 0.2609572410583496, 'learning_rate': 1.5852228303362004e-05, 'epoch': 2.075434825092828}\nEpoch: 2.079343365253078, Logs: {'loss': 0.0552, 'grad_norm': 0.11807336658239365, 'learning_rate': 1.584440969507428e-05, 'epoch': 2.079343365253078}\nEpoch: 2.083251905413328, Logs: {'loss': 0.0505, 'grad_norm': 0.07856093347072601, 'learning_rate': 1.583659108678655e-05, 'epoch': 2.083251905413328}\nEpoch: 2.0871604455735784, Logs: {'loss': 0.0423, 'grad_norm': 0.07446620613336563, 'learning_rate': 1.582877247849883e-05, 'epoch': 2.0871604455735784}\nEpoch: 2.0910689857338283, Logs: {'loss': 0.0416, 'grad_norm': 0.08754561841487885, 'learning_rate': 1.5820953870211104e-05, 'epoch': 2.0910689857338283}\nEpoch: 2.0949775258940786, Logs: {'loss': 0.0433, 'grad_norm': 0.12014732509851456, 'learning_rate': 1.581313526192338e-05, 'epoch': 2.0949775258940786}\nEpoch: 2.0988860660543285, Logs: {'loss': 0.0496, 'grad_norm': 0.23019464313983917, 'learning_rate': 1.5805316653635654e-05, 'epoch': 2.0988860660543285}\nEpoch: 2.102794606214579, Logs: {'loss': 0.0503, 'grad_norm': 0.17770949006080627, 'learning_rate': 1.579749804534793e-05, 'epoch': 2.102794606214579}\nEpoch: 2.106703146374829, Logs: {'loss': 0.043, 'grad_norm': 0.12116004526615143, 'learning_rate': 1.5789679437060203e-05, 'epoch': 2.106703146374829}\nEpoch: 2.110611686535079, Logs: {'loss': 0.0499, 'grad_norm': 0.12395736575126648, 'learning_rate': 1.5781860828772478e-05, 'epoch': 2.110611686535079}\nEpoch: 2.1145202266953294, Logs: {'loss': 0.0507, 'grad_norm': 0.14467941224575043, 'learning_rate': 1.5774042220484756e-05, 'epoch': 2.1145202266953294}\nEpoch: 2.1184287668555792, Logs: {'loss': 0.0478, 'grad_norm': 0.13909529149532318, 'learning_rate': 1.5766223612197028e-05, 'epoch': 2.1184287668555792}\nEpoch: 2.1223373070158296, Logs: {'loss': 0.049, 'grad_norm': 0.18624404072761536, 'learning_rate': 1.5758405003909306e-05, 'epoch': 2.1223373070158296}\nEpoch: 2.12624584717608, Logs: {'loss': 0.0472, 'grad_norm': 0.1004026010632515, 'learning_rate': 1.575058639562158e-05, 'epoch': 2.12624584717608}\nEpoch: 2.13015438733633, Logs: {'loss': 0.0438, 'grad_norm': 0.19011135399341583, 'learning_rate': 1.5742767787333856e-05, 'epoch': 2.13015438733633}\nEpoch: 2.13406292749658, Logs: {'loss': 0.0469, 'grad_norm': 0.14161112904548645, 'learning_rate': 1.573494917904613e-05, 'epoch': 2.13406292749658}\nEpoch: 2.13797146765683, Logs: {'loss': 0.0456, 'grad_norm': 0.3726438283920288, 'learning_rate': 1.5727130570758406e-05, 'epoch': 2.13797146765683}\nEpoch: 2.1418800078170803, Logs: {'loss': 0.052, 'grad_norm': 0.2951350510120392, 'learning_rate': 1.5719311962470684e-05, 'epoch': 2.1418800078170803}\nEpoch: 2.1457885479773307, Logs: {'loss': 0.0446, 'grad_norm': 0.25809982419013977, 'learning_rate': 1.5711493354182955e-05, 'epoch': 2.1457885479773307}\nEpoch: 2.1496970881375805, Logs: {'loss': 0.0464, 'grad_norm': 0.10903880000114441, 'learning_rate': 1.5703674745895234e-05, 'epoch': 2.1496970881375805}\nEpoch: 2.153605628297831, Logs: {'loss': 0.0443, 'grad_norm': 0.12378698587417603, 'learning_rate': 1.569585613760751e-05, 'epoch': 2.153605628297831}\nEpoch: 2.1575141684580807, Logs: {'loss': 0.0433, 'grad_norm': 0.5684053897857666, 'learning_rate': 1.5688037529319783e-05, 'epoch': 2.1575141684580807}\nEpoch: 2.161422708618331, Logs: {'loss': 0.0525, 'grad_norm': 0.13262183964252472, 'learning_rate': 1.5680218921032058e-05, 'epoch': 2.161422708618331}\nEpoch: 2.1653312487785814, Logs: {'loss': 0.0505, 'grad_norm': 0.2576124668121338, 'learning_rate': 1.5672400312744333e-05, 'epoch': 2.1653312487785814}\nEpoch: 2.1692397889388313, Logs: {'loss': 0.057, 'grad_norm': 0.6773712038993835, 'learning_rate': 1.5664581704456608e-05, 'epoch': 2.1692397889388313}\nEpoch: 2.1731483290990816, Logs: {'loss': 0.0462, 'grad_norm': 0.13030844926834106, 'learning_rate': 1.5656763096168883e-05, 'epoch': 2.1731483290990816}\nEpoch: 2.1770568692593315, Logs: {'loss': 0.0412, 'grad_norm': 0.1402483731508255, 'learning_rate': 1.5648944487881158e-05, 'epoch': 2.1770568692593315}\nEpoch: 2.180965409419582, Logs: {'loss': 0.0502, 'grad_norm': 0.3668361008167267, 'learning_rate': 1.5641125879593433e-05, 'epoch': 2.180965409419582}\nEpoch: 2.184873949579832, Logs: {'loss': 0.0471, 'grad_norm': 0.1763053983449936, 'learning_rate': 1.5633307271305707e-05, 'epoch': 2.184873949579832}\nEpoch: 2.188782489740082, Logs: {'loss': 0.0461, 'grad_norm': 0.1674516201019287, 'learning_rate': 1.5625488663017986e-05, 'epoch': 2.188782489740082}\nEpoch: 2.1926910299003324, Logs: {'loss': 0.0474, 'grad_norm': 0.12209779024124146, 'learning_rate': 1.5617670054730257e-05, 'epoch': 2.1926910299003324}\nEpoch: 2.1965995700605823, Logs: {'loss': 0.0437, 'grad_norm': 0.18607965111732483, 'learning_rate': 1.5609851446442535e-05, 'epoch': 2.1965995700605823}\nEpoch: 2.2005081102208326, Logs: {'loss': 0.0488, 'grad_norm': 0.21903486549854279, 'learning_rate': 1.560203283815481e-05, 'epoch': 2.2005081102208326}\nEpoch: 2.2044166503810825, Logs: {'loss': 0.0481, 'grad_norm': 0.31026819348335266, 'learning_rate': 1.5594214229867085e-05, 'epoch': 2.2044166503810825}\nEpoch: 2.208325190541333, Logs: {'loss': 0.0439, 'grad_norm': 0.19506196677684784, 'learning_rate': 1.558639562157936e-05, 'epoch': 2.208325190541333}\nEpoch: 2.212233730701583, Logs: {'loss': 0.0491, 'grad_norm': 0.17670758068561554, 'learning_rate': 1.5578577013291635e-05, 'epoch': 2.212233730701583}\nEpoch: 2.216142270861833, Logs: {'loss': 0.0448, 'grad_norm': 0.2783883213996887, 'learning_rate': 1.557075840500391e-05, 'epoch': 2.216142270861833}\nEpoch: 2.2200508110220833, Logs: {'loss': 0.0416, 'grad_norm': 0.19095931947231293, 'learning_rate': 1.5562939796716185e-05, 'epoch': 2.2200508110220833}\nEpoch: 2.2239593511823332, Logs: {'loss': 0.0442, 'grad_norm': 0.25108206272125244, 'learning_rate': 1.5555121188428463e-05, 'epoch': 2.2239593511823332}\nEpoch: 2.2278678913425836, Logs: {'loss': 0.0406, 'grad_norm': 0.20999978482723236, 'learning_rate': 1.5547302580140734e-05, 'epoch': 2.2278678913425836}\nEpoch: 2.231776431502834, Logs: {'loss': 0.0455, 'grad_norm': 0.19489522278308868, 'learning_rate': 1.5539483971853013e-05, 'epoch': 2.231776431502834}\nEpoch: 2.2356849716630838, Logs: {'loss': 0.0409, 'grad_norm': 0.12600667774677277, 'learning_rate': 1.5531665363565287e-05, 'epoch': 2.2356849716630838}\nEpoch: 2.239593511823334, Logs: {'loss': 0.0464, 'grad_norm': 0.2906225621700287, 'learning_rate': 1.5523846755277562e-05, 'epoch': 2.239593511823334}\nEpoch: 2.243502051983584, Logs: {'loss': 0.0418, 'grad_norm': 0.10932184010744095, 'learning_rate': 1.5516028146989837e-05, 'epoch': 2.243502051983584}\nEpoch: 2.2474105921438343, Logs: {'loss': 0.0481, 'grad_norm': 0.2113238126039505, 'learning_rate': 1.5508209538702112e-05, 'epoch': 2.2474105921438343}\nEpoch: 2.2513191323040846, Logs: {'loss': 0.0406, 'grad_norm': 0.212529718875885, 'learning_rate': 1.5500390930414387e-05, 'epoch': 2.2513191323040846}\nEpoch: 2.2552276724643345, Logs: {'loss': 0.0461, 'grad_norm': 0.12127645313739777, 'learning_rate': 1.5492572322126662e-05, 'epoch': 2.2552276724643345}\nEpoch: 2.259136212624585, Logs: {'loss': 0.0467, 'grad_norm': 0.19502578675746918, 'learning_rate': 1.5484753713838937e-05, 'epoch': 2.259136212624585}\nEpoch: 2.2630447527848347, Logs: {'loss': 0.0428, 'grad_norm': 0.13006652891635895, 'learning_rate': 1.5476935105551215e-05, 'epoch': 2.2630447527848347}\nEpoch: 2.266953292945085, Logs: {'loss': 0.0438, 'grad_norm': 0.3137853443622589, 'learning_rate': 1.5469116497263486e-05, 'epoch': 2.266953292945085}\nEpoch: 2.270861833105335, Logs: {'loss': 0.0411, 'grad_norm': 0.1886616200208664, 'learning_rate': 1.5461297888975765e-05, 'epoch': 2.270861833105335}\nEpoch: 2.2747703732655853, Logs: {'loss': 0.0439, 'grad_norm': 0.20969431102275848, 'learning_rate': 1.545347928068804e-05, 'epoch': 2.2747703732655853}\nEpoch: 2.2786789134258356, Logs: {'loss': 0.0471, 'grad_norm': 0.13828027248382568, 'learning_rate': 1.5445660672400314e-05, 'epoch': 2.2786789134258356}\nEpoch: 2.2825874535860855, Logs: {'loss': 0.0429, 'grad_norm': 0.17950992286205292, 'learning_rate': 1.543784206411259e-05, 'epoch': 2.2825874535860855}\nEpoch: 2.286495993746336, Logs: {'loss': 0.0439, 'grad_norm': 0.26078474521636963, 'learning_rate': 1.5430023455824864e-05, 'epoch': 2.286495993746336}\nEpoch: 2.2904045339065857, Logs: {'loss': 0.039, 'grad_norm': 0.39778316020965576, 'learning_rate': 1.542220484753714e-05, 'epoch': 2.2904045339065857}\nEpoch: 2.294313074066836, Logs: {'loss': 0.0455, 'grad_norm': 0.11336517333984375, 'learning_rate': 1.5414386239249414e-05, 'epoch': 2.294313074066836}\nEpoch: 2.2982216142270864, Logs: {'loss': 0.0486, 'grad_norm': 0.28365105390548706, 'learning_rate': 1.5406567630961692e-05, 'epoch': 2.2982216142270864}\nEpoch: 2.3021301543873363, Logs: {'loss': 0.0529, 'grad_norm': 0.1252690702676773, 'learning_rate': 1.5398749022673964e-05, 'epoch': 2.3021301543873363}\nEpoch: 2.3060386945475866, Logs: {'loss': 0.0453, 'grad_norm': 0.20355413854122162, 'learning_rate': 1.5390930414386242e-05, 'epoch': 2.3060386945475866}\nEpoch: 2.3099472347078365, Logs: {'loss': 0.0525, 'grad_norm': 0.14820991456508636, 'learning_rate': 1.5383111806098517e-05, 'epoch': 2.3099472347078365}\nEpoch: 2.313855774868087, Logs: {'loss': 0.0426, 'grad_norm': 0.19398586452007294, 'learning_rate': 1.537529319781079e-05, 'epoch': 2.313855774868087}\nEpoch: 2.317764315028337, Logs: {'loss': 0.0444, 'grad_norm': 0.13876934349536896, 'learning_rate': 1.5367474589523066e-05, 'epoch': 2.317764315028337}\nEpoch: 2.321672855188587, Logs: {'loss': 0.0491, 'grad_norm': 0.13402323424816132, 'learning_rate': 1.535965598123534e-05, 'epoch': 2.321672855188587}\nEpoch: 2.3255813953488373, Logs: {'loss': 0.0472, 'grad_norm': 0.1460108608007431, 'learning_rate': 1.5351837372947616e-05, 'epoch': 2.3255813953488373}\nEpoch: 2.3294899355090872, Logs: {'loss': 0.047, 'grad_norm': 0.1184716448187828, 'learning_rate': 1.534401876465989e-05, 'epoch': 2.3294899355090872}\nEpoch: 2.3333984756693376, Logs: {'loss': 0.0422, 'grad_norm': 0.26272302865982056, 'learning_rate': 1.5336200156372166e-05, 'epoch': 2.3333984756693376}\nEpoch: 2.3373070158295874, Logs: {'loss': 0.0471, 'grad_norm': 0.09902969002723694, 'learning_rate': 1.532838154808444e-05, 'epoch': 2.3373070158295874}\nEpoch: 2.3412155559898378, Logs: {'loss': 0.0461, 'grad_norm': 0.21022529900074005, 'learning_rate': 1.5320562939796716e-05, 'epoch': 2.3412155559898378}\nEpoch: 2.345124096150088, Logs: {'loss': 0.0428, 'grad_norm': 0.5785740613937378, 'learning_rate': 1.5312744331508994e-05, 'epoch': 2.345124096150088}\nEpoch: 2.349032636310338, Logs: {'loss': 0.0442, 'grad_norm': 0.18881511688232422, 'learning_rate': 1.530492572322127e-05, 'epoch': 2.349032636310338}\nEpoch: 2.3529411764705883, Logs: {'loss': 0.0491, 'grad_norm': 0.1505739986896515, 'learning_rate': 1.5297107114933544e-05, 'epoch': 2.3529411764705883}\nEpoch: 2.356849716630838, Logs: {'loss': 0.0433, 'grad_norm': 0.14014936983585358, 'learning_rate': 1.528928850664582e-05, 'epoch': 2.356849716630838}\nEpoch: 2.3607582567910885, Logs: {'loss': 0.0482, 'grad_norm': 0.7176768779754639, 'learning_rate': 1.5281469898358093e-05, 'epoch': 2.3607582567910885}\nEpoch: 2.364666796951339, Logs: {'loss': 0.0503, 'grad_norm': 0.6628318428993225, 'learning_rate': 1.5273651290070368e-05, 'epoch': 2.364666796951339}\nEpoch: 2.3685753371115887, Logs: {'loss': 0.0524, 'grad_norm': 0.09391767531633377, 'learning_rate': 1.5265832681782643e-05, 'epoch': 2.3685753371115887}\nEpoch: 2.372483877271839, Logs: {'loss': 0.044, 'grad_norm': 0.26617833971977234, 'learning_rate': 1.525801407349492e-05, 'epoch': 2.372483877271839}\nEpoch: 2.376392417432089, Logs: {'loss': 0.0412, 'grad_norm': 0.26311734318733215, 'learning_rate': 1.5250195465207195e-05, 'epoch': 2.376392417432089}\nEpoch: 2.3803009575923393, Logs: {'loss': 0.0419, 'grad_norm': 0.18907752633094788, 'learning_rate': 1.524237685691947e-05, 'epoch': 2.3803009575923393}\nEpoch: 2.3842094977525896, Logs: {'loss': 0.0431, 'grad_norm': 0.43551042675971985, 'learning_rate': 1.5234558248631746e-05, 'epoch': 2.3842094977525896}\nEpoch: 2.3881180379128395, Logs: {'loss': 0.0482, 'grad_norm': 0.12201104313135147, 'learning_rate': 1.5226739640344019e-05, 'epoch': 2.3881180379128395}\nEpoch: 2.39202657807309, Logs: {'loss': 0.0527, 'grad_norm': 0.2147703915834427, 'learning_rate': 1.5218921032056296e-05, 'epoch': 2.39202657807309}\nEpoch: 2.3959351182333397, Logs: {'loss': 0.0437, 'grad_norm': 0.1251133382320404, 'learning_rate': 1.521110242376857e-05, 'epoch': 2.3959351182333397}\nEpoch: 2.39984365839359, Logs: {'loss': 0.0498, 'grad_norm': 0.14697270095348358, 'learning_rate': 1.5203283815480845e-05, 'epoch': 2.39984365839359}\nEpoch: 2.40375219855384, Logs: {'loss': 0.0489, 'grad_norm': 0.21787256002426147, 'learning_rate': 1.519546520719312e-05, 'epoch': 2.40375219855384}\nEpoch: 2.4076607387140903, Logs: {'loss': 0.0575, 'grad_norm': 0.38190266489982605, 'learning_rate': 1.5187646598905397e-05, 'epoch': 2.4076607387140903}\nEpoch: 2.4115692788743406, Logs: {'loss': 0.0575, 'grad_norm': 0.11822143197059631, 'learning_rate': 1.517982799061767e-05, 'epoch': 2.4115692788743406}\nEpoch: 2.4154778190345905, Logs: {'loss': 0.0463, 'grad_norm': 0.2671492397785187, 'learning_rate': 1.5172009382329947e-05, 'epoch': 2.4154778190345905}\nEpoch: 2.419386359194841, Logs: {'loss': 0.0423, 'grad_norm': 0.13153477013111115, 'learning_rate': 1.5164190774042223e-05, 'epoch': 2.419386359194841}\nEpoch: 2.4232948993550907, Logs: {'loss': 0.0422, 'grad_norm': 0.1576082706451416, 'learning_rate': 1.5156372165754496e-05, 'epoch': 2.4232948993550907}\nEpoch: 2.427203439515341, Logs: {'loss': 0.0441, 'grad_norm': 0.1739480048418045, 'learning_rate': 1.5148553557466771e-05, 'epoch': 2.427203439515341}\nEpoch: 2.4311119796755913, Logs: {'loss': 0.0426, 'grad_norm': 0.1498243510723114, 'learning_rate': 1.5140734949179048e-05, 'epoch': 2.4311119796755913}\nEpoch: 2.4350205198358412, Logs: {'loss': 0.0436, 'grad_norm': 0.1689714789390564, 'learning_rate': 1.5132916340891321e-05, 'epoch': 2.4350205198358412}\nEpoch: 2.4389290599960916, Logs: {'loss': 0.0425, 'grad_norm': 0.1268755942583084, 'learning_rate': 1.5125097732603597e-05, 'epoch': 2.4389290599960916}\nEpoch: 2.4428376001563414, Logs: {'loss': 0.0428, 'grad_norm': 0.27519750595092773, 'learning_rate': 1.5117279124315874e-05, 'epoch': 2.4428376001563414}\nEpoch: 2.4467461403165918, Logs: {'loss': 0.0452, 'grad_norm': 0.12959641218185425, 'learning_rate': 1.5109460516028147e-05, 'epoch': 2.4467461403165918}\nEpoch: 2.450654680476842, Logs: {'loss': 0.0429, 'grad_norm': 0.13062092661857605, 'learning_rate': 1.5101641907740424e-05, 'epoch': 2.450654680476842}\nEpoch: 2.454563220637092, Logs: {'loss': 0.0444, 'grad_norm': 0.3362739682197571, 'learning_rate': 1.5093823299452699e-05, 'epoch': 2.454563220637092}\nEpoch: 2.4584717607973423, Logs: {'loss': 0.0466, 'grad_norm': 0.16862671077251434, 'learning_rate': 1.5086004691164975e-05, 'epoch': 2.4584717607973423}\nEpoch: 2.462380300957592, Logs: {'loss': 0.0498, 'grad_norm': 0.23553332686424255, 'learning_rate': 1.5078186082877248e-05, 'epoch': 2.462380300957592}\nEpoch: 2.4662888411178425, Logs: {'loss': 0.0459, 'grad_norm': 0.17387866973876953, 'learning_rate': 1.5070367474589525e-05, 'epoch': 2.4662888411178425}\nEpoch: 2.4701973812780924, Logs: {'loss': 0.0495, 'grad_norm': 0.1705721616744995, 'learning_rate': 1.50625488663018e-05, 'epoch': 2.4701973812780924}\nEpoch: 2.4741059214383427, Logs: {'loss': 0.0421, 'grad_norm': 0.1407926231622696, 'learning_rate': 1.5054730258014075e-05, 'epoch': 2.4741059214383427}\nEpoch: 2.478014461598593, Logs: {'loss': 0.0485, 'grad_norm': 0.15993937849998474, 'learning_rate': 1.504691164972635e-05, 'epoch': 2.478014461598593}\nEpoch: 2.481923001758843, Logs: {'loss': 0.045, 'grad_norm': 0.36973321437835693, 'learning_rate': 1.5039093041438626e-05, 'epoch': 2.481923001758843}\nEpoch: 2.4858315419190933, Logs: {'loss': 0.0412, 'grad_norm': 0.33303573727607727, 'learning_rate': 1.50312744331509e-05, 'epoch': 2.4858315419190933}\nEpoch: 2.4897400820793436, Logs: {'loss': 0.0441, 'grad_norm': 0.264081209897995, 'learning_rate': 1.5023455824863176e-05, 'epoch': 2.4897400820793436}\nEpoch: 2.4936486222395935, Logs: {'loss': 0.0469, 'grad_norm': 0.4137026071548462, 'learning_rate': 1.5015637216575452e-05, 'epoch': 2.4936486222395935}\nEpoch: 2.497557162399844, Logs: {'loss': 0.0439, 'grad_norm': 0.17527645826339722, 'learning_rate': 1.5007818608287726e-05, 'epoch': 2.497557162399844}\nEpoch: 2.5014657025600937, Logs: {'loss': 0.0448, 'grad_norm': 0.16157972812652588, 'learning_rate': 1.5000000000000002e-05, 'epoch': 2.5014657025600937}\nEpoch: 2.505374242720344, Logs: {'loss': 0.0441, 'grad_norm': 0.1581769734621048, 'learning_rate': 1.4992181391712277e-05, 'epoch': 2.505374242720344}\nEpoch: 2.509282782880594, Logs: {'loss': 0.0474, 'grad_norm': 0.1694185882806778, 'learning_rate': 1.498436278342455e-05, 'epoch': 2.509282782880594}\nEpoch: 2.5131913230408442, Logs: {'loss': 0.0485, 'grad_norm': 0.20582863688468933, 'learning_rate': 1.4976544175136827e-05, 'epoch': 2.5131913230408442}\nEpoch: 2.5170998632010946, Logs: {'loss': 0.0521, 'grad_norm': 1.044708013534546, 'learning_rate': 1.4968725566849103e-05, 'epoch': 2.5170998632010946}\nEpoch: 2.5210084033613445, Logs: {'loss': 0.0404, 'grad_norm': 0.07644505798816681, 'learning_rate': 1.4960906958561376e-05, 'epoch': 2.5210084033613445}\nEpoch: 2.524916943521595, Logs: {'loss': 0.0453, 'grad_norm': 0.15889707207679749, 'learning_rate': 1.4953088350273653e-05, 'epoch': 2.524916943521595}\nEpoch: 2.5288254836818447, Logs: {'loss': 0.0447, 'grad_norm': 0.17946520447731018, 'learning_rate': 1.4945269741985928e-05, 'epoch': 2.5288254836818447}\nEpoch: 2.532734023842095, Logs: {'loss': 0.0475, 'grad_norm': 0.12830618023872375, 'learning_rate': 1.4937451133698203e-05, 'epoch': 2.532734023842095}\nEpoch: 2.536642564002345, Logs: {'loss': 0.0493, 'grad_norm': 0.19309401512145996, 'learning_rate': 1.4929632525410478e-05, 'epoch': 2.536642564002345}\nEpoch: 2.540551104162595, Logs: {'loss': 0.0498, 'grad_norm': 0.21561281383037567, 'learning_rate': 1.4921813917122754e-05, 'epoch': 2.540551104162595}\nEpoch: 2.5444596443228455, Logs: {'loss': 0.0463, 'grad_norm': 0.2208581268787384, 'learning_rate': 1.4913995308835027e-05, 'epoch': 2.5444596443228455}\nEpoch: 2.5483681844830954, Logs: {'loss': 0.0443, 'grad_norm': 0.2583093047142029, 'learning_rate': 1.4906176700547304e-05, 'epoch': 2.5483681844830954}\nEpoch: 2.5522767246433458, Logs: {'loss': 0.0454, 'grad_norm': 0.09381739050149918, 'learning_rate': 1.4898358092259579e-05, 'epoch': 2.5522767246433458}\nEpoch: 2.556185264803596, Logs: {'loss': 0.045, 'grad_norm': 0.10541673749685287, 'learning_rate': 1.4890539483971855e-05, 'epoch': 2.556185264803596}\nEpoch: 2.560093804963846, Logs: {'loss': 0.045, 'grad_norm': 0.09871795773506165, 'learning_rate': 1.4882720875684128e-05, 'epoch': 2.560093804963846}\nEpoch: 2.5640023451240963, Logs: {'loss': 0.0466, 'grad_norm': 0.1479853391647339, 'learning_rate': 1.4874902267396405e-05, 'epoch': 2.5640023451240963}\nEpoch: 2.567910885284346, Logs: {'loss': 0.0448, 'grad_norm': 0.19603431224822998, 'learning_rate': 1.4867083659108682e-05, 'epoch': 2.567910885284346}\nEpoch: 2.5718194254445965, Logs: {'loss': 0.0454, 'grad_norm': 0.14518576860427856, 'learning_rate': 1.4859265050820955e-05, 'epoch': 2.5718194254445965}\nEpoch: 2.5757279656048464, Logs: {'loss': 0.0447, 'grad_norm': 0.8748061060905457, 'learning_rate': 1.4851446442533231e-05, 'epoch': 2.5757279656048464}\nEpoch: 2.5796365057650967, Logs: {'loss': 0.0486, 'grad_norm': 0.1700485646724701, 'learning_rate': 1.4843627834245506e-05, 'epoch': 2.5796365057650967}\nEpoch: 2.583545045925347, Logs: {'loss': 0.0512, 'grad_norm': 0.0860549733042717, 'learning_rate': 1.483580922595778e-05, 'epoch': 2.583545045925347}\nEpoch: 2.587453586085597, Logs: {'loss': 0.0435, 'grad_norm': 0.24984095990657806, 'learning_rate': 1.4827990617670056e-05, 'epoch': 2.587453586085597}\nEpoch: 2.5913621262458473, Logs: {'loss': 0.0416, 'grad_norm': 0.20484650135040283, 'learning_rate': 1.4820172009382332e-05, 'epoch': 2.5913621262458473}\nEpoch: 2.595270666406097, Logs: {'loss': 0.0406, 'grad_norm': 0.1670943945646286, 'learning_rate': 1.4812353401094606e-05, 'epoch': 2.595270666406097}\nEpoch: 2.5991792065663475, Logs: {'loss': 0.0502, 'grad_norm': 0.27850282192230225, 'learning_rate': 1.4804534792806882e-05, 'epoch': 2.5991792065663475}\nEpoch: 2.6030877467265974, Logs: {'loss': 0.0423, 'grad_norm': 0.09534201771020889, 'learning_rate': 1.4796716184519157e-05, 'epoch': 2.6030877467265974}\nEpoch: 2.6069962868868477, Logs: {'loss': 0.0452, 'grad_norm': 0.09272083640098572, 'learning_rate': 1.4788897576231432e-05, 'epoch': 2.6069962868868477}\nEpoch: 2.610904827047098, Logs: {'loss': 0.0542, 'grad_norm': 0.26348796486854553, 'learning_rate': 1.4781078967943707e-05, 'epoch': 2.610904827047098}\nEpoch: 2.614813367207348, Logs: {'loss': 0.0456, 'grad_norm': 0.17217618227005005, 'learning_rate': 1.4773260359655983e-05, 'epoch': 2.614813367207348}\nEpoch: 2.6187219073675982, Logs: {'loss': 0.0507, 'grad_norm': 0.32036954164505005, 'learning_rate': 1.4765441751368257e-05, 'epoch': 2.6187219073675982}\nEpoch: 2.6226304475278486, Logs: {'loss': 0.0454, 'grad_norm': 0.3543398082256317, 'learning_rate': 1.4757623143080533e-05, 'epoch': 2.6226304475278486}\nEpoch: 2.6265389876880985, Logs: {'loss': 0.0515, 'grad_norm': 0.2542688250541687, 'learning_rate': 1.4749804534792808e-05, 'epoch': 2.6265389876880985}\nEpoch: 2.630447527848349, Logs: {'loss': 0.0454, 'grad_norm': 0.19981734454631805, 'learning_rate': 1.4741985926505083e-05, 'epoch': 2.630447527848349}\nEpoch: 2.6343560680085987, Logs: {'loss': 0.0444, 'grad_norm': 0.1770447939634323, 'learning_rate': 1.4734167318217358e-05, 'epoch': 2.6343560680085987}\nEpoch: 2.638264608168849, Logs: {'loss': 0.0495, 'grad_norm': 0.19146724045276642, 'learning_rate': 1.4726348709929634e-05, 'epoch': 2.638264608168849}\nEpoch: 2.642173148329099, Logs: {'loss': 0.0509, 'grad_norm': 0.22197143733501434, 'learning_rate': 1.4718530101641907e-05, 'epoch': 2.642173148329099}\nEpoch: 2.646081688489349, Logs: {'loss': 0.0463, 'grad_norm': 0.08810985833406448, 'learning_rate': 1.4710711493354184e-05, 'epoch': 2.646081688489349}\nEpoch: 2.6499902286495995, Logs: {'loss': 0.0449, 'grad_norm': 0.12131232768297195, 'learning_rate': 1.470289288506646e-05, 'epoch': 2.6499902286495995}\nEpoch: 2.6538987688098494, Logs: {'loss': 0.0476, 'grad_norm': 0.17042966187000275, 'learning_rate': 1.4695074276778734e-05, 'epoch': 2.6538987688098494}\nEpoch: 2.6578073089700998, Logs: {'loss': 0.0406, 'grad_norm': 0.2121834009885788, 'learning_rate': 1.468725566849101e-05, 'epoch': 2.6578073089700998}\nEpoch: 2.66171584913035, Logs: {'loss': 0.0479, 'grad_norm': 0.4737032353878021, 'learning_rate': 1.4679437060203285e-05, 'epoch': 2.66171584913035}\nEpoch: 2.6656243892906, Logs: {'loss': 0.0459, 'grad_norm': 0.3176840841770172, 'learning_rate': 1.4671618451915562e-05, 'epoch': 2.6656243892906}\nEpoch: 2.66953292945085, Logs: {'loss': 0.0426, 'grad_norm': 0.14611858129501343, 'learning_rate': 1.4663799843627835e-05, 'epoch': 2.66953292945085}\nEpoch: 2.6734414696111, Logs: {'loss': 0.0442, 'grad_norm': 0.24092985689640045, 'learning_rate': 1.4655981235340111e-05, 'epoch': 2.6734414696111}\nEpoch: 2.6773500097713505, Logs: {'loss': 0.0533, 'grad_norm': 0.16771656274795532, 'learning_rate': 1.4648162627052386e-05, 'epoch': 2.6773500097713505}\nEpoch: 2.6812585499316004, Logs: {'loss': 0.0403, 'grad_norm': 0.1888020932674408, 'learning_rate': 1.4640344018764661e-05, 'epoch': 2.6812585499316004}\nEpoch: 2.6851670900918507, Logs: {'loss': 0.0415, 'grad_norm': 0.1786459982395172, 'learning_rate': 1.4632525410476936e-05, 'epoch': 2.6851670900918507}\nEpoch: 2.689075630252101, Logs: {'loss': 0.045, 'grad_norm': 0.2550435960292816, 'learning_rate': 1.4624706802189213e-05, 'epoch': 2.689075630252101}\nEpoch: 2.692984170412351, Logs: {'loss': 0.0497, 'grad_norm': 0.20289874076843262, 'learning_rate': 1.4616888193901486e-05, 'epoch': 2.692984170412351}\nEpoch: 2.6968927105726013, Logs: {'loss': 0.0453, 'grad_norm': 0.261334627866745, 'learning_rate': 1.4609069585613762e-05, 'epoch': 2.6968927105726013}\nEpoch: 2.700801250732851, Logs: {'loss': 0.0474, 'grad_norm': 0.27142608165740967, 'learning_rate': 1.4601250977326037e-05, 'epoch': 2.700801250732851}\nEpoch: 2.7047097908931015, Logs: {'loss': 0.056, 'grad_norm': 0.180466428399086, 'learning_rate': 1.4593432369038312e-05, 'epoch': 2.7047097908931015}\nEpoch: 2.7086183310533514, Logs: {'loss': 0.0436, 'grad_norm': 0.16311827301979065, 'learning_rate': 1.4585613760750587e-05, 'epoch': 2.7086183310533514}\nEpoch: 2.7125268712136017, Logs: {'loss': 0.0492, 'grad_norm': 0.12791678309440613, 'learning_rate': 1.4577795152462863e-05, 'epoch': 2.7125268712136017}\nEpoch: 2.716435411373852, Logs: {'loss': 0.0481, 'grad_norm': 0.2779930830001831, 'learning_rate': 1.4569976544175137e-05, 'epoch': 2.716435411373852}\nEpoch: 2.720343951534102, Logs: {'loss': 0.046, 'grad_norm': 0.17386269569396973, 'learning_rate': 1.4562157935887413e-05, 'epoch': 2.720343951534102}\nEpoch: 2.7242524916943522, Logs: {'loss': 0.0405, 'grad_norm': 0.2051825374364853, 'learning_rate': 1.455433932759969e-05, 'epoch': 2.7242524916943522}\nEpoch: 2.7281610318546026, Logs: {'loss': 0.0527, 'grad_norm': 0.2753453850746155, 'learning_rate': 1.4546520719311963e-05, 'epoch': 2.7281610318546026}\nEpoch: 2.7320695720148525, Logs: {'loss': 0.0446, 'grad_norm': 0.3475615978240967, 'learning_rate': 1.453870211102424e-05, 'epoch': 2.7320695720148525}\nEpoch: 2.7359781121751023, Logs: {'loss': 0.046, 'grad_norm': 0.49435916543006897, 'learning_rate': 1.4530883502736514e-05, 'epoch': 2.7359781121751023}\nEpoch: 2.7398866523353527, Logs: {'loss': 0.0408, 'grad_norm': 0.08101559430360794, 'learning_rate': 1.452306489444879e-05, 'epoch': 2.7398866523353527}\nEpoch: 2.743795192495603, Logs: {'loss': 0.0407, 'grad_norm': 0.20268647372722626, 'learning_rate': 1.4515246286161064e-05, 'epoch': 2.743795192495603}\nEpoch: 2.747703732655853, Logs: {'loss': 0.0425, 'grad_norm': 0.10806787759065628, 'learning_rate': 1.450742767787334e-05, 'epoch': 2.747703732655853}\nEpoch: 2.751612272816103, Logs: {'loss': 0.0465, 'grad_norm': 0.19995082914829254, 'learning_rate': 1.4499609069585614e-05, 'epoch': 2.751612272816103}\nEpoch: 2.7555208129763535, Logs: {'loss': 0.0478, 'grad_norm': 0.1810908317565918, 'learning_rate': 1.449179046129789e-05, 'epoch': 2.7555208129763535}\nEpoch: 2.7594293531366034, Logs: {'loss': 0.0437, 'grad_norm': 0.07101718336343765, 'learning_rate': 1.4483971853010165e-05, 'epoch': 2.7594293531366034}\nEpoch: 2.7633378932968538, Logs: {'loss': 0.0479, 'grad_norm': 0.21402662992477417, 'learning_rate': 1.447615324472244e-05, 'epoch': 2.7633378932968538}\nEpoch: 2.7672464334571036, Logs: {'loss': 0.0473, 'grad_norm': 0.1418786197900772, 'learning_rate': 1.4468334636434715e-05, 'epoch': 2.7672464334571036}\nEpoch: 2.771154973617354, Logs: {'loss': 0.0446, 'grad_norm': 0.15781636536121368, 'learning_rate': 1.4460516028146992e-05, 'epoch': 2.771154973617354}\nEpoch: 2.775063513777604, Logs: {'loss': 0.0484, 'grad_norm': 0.10643564164638519, 'learning_rate': 1.4452697419859268e-05, 'epoch': 2.775063513777604}\nEpoch: 2.778972053937854, Logs: {'loss': 0.0456, 'grad_norm': 0.12034762650728226, 'learning_rate': 1.4444878811571541e-05, 'epoch': 2.778972053937854}\nEpoch: 2.7828805940981045, Logs: {'loss': 0.0449, 'grad_norm': 0.26013630628585815, 'learning_rate': 1.4437060203283816e-05, 'epoch': 2.7828805940981045}\nEpoch: 2.7867891342583544, Logs: {'loss': 0.0422, 'grad_norm': 0.2162584364414215, 'learning_rate': 1.4429241594996093e-05, 'epoch': 2.7867891342583544}\nEpoch: 2.7906976744186047, Logs: {'loss': 0.0417, 'grad_norm': 0.17687666416168213, 'learning_rate': 1.4421422986708366e-05, 'epoch': 2.7906976744186047}\nEpoch: 2.794606214578855, Logs: {'loss': 0.0467, 'grad_norm': 0.2703340947628021, 'learning_rate': 1.4413604378420642e-05, 'epoch': 2.794606214578855}\nEpoch: 2.798514754739105, Logs: {'loss': 0.0503, 'grad_norm': 0.15155330300331116, 'learning_rate': 1.4405785770132919e-05, 'epoch': 2.798514754739105}\nEpoch: 2.802423294899355, Logs: {'loss': 0.0477, 'grad_norm': 0.21161490678787231, 'learning_rate': 1.4397967161845192e-05, 'epoch': 2.802423294899355}\nEpoch: 2.806331835059605, Logs: {'loss': 0.0452, 'grad_norm': 0.12998385727405548, 'learning_rate': 1.4390148553557469e-05, 'epoch': 2.806331835059605}\nEpoch: 2.8102403752198555, Logs: {'loss': 0.0422, 'grad_norm': 0.16123878955841064, 'learning_rate': 1.4382329945269744e-05, 'epoch': 2.8102403752198555}\nEpoch: 2.8141489153801054, Logs: {'loss': 0.0463, 'grad_norm': 0.13564299046993256, 'learning_rate': 1.4374511336982018e-05, 'epoch': 2.8141489153801054}\nEpoch: 2.8180574555403557, Logs: {'loss': 0.0496, 'grad_norm': 0.27957287430763245, 'learning_rate': 1.4366692728694293e-05, 'epoch': 2.8180574555403557}\nEpoch: 2.821965995700606, Logs: {'loss': 0.0458, 'grad_norm': 0.2160501629114151, 'learning_rate': 1.435887412040657e-05, 'epoch': 2.821965995700606}\nEpoch: 2.825874535860856, Logs: {'loss': 0.0433, 'grad_norm': 0.16482307016849518, 'learning_rate': 1.4351055512118843e-05, 'epoch': 2.825874535860856}\nEpoch: 2.8297830760211062, Logs: {'loss': 0.0455, 'grad_norm': 0.4411763846874237, 'learning_rate': 1.434323690383112e-05, 'epoch': 2.8297830760211062}\nEpoch: 2.833691616181356, Logs: {'loss': 0.0406, 'grad_norm': 0.07140772044658661, 'learning_rate': 1.4335418295543394e-05, 'epoch': 2.833691616181356}\nEpoch: 2.8376001563416064, Logs: {'loss': 0.041, 'grad_norm': 0.14604100584983826, 'learning_rate': 1.432759968725567e-05, 'epoch': 2.8376001563416064}\nEpoch: 2.8415086965018563, Logs: {'loss': 0.053, 'grad_norm': 0.10758186131715775, 'learning_rate': 1.4319781078967944e-05, 'epoch': 2.8415086965018563}\nEpoch: 2.8454172366621067, Logs: {'loss': 0.0455, 'grad_norm': 0.10740789026021957, 'learning_rate': 1.431196247068022e-05, 'epoch': 2.8454172366621067}\nEpoch: 2.849325776822357, Logs: {'loss': 0.0488, 'grad_norm': 0.3036551773548126, 'learning_rate': 1.4304143862392494e-05, 'epoch': 2.849325776822357}\nEpoch: 2.853234316982607, Logs: {'loss': 0.0443, 'grad_norm': 0.15995444357395172, 'learning_rate': 1.429632525410477e-05, 'epoch': 2.853234316982607}\nEpoch: 2.857142857142857, Logs: {'loss': 0.0446, 'grad_norm': 0.18785510957241058, 'learning_rate': 1.4288506645817047e-05, 'epoch': 2.857142857142857}\nEpoch: 2.8610513973031075, Logs: {'loss': 0.0473, 'grad_norm': 0.2131476104259491, 'learning_rate': 1.428068803752932e-05, 'epoch': 2.8610513973031075}\nEpoch: 2.8649599374633574, Logs: {'loss': 0.0423, 'grad_norm': 0.11116845905780792, 'learning_rate': 1.4272869429241595e-05, 'epoch': 2.8649599374633574}\nEpoch: 2.8688684776236077, Logs: {'loss': 0.0509, 'grad_norm': 0.11907985806465149, 'learning_rate': 1.4265050820953872e-05, 'epoch': 2.8688684776236077}\nEpoch: 2.8727770177838576, Logs: {'loss': 0.0452, 'grad_norm': 0.2615731954574585, 'learning_rate': 1.4257232212666145e-05, 'epoch': 2.8727770177838576}\nEpoch: 2.876685557944108, Logs: {'loss': 0.0444, 'grad_norm': 0.1288926899433136, 'learning_rate': 1.4249413604378421e-05, 'epoch': 2.876685557944108}\nEpoch: 2.880594098104358, Logs: {'loss': 0.0484, 'grad_norm': 0.36885225772857666, 'learning_rate': 1.4241594996090698e-05, 'epoch': 2.880594098104358}\nEpoch: 2.884502638264608, Logs: {'loss': 0.0454, 'grad_norm': 0.26421067118644714, 'learning_rate': 1.4233776387802973e-05, 'epoch': 2.884502638264608}\nEpoch: 2.8884111784248585, Logs: {'loss': 0.041, 'grad_norm': 0.24466538429260254, 'learning_rate': 1.4225957779515248e-05, 'epoch': 2.8884111784248585}\nEpoch: 2.8923197185851084, Logs: {'loss': 0.0415, 'grad_norm': 0.21640367805957794, 'learning_rate': 1.4218139171227523e-05, 'epoch': 2.8923197185851084}\nEpoch: 2.8962282587453587, Logs: {'loss': 0.0494, 'grad_norm': 0.3426019251346588, 'learning_rate': 1.4210320562939799e-05, 'epoch': 2.8962282587453587}\nEpoch: 2.9001367989056086, Logs: {'loss': 0.0461, 'grad_norm': 0.18074388802051544, 'learning_rate': 1.4202501954652072e-05, 'epoch': 2.9001367989056086}\nEpoch: 2.904045339065859, Logs: {'loss': 0.0414, 'grad_norm': 0.08799218386411667, 'learning_rate': 1.4194683346364349e-05, 'epoch': 2.904045339065859}\nEpoch: 2.907953879226109, Logs: {'loss': 0.0476, 'grad_norm': 0.48188191652297974, 'learning_rate': 1.4186864738076624e-05, 'epoch': 2.907953879226109}\nEpoch: 2.911862419386359, Logs: {'loss': 0.0434, 'grad_norm': 0.21025018393993378, 'learning_rate': 1.4179046129788899e-05, 'epoch': 2.911862419386359}\nEpoch: 2.9157709595466095, Logs: {'loss': 0.0435, 'grad_norm': 0.20409899950027466, 'learning_rate': 1.4171227521501173e-05, 'epoch': 2.9157709595466095}\nEpoch: 2.9196794997068594, Logs: {'loss': 0.0451, 'grad_norm': 0.27077582478523254, 'learning_rate': 1.416340891321345e-05, 'epoch': 2.9196794997068594}\nEpoch: 2.9235880398671097, Logs: {'loss': 0.0464, 'grad_norm': 0.16728712618350983, 'learning_rate': 1.4155590304925723e-05, 'epoch': 2.9235880398671097}\nEpoch: 2.92749658002736, Logs: {'loss': 0.0481, 'grad_norm': 0.504777729511261, 'learning_rate': 1.4147771696638e-05, 'epoch': 2.92749658002736}\nEpoch: 2.93140512018761, Logs: {'loss': 0.0419, 'grad_norm': 0.26499849557876587, 'learning_rate': 1.4139953088350276e-05, 'epoch': 2.93140512018761}\nEpoch: 2.9353136603478602, Logs: {'loss': 0.0442, 'grad_norm': 0.1543915867805481, 'learning_rate': 1.413213448006255e-05, 'epoch': 2.9353136603478602}\nEpoch: 2.93922220050811, Logs: {'loss': 0.0456, 'grad_norm': 0.24615183472633362, 'learning_rate': 1.4124315871774826e-05, 'epoch': 2.93922220050811}\nEpoch: 2.9431307406683604, Logs: {'loss': 0.0482, 'grad_norm': 0.1811792254447937, 'learning_rate': 1.4116497263487101e-05, 'epoch': 2.9431307406683604}\nEpoch: 2.9470392808286103, Logs: {'loss': 0.0398, 'grad_norm': 0.2686285376548767, 'learning_rate': 1.4108678655199374e-05, 'epoch': 2.9470392808286103}\nEpoch: 2.9509478209888607, Logs: {'loss': 0.0394, 'grad_norm': 0.10526534914970398, 'learning_rate': 1.410086004691165e-05, 'epoch': 2.9509478209888607}\nEpoch: 2.954856361149111, Logs: {'loss': 0.0415, 'grad_norm': 0.09241858124732971, 'learning_rate': 1.4093041438623927e-05, 'epoch': 2.954856361149111}\nEpoch: 2.958764901309361, Logs: {'loss': 0.0486, 'grad_norm': 0.13987427949905396, 'learning_rate': 1.40852228303362e-05, 'epoch': 2.958764901309361}\nEpoch: 2.962673441469611, Logs: {'loss': 0.0392, 'grad_norm': 0.22359777987003326, 'learning_rate': 1.4077404222048477e-05, 'epoch': 2.962673441469611}\nEpoch: 2.966581981629861, Logs: {'loss': 0.0464, 'grad_norm': 0.14804020524024963, 'learning_rate': 1.4069585613760752e-05, 'epoch': 2.966581981629861}\nEpoch: 2.9704905217901114, Logs: {'loss': 0.0461, 'grad_norm': 0.14956621825695038, 'learning_rate': 1.4061767005473027e-05, 'epoch': 2.9704905217901114}\nEpoch: 2.9743990619503613, Logs: {'loss': 0.0457, 'grad_norm': 0.14275158941745758, 'learning_rate': 1.4053948397185302e-05, 'epoch': 2.9743990619503613}\nEpoch: 2.9783076021106116, Logs: {'loss': 0.045, 'grad_norm': 0.13900373876094818, 'learning_rate': 1.4046129788897578e-05, 'epoch': 2.9783076021106116}\nEpoch: 2.982216142270862, Logs: {'loss': 0.043, 'grad_norm': 0.12451139092445374, 'learning_rate': 1.4038311180609851e-05, 'epoch': 2.982216142270862}\nEpoch: 2.986124682431112, Logs: {'loss': 0.0418, 'grad_norm': 0.41659972071647644, 'learning_rate': 1.4030492572322128e-05, 'epoch': 2.986124682431112}\nEpoch: 2.990033222591362, Logs: {'loss': 0.0461, 'grad_norm': 0.08484022319316864, 'learning_rate': 1.4022673964034403e-05, 'epoch': 2.990033222591362}\nEpoch: 2.9939417627516125, Logs: {'loss': 0.0412, 'grad_norm': 0.20946824550628662, 'learning_rate': 1.401485535574668e-05, 'epoch': 2.9939417627516125}\nEpoch: 2.9978503029118624, Logs: {'loss': 0.0417, 'grad_norm': 0.20982524752616882, 'learning_rate': 1.4007036747458952e-05, 'epoch': 2.9978503029118624}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2.9998045729919873, Logs: {'eval_loss': 0.04442790895700455, 'eval_bleu': 2.9159906177183913e-10, 'eval_accuracy': 0.9439992181391712, 'eval_runtime': 349.9562, 'eval_samples_per_second': 58.476, 'eval_steps_per_second': 3.655, 'epoch': 2.9998045729919873}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3.0017588430721127, Logs: {'loss': 0.0459, 'grad_norm': 0.16296924650669098, 'learning_rate': 1.3999218139171229e-05, 'epoch': 3.0017588430721127}\nEpoch: 3.0056673832323626, Logs: {'loss': 0.0424, 'grad_norm': 0.18742477893829346, 'learning_rate': 1.3991399530883506e-05, 'epoch': 3.0056673832323626}\nEpoch: 3.009575923392613, Logs: {'loss': 0.045, 'grad_norm': 0.3165649175643921, 'learning_rate': 1.3983580922595779e-05, 'epoch': 3.009575923392613}\nEpoch: 3.013484463552863, Logs: {'loss': 0.0478, 'grad_norm': 0.3389853537082672, 'learning_rate': 1.3975762314308055e-05, 'epoch': 3.013484463552863}\nEpoch: 3.017393003713113, Logs: {'loss': 0.0446, 'grad_norm': 0.48451682925224304, 'learning_rate': 1.396794370602033e-05, 'epoch': 3.017393003713113}\nEpoch: 3.0213015438733635, Logs: {'loss': 0.0428, 'grad_norm': 0.12960757315158844, 'learning_rate': 1.3960125097732605e-05, 'epoch': 3.0213015438733635}\nEpoch: 3.0252100840336134, Logs: {'loss': 0.048, 'grad_norm': 0.13385401666164398, 'learning_rate': 1.395230648944488e-05, 'epoch': 3.0252100840336134}\nEpoch: 3.0291186241938637, Logs: {'loss': 0.0462, 'grad_norm': 0.4007541537284851, 'learning_rate': 1.3944487881157156e-05, 'epoch': 3.0291186241938637}\nEpoch: 3.0330271643541136, Logs: {'loss': 0.0456, 'grad_norm': 0.10355103760957718, 'learning_rate': 1.393666927286943e-05, 'epoch': 3.0330271643541136}\nEpoch: 3.036935704514364, Logs: {'loss': 0.0426, 'grad_norm': 0.14296965301036835, 'learning_rate': 1.3928850664581706e-05, 'epoch': 3.036935704514364}\nEpoch: 3.0408442446746142, Logs: {'loss': 0.0394, 'grad_norm': 0.08119751513004303, 'learning_rate': 1.3921032056293981e-05, 'epoch': 3.0408442446746142}\nEpoch: 3.044752784834864, Logs: {'loss': 0.0492, 'grad_norm': 0.2638035714626312, 'learning_rate': 1.3913213448006256e-05, 'epoch': 3.044752784834864}\nEpoch: 3.0486613249951144, Logs: {'loss': 0.0483, 'grad_norm': 0.1229136735200882, 'learning_rate': 1.390539483971853e-05, 'epoch': 3.0486613249951144}\nEpoch: 3.0525698651553643, Logs: {'loss': 0.0481, 'grad_norm': 0.2011537402868271, 'learning_rate': 1.3897576231430807e-05, 'epoch': 3.0525698651553643}\nEpoch: 3.0564784053156147, Logs: {'loss': 0.0436, 'grad_norm': 0.0660412460565567, 'learning_rate': 1.388975762314308e-05, 'epoch': 3.0564784053156147}\nEpoch: 3.060386945475865, Logs: {'loss': 0.0423, 'grad_norm': 0.14762288331985474, 'learning_rate': 1.3881939014855357e-05, 'epoch': 3.060386945475865}\nEpoch: 3.064295485636115, Logs: {'loss': 0.0413, 'grad_norm': 0.23355433344841003, 'learning_rate': 1.3874120406567632e-05, 'epoch': 3.064295485636115}\nEpoch: 3.068204025796365, Logs: {'loss': 0.0471, 'grad_norm': 0.15137894451618195, 'learning_rate': 1.3866301798279907e-05, 'epoch': 3.068204025796365}\nEpoch: 3.072112565956615, Logs: {'loss': 0.0429, 'grad_norm': 0.16005513072013855, 'learning_rate': 1.3858483189992182e-05, 'epoch': 3.072112565956615}\nEpoch: 3.0760211061168654, Logs: {'loss': 0.0412, 'grad_norm': 0.16696149110794067, 'learning_rate': 1.3850664581704458e-05, 'epoch': 3.0760211061168654}\nEpoch: 3.0799296462771153, Logs: {'loss': 0.0426, 'grad_norm': 0.3507038652896881, 'learning_rate': 1.3842845973416731e-05, 'epoch': 3.0799296462771153}\nEpoch: 3.0838381864373656, Logs: {'loss': 0.0432, 'grad_norm': 0.20341432094573975, 'learning_rate': 1.3835027365129008e-05, 'epoch': 3.0838381864373656}\nEpoch: 3.087746726597616, Logs: {'loss': 0.0471, 'grad_norm': 0.22395777702331543, 'learning_rate': 1.3827208756841284e-05, 'epoch': 3.087746726597616}\nEpoch: 3.091655266757866, Logs: {'loss': 0.0415, 'grad_norm': 0.17463193833827972, 'learning_rate': 1.3819390148553558e-05, 'epoch': 3.091655266757866}\nEpoch: 3.095563806918116, Logs: {'loss': 0.0416, 'grad_norm': 0.37189504504203796, 'learning_rate': 1.3811571540265834e-05, 'epoch': 3.095563806918116}\nEpoch: 3.099472347078366, Logs: {'loss': 0.0472, 'grad_norm': 0.09877040982246399, 'learning_rate': 1.3803752931978109e-05, 'epoch': 3.099472347078366}\nEpoch: 3.1033808872386164, Logs: {'loss': 0.0454, 'grad_norm': 0.4601075053215027, 'learning_rate': 1.3795934323690386e-05, 'epoch': 3.1033808872386164}\nEpoch: 3.1072894273988667, Logs: {'loss': 0.0424, 'grad_norm': 0.3517996668815613, 'learning_rate': 1.3788115715402659e-05, 'epoch': 3.1072894273988667}\nEpoch: 3.1111979675591166, Logs: {'loss': 0.0534, 'grad_norm': 0.1373409479856491, 'learning_rate': 1.3780297107114935e-05, 'epoch': 3.1111979675591166}\nEpoch: 3.115106507719367, Logs: {'loss': 0.0446, 'grad_norm': 0.30580583214759827, 'learning_rate': 1.377247849882721e-05, 'epoch': 3.115106507719367}\nEpoch: 3.119015047879617, Logs: {'loss': 0.0427, 'grad_norm': 0.1327885240316391, 'learning_rate': 1.3764659890539485e-05, 'epoch': 3.119015047879617}\nEpoch: 3.122923588039867, Logs: {'loss': 0.0427, 'grad_norm': 0.14492273330688477, 'learning_rate': 1.375684128225176e-05, 'epoch': 3.122923588039867}\nEpoch: 3.1268321282001175, Logs: {'loss': 0.0397, 'grad_norm': 0.09249449521303177, 'learning_rate': 1.3749022673964037e-05, 'epoch': 3.1268321282001175}\nEpoch: 3.1307406683603674, Logs: {'loss': 0.0437, 'grad_norm': 0.17430993914604187, 'learning_rate': 1.374120406567631e-05, 'epoch': 3.1307406683603674}\nEpoch: 3.1346492085206177, Logs: {'loss': 0.0412, 'grad_norm': 0.15878859162330627, 'learning_rate': 1.3733385457388586e-05, 'epoch': 3.1346492085206177}\nEpoch: 3.1385577486808676, Logs: {'loss': 0.0447, 'grad_norm': 0.3758748471736908, 'learning_rate': 1.3725566849100863e-05, 'epoch': 3.1385577486808676}\nEpoch: 3.142466288841118, Logs: {'loss': 0.0468, 'grad_norm': 0.16835999488830566, 'learning_rate': 1.3717748240813136e-05, 'epoch': 3.142466288841118}\nEpoch: 3.146374829001368, Logs: {'loss': 0.0438, 'grad_norm': 0.2000693827867508, 'learning_rate': 1.3709929632525411e-05, 'epoch': 3.146374829001368}\nEpoch: 3.150283369161618, Logs: {'loss': 0.0441, 'grad_norm': 0.11932666599750519, 'learning_rate': 1.3702111024237687e-05, 'epoch': 3.150283369161618}\nEpoch: 3.1541919093218684, Logs: {'loss': 0.0438, 'grad_norm': 0.1162906289100647, 'learning_rate': 1.369429241594996e-05, 'epoch': 3.1541919093218684}\nEpoch: 3.1581004494821183, Logs: {'loss': 0.047, 'grad_norm': 0.3309028446674347, 'learning_rate': 1.3686473807662237e-05, 'epoch': 3.1581004494821183}\nEpoch: 3.1620089896423687, Logs: {'loss': 0.0481, 'grad_norm': 0.0922156572341919, 'learning_rate': 1.3678655199374514e-05, 'epoch': 3.1620089896423687}\nEpoch: 3.1659175298026185, Logs: {'loss': 0.0488, 'grad_norm': 0.14027079939842224, 'learning_rate': 1.3670836591086787e-05, 'epoch': 3.1659175298026185}\nEpoch: 3.169826069962869, Logs: {'loss': 0.0407, 'grad_norm': 0.24674075841903687, 'learning_rate': 1.3663017982799063e-05, 'epoch': 3.169826069962869}\nEpoch: 3.173734610123119, Logs: {'loss': 0.0448, 'grad_norm': 0.20653009414672852, 'learning_rate': 1.3655199374511338e-05, 'epoch': 3.173734610123119}\nEpoch: 3.177643150283369, Logs: {'loss': 0.0426, 'grad_norm': 0.20275036990642548, 'learning_rate': 1.3647380766223613e-05, 'epoch': 3.177643150283369}\nEpoch: 3.1815516904436194, Logs: {'loss': 0.0473, 'grad_norm': 0.1209586039185524, 'learning_rate': 1.3639562157935888e-05, 'epoch': 3.1815516904436194}\nEpoch: 3.1854602306038693, Logs: {'loss': 0.0456, 'grad_norm': 0.15084756910800934, 'learning_rate': 1.3631743549648165e-05, 'epoch': 3.1854602306038693}\nEpoch: 3.1893687707641196, Logs: {'loss': 0.0436, 'grad_norm': 0.2784373462200165, 'learning_rate': 1.3623924941360438e-05, 'epoch': 3.1893687707641196}\nEpoch: 3.19327731092437, Logs: {'loss': 0.0437, 'grad_norm': 0.2378896027803421, 'learning_rate': 1.3616106333072714e-05, 'epoch': 3.19327731092437}\nEpoch: 3.19718585108462, Logs: {'loss': 0.0543, 'grad_norm': 0.22763969004154205, 'learning_rate': 1.360828772478499e-05, 'epoch': 3.19718585108462}\nEpoch: 3.20109439124487, Logs: {'loss': 0.0441, 'grad_norm': 0.22926978766918182, 'learning_rate': 1.3600469116497264e-05, 'epoch': 3.20109439124487}\nEpoch: 3.20500293140512, Logs: {'loss': 0.0444, 'grad_norm': 0.16366605460643768, 'learning_rate': 1.3592650508209539e-05, 'epoch': 3.20500293140512}\nEpoch: 3.2089114715653704, Logs: {'loss': 0.0453, 'grad_norm': 0.18534623086452484, 'learning_rate': 1.3584831899921816e-05, 'epoch': 3.2089114715653704}\nEpoch: 3.2128200117256203, Logs: {'loss': 0.0463, 'grad_norm': 0.12446732819080353, 'learning_rate': 1.3577013291634092e-05, 'epoch': 3.2128200117256203}\nEpoch: 3.2167285518858706, Logs: {'loss': 0.045, 'grad_norm': 0.07178463041782379, 'learning_rate': 1.3569194683346365e-05, 'epoch': 3.2167285518858706}\nEpoch: 3.220637092046121, Logs: {'loss': 0.044, 'grad_norm': 0.1443656086921692, 'learning_rate': 1.3561376075058642e-05, 'epoch': 3.220637092046121}\nEpoch: 3.224545632206371, Logs: {'loss': 0.042, 'grad_norm': 0.2790038287639618, 'learning_rate': 1.3553557466770917e-05, 'epoch': 3.224545632206371}\nEpoch: 3.228454172366621, Logs: {'loss': 0.0437, 'grad_norm': 0.155269593000412, 'learning_rate': 1.354573885848319e-05, 'epoch': 3.228454172366621}\nEpoch: 3.232362712526871, Logs: {'loss': 0.0415, 'grad_norm': 0.14701327681541443, 'learning_rate': 1.3537920250195466e-05, 'epoch': 3.232362712526871}\nEpoch: 3.2362712526871213, Logs: {'loss': 0.0433, 'grad_norm': 0.17370472848415375, 'learning_rate': 1.3530101641907743e-05, 'epoch': 3.2362712526871213}\nEpoch: 3.2401797928473717, Logs: {'loss': 0.0464, 'grad_norm': 0.21055422723293304, 'learning_rate': 1.3522283033620016e-05, 'epoch': 3.2401797928473717}\nEpoch: 3.2440883330076216, Logs: {'loss': 0.0482, 'grad_norm': 0.24275192618370056, 'learning_rate': 1.3514464425332293e-05, 'epoch': 3.2440883330076216}\nEpoch: 3.247996873167872, Logs: {'loss': 0.0391, 'grad_norm': 0.07334014028310776, 'learning_rate': 1.3506645817044568e-05, 'epoch': 3.247996873167872}\nEpoch: 3.2519054133281218, Logs: {'loss': 0.0524, 'grad_norm': 0.22694651782512665, 'learning_rate': 1.3498827208756842e-05, 'epoch': 3.2519054133281218}\nEpoch: 3.255813953488372, Logs: {'loss': 0.0463, 'grad_norm': 0.07763063907623291, 'learning_rate': 1.3491008600469117e-05, 'epoch': 3.255813953488372}\nEpoch: 3.2597224936486224, Logs: {'loss': 0.0441, 'grad_norm': 0.18078404664993286, 'learning_rate': 1.3483189992181394e-05, 'epoch': 3.2597224936486224}\nEpoch: 3.2636310338088723, Logs: {'loss': 0.0452, 'grad_norm': 0.35712099075317383, 'learning_rate': 1.3475371383893667e-05, 'epoch': 3.2636310338088723}\nEpoch: 3.2675395739691226, Logs: {'loss': 0.0428, 'grad_norm': 0.1738983392715454, 'learning_rate': 1.3467552775605944e-05, 'epoch': 3.2675395739691226}\nEpoch: 3.2714481141293725, Logs: {'loss': 0.0402, 'grad_norm': 0.1617138832807541, 'learning_rate': 1.3459734167318218e-05, 'epoch': 3.2714481141293725}\nEpoch: 3.275356654289623, Logs: {'loss': 0.0562, 'grad_norm': 0.1243579238653183, 'learning_rate': 1.3451915559030493e-05, 'epoch': 3.275356654289623}\nEpoch: 3.2792651944498727, Logs: {'loss': 0.0467, 'grad_norm': 0.12385644018650055, 'learning_rate': 1.3444096950742768e-05, 'epoch': 3.2792651944498727}\nEpoch: 3.283173734610123, Logs: {'loss': 0.0408, 'grad_norm': 0.4179503321647644, 'learning_rate': 1.3436278342455045e-05, 'epoch': 3.283173734610123}\nEpoch: 3.2870822747703734, Logs: {'loss': 0.0474, 'grad_norm': 0.11044542491436005, 'learning_rate': 1.3428459734167318e-05, 'epoch': 3.2870822747703734}\nEpoch: 3.2909908149306233, Logs: {'loss': 0.0422, 'grad_norm': 0.1729966551065445, 'learning_rate': 1.3420641125879594e-05, 'epoch': 3.2909908149306233}\nEpoch: 3.2948993550908736, Logs: {'loss': 0.0438, 'grad_norm': 0.2421647310256958, 'learning_rate': 1.3412822517591871e-05, 'epoch': 3.2948993550908736}\nEpoch: 3.298807895251124, Logs: {'loss': 0.0419, 'grad_norm': 0.16546715795993805, 'learning_rate': 1.3405003909304144e-05, 'epoch': 3.298807895251124}\nEpoch: 3.302716435411374, Logs: {'loss': 0.0454, 'grad_norm': 0.10170208662748337, 'learning_rate': 1.3397185301016419e-05, 'epoch': 3.302716435411374}\nEpoch: 3.306624975571624, Logs: {'loss': 0.0401, 'grad_norm': 0.193318173289299, 'learning_rate': 1.3389366692728696e-05, 'epoch': 3.306624975571624}\nEpoch: 3.310533515731874, Logs: {'loss': 0.0492, 'grad_norm': 0.1580623835325241, 'learning_rate': 1.3381548084440969e-05, 'epoch': 3.310533515731874}\nEpoch: 3.3144420558921244, Logs: {'loss': 0.0515, 'grad_norm': 0.38821688294410706, 'learning_rate': 1.3373729476153245e-05, 'epoch': 3.3144420558921244}\nEpoch: 3.3183505960523743, Logs: {'loss': 0.0477, 'grad_norm': 0.1851135790348053, 'learning_rate': 1.3365910867865522e-05, 'epoch': 3.3183505960523743}\nEpoch: 3.3222591362126246, Logs: {'loss': 0.0476, 'grad_norm': 0.10146702826023102, 'learning_rate': 1.3358092259577797e-05, 'epoch': 3.3222591362126246}\nEpoch: 3.326167676372875, Logs: {'loss': 0.0407, 'grad_norm': 0.259194940328598, 'learning_rate': 1.3350273651290072e-05, 'epoch': 3.326167676372875}\nEpoch: 3.330076216533125, Logs: {'loss': 0.0396, 'grad_norm': 0.09333839267492294, 'learning_rate': 1.3342455043002347e-05, 'epoch': 3.330076216533125}\nEpoch: 3.333984756693375, Logs: {'loss': 0.0437, 'grad_norm': 0.24719864130020142, 'learning_rate': 1.3334636434714623e-05, 'epoch': 3.333984756693375}\nEpoch: 3.337893296853625, Logs: {'loss': 0.0441, 'grad_norm': 0.19013740122318268, 'learning_rate': 1.3326817826426896e-05, 'epoch': 3.337893296853625}\nEpoch: 3.3418018370138753, Logs: {'loss': 0.0417, 'grad_norm': 0.17706690728664398, 'learning_rate': 1.3318999218139173e-05, 'epoch': 3.3418018370138753}\nEpoch: 3.3457103771741252, Logs: {'loss': 0.0499, 'grad_norm': 0.23453998565673828, 'learning_rate': 1.3311180609851448e-05, 'epoch': 3.3457103771741252}\nEpoch: 3.3496189173343756, Logs: {'loss': 0.0458, 'grad_norm': 0.22147712111473083, 'learning_rate': 1.3303362001563723e-05, 'epoch': 3.3496189173343756}\nEpoch: 3.353527457494626, Logs: {'loss': 0.0379, 'grad_norm': 0.1576903909444809, 'learning_rate': 1.3295543393275997e-05, 'epoch': 3.353527457494626}\nEpoch: 3.3574359976548758, Logs: {'loss': 0.0393, 'grad_norm': 0.12855376303195953, 'learning_rate': 1.3287724784988274e-05, 'epoch': 3.3574359976548758}\nEpoch: 3.361344537815126, Logs: {'loss': 0.0483, 'grad_norm': 0.15498456358909607, 'learning_rate': 1.3279906176700547e-05, 'epoch': 3.361344537815126}\nEpoch: 3.3652530779753764, Logs: {'loss': 0.0411, 'grad_norm': 0.20364457368850708, 'learning_rate': 1.3272087568412824e-05, 'epoch': 3.3652530779753764}\nEpoch: 3.3691616181356263, Logs: {'loss': 0.0466, 'grad_norm': 0.17164765298366547, 'learning_rate': 1.32642689601251e-05, 'epoch': 3.3691616181356263}\nEpoch: 3.3730701582958766, Logs: {'loss': 0.0424, 'grad_norm': 0.13204516470432281, 'learning_rate': 1.3256450351837373e-05, 'epoch': 3.3730701582958766}\nEpoch: 3.3769786984561265, Logs: {'loss': 0.044, 'grad_norm': 0.1878218948841095, 'learning_rate': 1.324863174354965e-05, 'epoch': 3.3769786984561265}\nEpoch: 3.380887238616377, Logs: {'loss': 0.0401, 'grad_norm': 0.12744098901748657, 'learning_rate': 1.3240813135261925e-05, 'epoch': 3.380887238616377}\nEpoch: 3.3847957787766267, Logs: {'loss': 0.0435, 'grad_norm': 0.24269451200962067, 'learning_rate': 1.3232994526974198e-05, 'epoch': 3.3847957787766267}\nEpoch: 3.388704318936877, Logs: {'loss': 0.044, 'grad_norm': 0.11704371869564056, 'learning_rate': 1.3225175918686475e-05, 'epoch': 3.388704318936877}\nEpoch: 3.3926128590971274, Logs: {'loss': 0.0407, 'grad_norm': 0.07487011700868607, 'learning_rate': 1.3217357310398751e-05, 'epoch': 3.3926128590971274}\nEpoch: 3.3965213992573773, Logs: {'loss': 0.0399, 'grad_norm': 0.12298136204481125, 'learning_rate': 1.3209538702111024e-05, 'epoch': 3.3965213992573773}\nEpoch: 3.4004299394176276, Logs: {'loss': 0.0486, 'grad_norm': 0.20256677269935608, 'learning_rate': 1.3201720093823301e-05, 'epoch': 3.4004299394176276}\nEpoch: 3.4043384795778775, Logs: {'loss': 0.0444, 'grad_norm': 0.22577431797981262, 'learning_rate': 1.3193901485535576e-05, 'epoch': 3.4043384795778775}\nEpoch: 3.408247019738128, Logs: {'loss': 0.0418, 'grad_norm': 0.09118421375751495, 'learning_rate': 1.318608287724785e-05, 'epoch': 3.408247019738128}\nEpoch: 3.4121555598983777, Logs: {'loss': 0.0513, 'grad_norm': 0.17011219263076782, 'learning_rate': 1.3178264268960125e-05, 'epoch': 3.4121555598983777}\nEpoch: 3.416064100058628, Logs: {'loss': 0.0555, 'grad_norm': 0.1166393831372261, 'learning_rate': 1.3170445660672402e-05, 'epoch': 3.416064100058628}\nEpoch: 3.4199726402188784, Logs: {'loss': 0.0452, 'grad_norm': 0.12917934358119965, 'learning_rate': 1.3162627052384675e-05, 'epoch': 3.4199726402188784}\nEpoch: 3.4238811803791283, Logs: {'loss': 0.0435, 'grad_norm': 0.37373074889183044, 'learning_rate': 1.3154808444096952e-05, 'epoch': 3.4238811803791283}\nEpoch: 3.4277897205393786, Logs: {'loss': 0.039, 'grad_norm': 0.13671821355819702, 'learning_rate': 1.3146989835809227e-05, 'epoch': 3.4277897205393786}\nEpoch: 3.431698260699629, Logs: {'loss': 0.0489, 'grad_norm': 0.16582998633384705, 'learning_rate': 1.3139171227521503e-05, 'epoch': 3.431698260699629}\nEpoch: 3.435606800859879, Logs: {'loss': 0.0417, 'grad_norm': 0.2899939715862274, 'learning_rate': 1.3131352619233776e-05, 'epoch': 3.435606800859879}\nEpoch: 3.439515341020129, Logs: {'loss': 0.0414, 'grad_norm': 0.2636837959289551, 'learning_rate': 1.3123534010946053e-05, 'epoch': 3.439515341020129}\nEpoch: 3.443423881180379, Logs: {'loss': 0.0421, 'grad_norm': 0.16656087338924408, 'learning_rate': 1.311571540265833e-05, 'epoch': 3.443423881180379}\nEpoch: 3.4473324213406293, Logs: {'loss': 0.0456, 'grad_norm': 0.09158007055521011, 'learning_rate': 1.3107896794370603e-05, 'epoch': 3.4473324213406293}\nEpoch: 3.4512409615008792, Logs: {'loss': 0.0453, 'grad_norm': 0.10012983530759811, 'learning_rate': 1.310007818608288e-05, 'epoch': 3.4512409615008792}\nEpoch: 3.4551495016611296, Logs: {'loss': 0.0437, 'grad_norm': 0.15732769668102264, 'learning_rate': 1.3092259577795154e-05, 'epoch': 3.4551495016611296}\nEpoch: 3.45905804182138, Logs: {'loss': 0.0461, 'grad_norm': 0.20036229491233826, 'learning_rate': 1.3084440969507429e-05, 'epoch': 3.45905804182138}\nEpoch: 3.4629665819816298, Logs: {'loss': 0.0449, 'grad_norm': 0.24447357654571533, 'learning_rate': 1.3076622361219704e-05, 'epoch': 3.4629665819816298}\nEpoch: 3.46687512214188, Logs: {'loss': 0.0462, 'grad_norm': 0.13721811771392822, 'learning_rate': 1.306880375293198e-05, 'epoch': 3.46687512214188}\nEpoch: 3.47078366230213, Logs: {'loss': 0.0396, 'grad_norm': 0.13419803977012634, 'learning_rate': 1.3060985144644254e-05, 'epoch': 3.47078366230213}\nEpoch: 3.4746922024623803, Logs: {'loss': 0.0424, 'grad_norm': 0.10814777761697769, 'learning_rate': 1.305316653635653e-05, 'epoch': 3.4746922024623803}\nEpoch: 3.4786007426226306, Logs: {'loss': 0.0454, 'grad_norm': 0.19808940589427948, 'learning_rate': 1.3045347928068805e-05, 'epoch': 3.4786007426226306}\nEpoch: 3.4825092827828805, Logs: {'loss': 0.0427, 'grad_norm': 0.15708908438682556, 'learning_rate': 1.303752931978108e-05, 'epoch': 3.4825092827828805}\nEpoch: 3.486417822943131, Logs: {'loss': 0.0465, 'grad_norm': 0.28641098737716675, 'learning_rate': 1.3029710711493355e-05, 'epoch': 3.486417822943131}\nEpoch: 3.4903263631033807, Logs: {'loss': 0.0464, 'grad_norm': 0.10891515761613846, 'learning_rate': 1.3021892103205631e-05, 'epoch': 3.4903263631033807}\nEpoch: 3.494234903263631, Logs: {'loss': 0.0478, 'grad_norm': 0.17767176032066345, 'learning_rate': 1.3014073494917904e-05, 'epoch': 3.494234903263631}\nEpoch: 3.4981434434238814, Logs: {'loss': 0.0426, 'grad_norm': 0.14982973039150238, 'learning_rate': 1.3006254886630181e-05, 'epoch': 3.4981434434238814}\nEpoch: 3.5020519835841313, Logs: {'loss': 0.0441, 'grad_norm': 0.11366652697324753, 'learning_rate': 1.2998436278342456e-05, 'epoch': 3.5020519835841313}\nEpoch: 3.5059605237443816, Logs: {'loss': 0.0463, 'grad_norm': 0.28687843680381775, 'learning_rate': 1.299061767005473e-05, 'epoch': 3.5059605237443816}\nEpoch: 3.5098690639046315, Logs: {'loss': 0.0425, 'grad_norm': 0.21302124857902527, 'learning_rate': 1.2982799061767006e-05, 'epoch': 3.5098690639046315}\nEpoch: 3.513777604064882, Logs: {'loss': 0.0463, 'grad_norm': 0.3393036127090454, 'learning_rate': 1.2974980453479282e-05, 'epoch': 3.513777604064882}\nEpoch: 3.5176861442251317, Logs: {'loss': 0.0482, 'grad_norm': 0.2634370028972626, 'learning_rate': 1.2967161845191555e-05, 'epoch': 3.5176861442251317}\nEpoch: 3.521594684385382, Logs: {'loss': 0.042, 'grad_norm': 0.3760010600090027, 'learning_rate': 1.2959343236903832e-05, 'epoch': 3.521594684385382}\nEpoch: 3.5255032245456324, Logs: {'loss': 0.0435, 'grad_norm': 0.1849537193775177, 'learning_rate': 1.2951524628616108e-05, 'epoch': 3.5255032245456324}\nEpoch: 3.5294117647058822, Logs: {'loss': 0.0502, 'grad_norm': 0.11750133335590363, 'learning_rate': 1.2943706020328382e-05, 'epoch': 3.5294117647058822}\nEpoch: 3.5333203048661326, Logs: {'loss': 0.0478, 'grad_norm': 0.10735897719860077, 'learning_rate': 1.2935887412040658e-05, 'epoch': 3.5333203048661326}\nEpoch: 3.537228845026383, Logs: {'loss': 0.0418, 'grad_norm': 0.0890931710600853, 'learning_rate': 1.2928068803752933e-05, 'epoch': 3.537228845026383}\nEpoch: 3.541137385186633, Logs: {'loss': 0.0461, 'grad_norm': 0.13969188928604126, 'learning_rate': 1.292025019546521e-05, 'epoch': 3.541137385186633}\nEpoch: 3.5450459253468827, Logs: {'loss': 0.0397, 'grad_norm': 0.2823050320148468, 'learning_rate': 1.2912431587177483e-05, 'epoch': 3.5450459253468827}\nEpoch: 3.548954465507133, Logs: {'loss': 0.0426, 'grad_norm': 0.18069708347320557, 'learning_rate': 1.290461297888976e-05, 'epoch': 3.548954465507133}\nEpoch: 3.5528630056673833, Logs: {'loss': 0.045, 'grad_norm': 0.09588892012834549, 'learning_rate': 1.2896794370602034e-05, 'epoch': 3.5528630056673833}\nEpoch: 3.556771545827633, Logs: {'loss': 0.048, 'grad_norm': 0.19850899279117584, 'learning_rate': 1.2888975762314309e-05, 'epoch': 3.556771545827633}\nEpoch: 3.5606800859878835, Logs: {'loss': 0.0419, 'grad_norm': 0.1682162880897522, 'learning_rate': 1.2881157154026584e-05, 'epoch': 3.5606800859878835}\nEpoch: 3.564588626148134, Logs: {'loss': 0.0453, 'grad_norm': 0.15611667931079865, 'learning_rate': 1.287333854573886e-05, 'epoch': 3.564588626148134}\nEpoch: 3.5684971663083838, Logs: {'loss': 0.0418, 'grad_norm': 0.19307583570480347, 'learning_rate': 1.2865519937451134e-05, 'epoch': 3.5684971663083838}\nEpoch: 3.572405706468634, Logs: {'loss': 0.0391, 'grad_norm': 0.16446056962013245, 'learning_rate': 1.285770132916341e-05, 'epoch': 3.572405706468634}\nEpoch: 3.576314246628884, Logs: {'loss': 0.0455, 'grad_norm': 0.48151740431785583, 'learning_rate': 1.2849882720875687e-05, 'epoch': 3.576314246628884}\nEpoch: 3.5802227867891343, Logs: {'loss': 0.0422, 'grad_norm': 0.15332262217998505, 'learning_rate': 1.284206411258796e-05, 'epoch': 3.5802227867891343}\nEpoch: 3.584131326949384, Logs: {'loss': 0.0548, 'grad_norm': 0.16831833124160767, 'learning_rate': 1.2834245504300235e-05, 'epoch': 3.584131326949384}\nEpoch: 3.5880398671096345, Logs: {'loss': 0.0495, 'grad_norm': 0.23724988102912903, 'learning_rate': 1.2826426896012511e-05, 'epoch': 3.5880398671096345}\nEpoch: 3.591948407269885, Logs: {'loss': 0.046, 'grad_norm': 0.48998457193374634, 'learning_rate': 1.2818608287724785e-05, 'epoch': 3.591948407269885}\nEpoch: 3.5958569474301347, Logs: {'loss': 0.0415, 'grad_norm': 0.18420742452144623, 'learning_rate': 1.2810789679437061e-05, 'epoch': 3.5958569474301347}\nEpoch: 3.599765487590385, Logs: {'loss': 0.0512, 'grad_norm': 0.10712811350822449, 'learning_rate': 1.2802971071149338e-05, 'epoch': 3.599765487590385}\nEpoch: 3.6036740277506354, Logs: {'loss': 0.0435, 'grad_norm': 0.1650659590959549, 'learning_rate': 1.2795152462861611e-05, 'epoch': 3.6036740277506354}\nEpoch: 3.6075825679108853, Logs: {'loss': 0.0496, 'grad_norm': 0.11913684010505676, 'learning_rate': 1.2787333854573887e-05, 'epoch': 3.6075825679108853}\nEpoch: 3.611491108071135, Logs: {'loss': 0.0413, 'grad_norm': 0.49006617069244385, 'learning_rate': 1.2779515246286162e-05, 'epoch': 3.611491108071135}\nEpoch: 3.6153996482313855, Logs: {'loss': 0.0455, 'grad_norm': 0.17801757156848907, 'learning_rate': 1.2771696637998437e-05, 'epoch': 3.6153996482313855}\nEpoch: 3.619308188391636, Logs: {'loss': 0.0391, 'grad_norm': 0.1476294845342636, 'learning_rate': 1.2763878029710712e-05, 'epoch': 3.619308188391636}\nEpoch: 3.6232167285518857, Logs: {'loss': 0.0396, 'grad_norm': 0.15005183219909668, 'learning_rate': 1.2756059421422989e-05, 'epoch': 3.6232167285518857}\nEpoch: 3.627125268712136, Logs: {'loss': 0.0434, 'grad_norm': 0.1306222379207611, 'learning_rate': 1.2748240813135262e-05, 'epoch': 3.627125268712136}\nEpoch: 3.6310338088723864, Logs: {'loss': 0.0439, 'grad_norm': 0.21145527064800262, 'learning_rate': 1.2740422204847538e-05, 'epoch': 3.6310338088723864}\nEpoch: 3.6349423490326362, Logs: {'loss': 0.0433, 'grad_norm': 0.2371564358472824, 'learning_rate': 1.2732603596559813e-05, 'epoch': 3.6349423490326362}\nEpoch: 3.6388508891928866, Logs: {'loss': 0.0393, 'grad_norm': 0.10235974192619324, 'learning_rate': 1.2724784988272088e-05, 'epoch': 3.6388508891928866}\nEpoch: 3.6427594293531365, Logs: {'loss': 0.0418, 'grad_norm': 0.14545337855815887, 'learning_rate': 1.2716966379984363e-05, 'epoch': 3.6427594293531365}\nEpoch: 3.646667969513387, Logs: {'loss': 0.0419, 'grad_norm': 0.14747829735279083, 'learning_rate': 1.270914777169664e-05, 'epoch': 3.646667969513387}\nEpoch: 3.6505765096736367, Logs: {'loss': 0.0445, 'grad_norm': 0.2529006898403168, 'learning_rate': 1.2701329163408916e-05, 'epoch': 3.6505765096736367}\nEpoch: 3.654485049833887, Logs: {'loss': 0.0444, 'grad_norm': 0.14289794862270355, 'learning_rate': 1.269351055512119e-05, 'epoch': 3.654485049833887}\nEpoch: 3.6583935899941373, Logs: {'loss': 0.0523, 'grad_norm': 0.2997419536113739, 'learning_rate': 1.2685691946833466e-05, 'epoch': 3.6583935899941373}\nEpoch: 3.662302130154387, Logs: {'loss': 0.0407, 'grad_norm': 0.2657180726528168, 'learning_rate': 1.267787333854574e-05, 'epoch': 3.662302130154387}\nEpoch: 3.6662106703146375, Logs: {'loss': 0.0385, 'grad_norm': 0.2334882616996765, 'learning_rate': 1.2670054730258014e-05, 'epoch': 3.6662106703146375}\nEpoch: 3.670119210474888, Logs: {'loss': 0.0454, 'grad_norm': 0.1001594290137291, 'learning_rate': 1.266223612197029e-05, 'epoch': 3.670119210474888}\nEpoch: 3.6740277506351378, Logs: {'loss': 0.0476, 'grad_norm': 0.15856541693210602, 'learning_rate': 1.2654417513682567e-05, 'epoch': 3.6740277506351378}\nEpoch: 3.677936290795388, Logs: {'loss': 0.0439, 'grad_norm': 0.10916759818792343, 'learning_rate': 1.264659890539484e-05, 'epoch': 3.677936290795388}\nEpoch: 3.681844830955638, Logs: {'loss': 0.0467, 'grad_norm': 0.21844357252120972, 'learning_rate': 1.2638780297107117e-05, 'epoch': 3.681844830955638}\nEpoch: 3.6857533711158883, Logs: {'loss': 0.0445, 'grad_norm': 0.3692493736743927, 'learning_rate': 1.2630961688819392e-05, 'epoch': 3.6857533711158883}\nEpoch: 3.689661911276138, Logs: {'loss': 0.0467, 'grad_norm': 0.13570044934749603, 'learning_rate': 1.2623143080531666e-05, 'epoch': 3.689661911276138}\nEpoch: 3.6935704514363885, Logs: {'loss': 0.0424, 'grad_norm': 0.1266576200723648, 'learning_rate': 1.2615324472243941e-05, 'epoch': 3.6935704514363885}\nEpoch: 3.697478991596639, Logs: {'loss': 0.0424, 'grad_norm': 0.21375223994255066, 'learning_rate': 1.2607505863956218e-05, 'epoch': 3.697478991596639}\nEpoch: 3.7013875317568887, Logs: {'loss': 0.0424, 'grad_norm': 0.10226050764322281, 'learning_rate': 1.2599687255668491e-05, 'epoch': 3.7013875317568887}\nEpoch: 3.705296071917139, Logs: {'loss': 0.0433, 'grad_norm': 0.20928269624710083, 'learning_rate': 1.2591868647380768e-05, 'epoch': 3.705296071917139}\nEpoch: 3.709204612077389, Logs: {'loss': 0.0438, 'grad_norm': 0.17060819268226624, 'learning_rate': 1.2584050039093042e-05, 'epoch': 3.709204612077389}\nEpoch: 3.7131131522376393, Logs: {'loss': 0.0421, 'grad_norm': 0.14663110673427582, 'learning_rate': 1.2576231430805317e-05, 'epoch': 3.7131131522376393}\nEpoch: 3.717021692397889, Logs: {'loss': 0.0402, 'grad_norm': 0.06874768435955048, 'learning_rate': 1.2568412822517592e-05, 'epoch': 3.717021692397889}\nEpoch: 3.7209302325581395, Logs: {'loss': 0.0442, 'grad_norm': 0.2559862732887268, 'learning_rate': 1.2560594214229869e-05, 'epoch': 3.7209302325581395}\nEpoch: 3.72483877271839, Logs: {'loss': 0.0466, 'grad_norm': 0.10703476518392563, 'learning_rate': 1.2552775605942142e-05, 'epoch': 3.72483877271839}\nEpoch: 3.7287473128786397, Logs: {'loss': 0.0428, 'grad_norm': 0.15872906148433685, 'learning_rate': 1.2544956997654418e-05, 'epoch': 3.7287473128786397}\nEpoch: 3.73265585303889, Logs: {'loss': 0.0416, 'grad_norm': 0.15945787727832794, 'learning_rate': 1.2537138389366695e-05, 'epoch': 3.73265585303889}\nEpoch: 3.7365643931991404, Logs: {'loss': 0.0432, 'grad_norm': 0.14897625148296356, 'learning_rate': 1.2529319781078968e-05, 'epoch': 3.7365643931991404}\nEpoch: 3.7404729333593902, Logs: {'loss': 0.0493, 'grad_norm': 0.29457253217697144, 'learning_rate': 1.2521501172791245e-05, 'epoch': 3.7404729333593902}\nEpoch: 3.7443814735196406, Logs: {'loss': 0.0461, 'grad_norm': 0.12405112385749817, 'learning_rate': 1.251368256450352e-05, 'epoch': 3.7443814735196406}\nEpoch: 3.7482900136798905, Logs: {'loss': 0.0441, 'grad_norm': 0.19712825119495392, 'learning_rate': 1.2505863956215793e-05, 'epoch': 3.7482900136798905}\nEpoch: 3.752198553840141, Logs: {'loss': 0.0428, 'grad_norm': 0.13230402767658234, 'learning_rate': 1.249804534792807e-05, 'epoch': 3.752198553840141}\nEpoch: 3.7561070940003907, Logs: {'loss': 0.0412, 'grad_norm': 0.3116990923881531, 'learning_rate': 1.2490226739640346e-05, 'epoch': 3.7561070940003907}\nEpoch: 3.760015634160641, Logs: {'loss': 0.0481, 'grad_norm': 0.8105075359344482, 'learning_rate': 1.248240813135262e-05, 'epoch': 3.760015634160641}\nEpoch: 3.7639241743208913, Logs: {'loss': 0.0469, 'grad_norm': 0.38564470410346985, 'learning_rate': 1.2474589523064896e-05, 'epoch': 3.7639241743208913}\nEpoch: 3.767832714481141, Logs: {'loss': 0.0407, 'grad_norm': 0.20669321715831757, 'learning_rate': 1.246677091477717e-05, 'epoch': 3.767832714481141}\nEpoch: 3.7717412546413915, Logs: {'loss': 0.0425, 'grad_norm': 0.18338610231876373, 'learning_rate': 1.2458952306489447e-05, 'epoch': 3.7717412546413915}\nEpoch: 3.7756497948016414, Logs: {'loss': 0.0451, 'grad_norm': 0.14393462240695953, 'learning_rate': 1.245113369820172e-05, 'epoch': 3.7756497948016414}\nEpoch: 3.7795583349618918, Logs: {'loss': 0.0423, 'grad_norm': 0.12365511804819107, 'learning_rate': 1.2443315089913997e-05, 'epoch': 3.7795583349618918}\nEpoch: 3.7834668751221416, Logs: {'loss': 0.0392, 'grad_norm': 0.25395825505256653, 'learning_rate': 1.2435496481626272e-05, 'epoch': 3.7834668751221416}\nEpoch: 3.787375415282392, Logs: {'loss': 0.0448, 'grad_norm': 0.16396138072013855, 'learning_rate': 1.2427677873338546e-05, 'epoch': 3.787375415282392}\nEpoch: 3.7912839554426423, Logs: {'loss': 0.0466, 'grad_norm': 0.2069532424211502, 'learning_rate': 1.2419859265050821e-05, 'epoch': 3.7912839554426423}\nEpoch: 3.795192495602892, Logs: {'loss': 0.045, 'grad_norm': 0.10893608629703522, 'learning_rate': 1.2412040656763098e-05, 'epoch': 3.795192495602892}\nEpoch: 3.7991010357631425, Logs: {'loss': 0.0475, 'grad_norm': 0.1352248638868332, 'learning_rate': 1.2404222048475371e-05, 'epoch': 3.7991010357631425}\nEpoch: 3.803009575923393, Logs: {'loss': 0.0431, 'grad_norm': 0.1153464987874031, 'learning_rate': 1.2396403440187648e-05, 'epoch': 3.803009575923393}\nEpoch: 3.8069181160836427, Logs: {'loss': 0.0385, 'grad_norm': 0.24429158866405487, 'learning_rate': 1.2388584831899924e-05, 'epoch': 3.8069181160836427}\nEpoch: 3.810826656243893, Logs: {'loss': 0.0438, 'grad_norm': 0.09792833775281906, 'learning_rate': 1.2380766223612197e-05, 'epoch': 3.810826656243893}\nEpoch: 3.814735196404143, Logs: {'loss': 0.0397, 'grad_norm': 0.19072411954402924, 'learning_rate': 1.2372947615324474e-05, 'epoch': 3.814735196404143}\nEpoch: 3.8186437365643933, Logs: {'loss': 0.0409, 'grad_norm': 0.1086345762014389, 'learning_rate': 1.2365129007036749e-05, 'epoch': 3.8186437365643933}\nEpoch: 3.822552276724643, Logs: {'loss': 0.0483, 'grad_norm': 0.23528635501861572, 'learning_rate': 1.2357310398749022e-05, 'epoch': 3.822552276724643}\nEpoch: 3.8264608168848935, Logs: {'loss': 0.042, 'grad_norm': 0.2347240298986435, 'learning_rate': 1.2349491790461299e-05, 'epoch': 3.8264608168848935}\nEpoch: 3.830369357045144, Logs: {'loss': 0.0501, 'grad_norm': 0.19296009838581085, 'learning_rate': 1.2341673182173575e-05, 'epoch': 3.830369357045144}\nEpoch: 3.8342778972053937, Logs: {'loss': 0.045, 'grad_norm': 0.13577549159526825, 'learning_rate': 1.2333854573885848e-05, 'epoch': 3.8342778972053937}\nEpoch: 3.838186437365644, Logs: {'loss': 0.0409, 'grad_norm': 0.19868211448192596, 'learning_rate': 1.2326035965598125e-05, 'epoch': 3.838186437365644}\nEpoch: 3.8420949775258944, Logs: {'loss': 0.0392, 'grad_norm': 0.13725760579109192, 'learning_rate': 1.23182173573104e-05, 'epoch': 3.8420949775258944}\nEpoch: 3.8460035176861442, Logs: {'loss': 0.0439, 'grad_norm': 0.17873398959636688, 'learning_rate': 1.2310398749022675e-05, 'epoch': 3.8460035176861442}\nEpoch: 3.849912057846394, Logs: {'loss': 0.049, 'grad_norm': 0.1756802201271057, 'learning_rate': 1.230258014073495e-05, 'epoch': 3.849912057846394}\nEpoch: 3.8538205980066444, Logs: {'loss': 0.0483, 'grad_norm': 0.15401224792003632, 'learning_rate': 1.2294761532447226e-05, 'epoch': 3.8538205980066444}\nEpoch: 3.8577291381668948, Logs: {'loss': 0.0465, 'grad_norm': 0.11694970726966858, 'learning_rate': 1.22869429241595e-05, 'epoch': 3.8577291381668948}\nEpoch: 3.8616376783271447, Logs: {'loss': 0.0432, 'grad_norm': 0.16527549922466278, 'learning_rate': 1.2279124315871776e-05, 'epoch': 3.8616376783271447}\nEpoch: 3.865546218487395, Logs: {'loss': 0.0476, 'grad_norm': 0.2571667730808258, 'learning_rate': 1.227130570758405e-05, 'epoch': 3.865546218487395}\nEpoch: 3.8694547586476453, Logs: {'loss': 0.0442, 'grad_norm': 0.579412579536438, 'learning_rate': 1.2263487099296327e-05, 'epoch': 3.8694547586476453}\nEpoch: 3.873363298807895, Logs: {'loss': 0.0412, 'grad_norm': 0.08113595098257065, 'learning_rate': 1.22556684910086e-05, 'epoch': 3.873363298807895}\nEpoch: 3.8772718389681455, Logs: {'loss': 0.0392, 'grad_norm': 0.28302860260009766, 'learning_rate': 1.2247849882720877e-05, 'epoch': 3.8772718389681455}\nEpoch: 3.8811803791283954, Logs: {'loss': 0.0431, 'grad_norm': 0.14000031352043152, 'learning_rate': 1.2240031274433153e-05, 'epoch': 3.8811803791283954}\nEpoch: 3.8850889192886457, Logs: {'loss': 0.0423, 'grad_norm': 0.1710783839225769, 'learning_rate': 1.2232212666145427e-05, 'epoch': 3.8850889192886457}\nEpoch: 3.8889974594488956, Logs: {'loss': 0.0453, 'grad_norm': 0.2581390142440796, 'learning_rate': 1.2224394057857703e-05, 'epoch': 3.8889974594488956}\nEpoch: 3.892905999609146, Logs: {'loss': 0.0448, 'grad_norm': 0.30178335309028625, 'learning_rate': 1.2216575449569978e-05, 'epoch': 3.892905999609146}\nEpoch: 3.8968145397693963, Logs: {'loss': 0.0434, 'grad_norm': 0.11301009356975555, 'learning_rate': 1.2208756841282253e-05, 'epoch': 3.8968145397693963}\nEpoch: 3.900723079929646, Logs: {'loss': 0.042, 'grad_norm': 0.13731591403484344, 'learning_rate': 1.2200938232994528e-05, 'epoch': 3.900723079929646}\nEpoch: 3.9046316200898965, Logs: {'loss': 0.0409, 'grad_norm': 0.17831739783287048, 'learning_rate': 1.2193119624706804e-05, 'epoch': 3.9046316200898965}\nEpoch: 3.908540160250147, Logs: {'loss': 0.046, 'grad_norm': 0.22233609855175018, 'learning_rate': 1.2185301016419078e-05, 'epoch': 3.908540160250147}\nEpoch: 3.9124487004103967, Logs: {'loss': 0.0445, 'grad_norm': 0.17061401903629303, 'learning_rate': 1.2177482408131354e-05, 'epoch': 3.9124487004103967}\nEpoch: 3.9163572405706466, Logs: {'loss': 0.0463, 'grad_norm': 0.12718093395233154, 'learning_rate': 1.2169663799843629e-05, 'epoch': 3.9163572405706466}\nEpoch: 3.920265780730897, Logs: {'loss': 0.0414, 'grad_norm': 0.34648194909095764, 'learning_rate': 1.2161845191555904e-05, 'epoch': 3.920265780730897}\nEpoch: 3.9241743208911473, Logs: {'loss': 0.0422, 'grad_norm': 0.2669019103050232, 'learning_rate': 1.2154026583268179e-05, 'epoch': 3.9241743208911473}\nEpoch: 3.928082861051397, Logs: {'loss': 0.0465, 'grad_norm': 0.11266491562128067, 'learning_rate': 1.2146207974980455e-05, 'epoch': 3.928082861051397}\nEpoch: 3.9319914012116475, Logs: {'loss': 0.0488, 'grad_norm': 0.16698108613491058, 'learning_rate': 1.2138389366692728e-05, 'epoch': 3.9319914012116475}\nEpoch: 3.935899941371898, Logs: {'loss': 0.0513, 'grad_norm': 0.42271721363067627, 'learning_rate': 1.2130570758405005e-05, 'epoch': 3.935899941371898}\nEpoch: 3.9398084815321477, Logs: {'loss': 0.0422, 'grad_norm': 0.19320574402809143, 'learning_rate': 1.2122752150117282e-05, 'epoch': 3.9398084815321477}\nEpoch: 3.943717021692398, Logs: {'loss': 0.0432, 'grad_norm': 0.12179933488368988, 'learning_rate': 1.2114933541829555e-05, 'epoch': 3.943717021692398}\nEpoch: 3.947625561852648, Logs: {'loss': 0.0436, 'grad_norm': 0.09096043556928635, 'learning_rate': 1.210711493354183e-05, 'epoch': 3.947625561852648}\nEpoch: 3.9515341020128982, Logs: {'loss': 0.0398, 'grad_norm': 0.37613415718078613, 'learning_rate': 1.2099296325254106e-05, 'epoch': 3.9515341020128982}\nEpoch: 3.955442642173148, Logs: {'loss': 0.0441, 'grad_norm': 0.22617939114570618, 'learning_rate': 1.209147771696638e-05, 'epoch': 3.955442642173148}\nEpoch: 3.9593511823333984, Logs: {'loss': 0.0475, 'grad_norm': 0.11038637906312943, 'learning_rate': 1.2083659108678656e-05, 'epoch': 3.9593511823333984}\nEpoch: 3.9632597224936488, Logs: {'loss': 0.0486, 'grad_norm': 0.16495753824710846, 'learning_rate': 1.2075840500390932e-05, 'epoch': 3.9632597224936488}\nEpoch: 3.9671682626538987, Logs: {'loss': 0.0407, 'grad_norm': 0.4012257754802704, 'learning_rate': 1.2068021892103206e-05, 'epoch': 3.9671682626538987}\nEpoch: 3.971076802814149, Logs: {'loss': 0.0424, 'grad_norm': 0.17080232501029968, 'learning_rate': 1.2060203283815482e-05, 'epoch': 3.971076802814149}\nEpoch: 3.9749853429743993, Logs: {'loss': 0.0482, 'grad_norm': 0.3718162477016449, 'learning_rate': 1.2052384675527757e-05, 'epoch': 3.9749853429743993}\nEpoch: 3.978893883134649, Logs: {'loss': 0.0452, 'grad_norm': 0.13068416714668274, 'learning_rate': 1.2044566067240034e-05, 'epoch': 3.978893883134649}\nEpoch: 3.982802423294899, Logs: {'loss': 0.0465, 'grad_norm': 0.3637269139289856, 'learning_rate': 1.2036747458952307e-05, 'epoch': 3.982802423294899}\nEpoch: 3.9867109634551494, Logs: {'loss': 0.0463, 'grad_norm': 0.21532955765724182, 'learning_rate': 1.2028928850664583e-05, 'epoch': 3.9867109634551494}\nEpoch: 3.9906195036153997, Logs: {'loss': 0.0418, 'grad_norm': 0.08864965289831161, 'learning_rate': 1.2021110242376858e-05, 'epoch': 3.9906195036153997}\nEpoch: 3.9945280437756496, Logs: {'loss': 0.0491, 'grad_norm': 0.149287149310112, 'learning_rate': 1.2013291634089133e-05, 'epoch': 3.9945280437756496}\nEpoch: 3.9984365839359, Logs: {'loss': 0.0436, 'grad_norm': 0.2090764343738556, 'learning_rate': 1.2005473025801408e-05, 'epoch': 3.9984365839359}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4.0, Logs: {'eval_loss': 0.04340833052992821, 'eval_bleu': 1.2426370595790903e-09, 'eval_accuracy': 0.9444390148553558, 'eval_runtime': 349.2939, 'eval_samples_per_second': 58.587, 'eval_steps_per_second': 3.662, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4.00234512409615, Logs: {'loss': 0.0354, 'grad_norm': 0.3446197807788849, 'learning_rate': 1.1998436278342455e-05, 'epoch': 4.00234512409615}\nEpoch: 4.0062536642564, Logs: {'loss': 0.0503, 'grad_norm': 0.27516886591911316, 'learning_rate': 1.1990617670054732e-05, 'epoch': 4.0062536642564}\nEpoch: 4.01016220441665, Logs: {'loss': 0.0427, 'grad_norm': 0.1182311400771141, 'learning_rate': 1.1982799061767005e-05, 'epoch': 4.01016220441665}\nEpoch: 4.014070744576901, Logs: {'loss': 0.0429, 'grad_norm': 0.1643344610929489, 'learning_rate': 1.1974980453479282e-05, 'epoch': 4.014070744576901}\nEpoch: 4.017979284737151, Logs: {'loss': 0.0436, 'grad_norm': 0.11349458247423172, 'learning_rate': 1.1967161845191556e-05, 'epoch': 4.017979284737151}\nEpoch: 4.021887824897401, Logs: {'loss': 0.0434, 'grad_norm': 0.10106781125068665, 'learning_rate': 1.1959343236903833e-05, 'epoch': 4.021887824897401}\nEpoch: 4.025796365057651, Logs: {'loss': 0.0442, 'grad_norm': 0.1695329248905182, 'learning_rate': 1.1951524628616106e-05, 'epoch': 4.025796365057651}\nEpoch: 4.029704905217901, Logs: {'loss': 0.0442, 'grad_norm': 0.5879572033882141, 'learning_rate': 1.1943706020328383e-05, 'epoch': 4.029704905217901}\nEpoch: 4.033613445378151, Logs: {'loss': 0.0446, 'grad_norm': 0.18580421805381775, 'learning_rate': 1.193588741204066e-05, 'epoch': 4.033613445378151}\nEpoch: 4.037521985538401, Logs: {'loss': 0.0394, 'grad_norm': 0.11355792731046677, 'learning_rate': 1.1928068803752932e-05, 'epoch': 4.037521985538401}\nEpoch: 4.041430525698652, Logs: {'loss': 0.0457, 'grad_norm': 0.35029178857803345, 'learning_rate': 1.1920250195465209e-05, 'epoch': 4.041430525698652}\nEpoch: 4.045339065858902, Logs: {'loss': 0.047, 'grad_norm': 0.21263599395751953, 'learning_rate': 1.1912431587177484e-05, 'epoch': 4.045339065858902}\nEpoch: 4.049247606019152, Logs: {'loss': 0.0477, 'grad_norm': 0.3122541606426239, 'learning_rate': 1.1904612978889759e-05, 'epoch': 4.049247606019152}\nEpoch: 4.053156146179402, Logs: {'loss': 0.0445, 'grad_norm': 0.13120082020759583, 'learning_rate': 1.1896794370602034e-05, 'epoch': 4.053156146179402}\nEpoch: 4.057064686339652, Logs: {'loss': 0.0414, 'grad_norm': 0.12402278184890747, 'learning_rate': 1.188897576231431e-05, 'epoch': 4.057064686339652}\nEpoch: 4.060973226499902, Logs: {'loss': 0.0421, 'grad_norm': 0.2073986828327179, 'learning_rate': 1.1881157154026583e-05, 'epoch': 4.060973226499902}\nEpoch: 4.064881766660152, Logs: {'loss': 0.0427, 'grad_norm': 0.37177807092666626, 'learning_rate': 1.187333854573886e-05, 'epoch': 4.064881766660152}\nEpoch: 4.068790306820403, Logs: {'loss': 0.0437, 'grad_norm': 0.16500698029994965, 'learning_rate': 1.1865519937451135e-05, 'epoch': 4.068790306820403}\nEpoch: 4.072698846980653, Logs: {'loss': 0.0441, 'grad_norm': 0.3750249445438385, 'learning_rate': 1.185770132916341e-05, 'epoch': 4.072698846980653}\nEpoch: 4.0766073871409025, Logs: {'loss': 0.0444, 'grad_norm': 0.329497367143631, 'learning_rate': 1.1849882720875685e-05, 'epoch': 4.0766073871409025}\nEpoch: 4.080515927301153, Logs: {'loss': 0.0494, 'grad_norm': 0.10603772103786469, 'learning_rate': 1.1842064112587961e-05, 'epoch': 4.080515927301153}\nEpoch: 4.084424467461403, Logs: {'loss': 0.0422, 'grad_norm': 0.2631414234638214, 'learning_rate': 1.1834245504300234e-05, 'epoch': 4.084424467461403}\nEpoch: 4.088333007621653, Logs: {'loss': 0.0495, 'grad_norm': 0.12541347742080688, 'learning_rate': 1.182642689601251e-05, 'epoch': 4.088333007621653}\nEpoch: 4.092241547781904, Logs: {'loss': 0.0415, 'grad_norm': 0.10165548324584961, 'learning_rate': 1.1818608287724787e-05, 'epoch': 4.092241547781904}\nEpoch: 4.096150087942154, Logs: {'loss': 0.0472, 'grad_norm': 0.19658778607845306, 'learning_rate': 1.181078967943706e-05, 'epoch': 4.096150087942154}\nEpoch: 4.100058628102404, Logs: {'loss': 0.0457, 'grad_norm': 0.3140968978404999, 'learning_rate': 1.1802971071149335e-05, 'epoch': 4.100058628102404}\nEpoch: 4.1039671682626535, Logs: {'loss': 0.0409, 'grad_norm': 0.3785325288772583, 'learning_rate': 1.1795152462861612e-05, 'epoch': 4.1039671682626535}\nEpoch: 4.107875708422904, Logs: {'loss': 0.046, 'grad_norm': 0.13445501029491425, 'learning_rate': 1.1787333854573885e-05, 'epoch': 4.107875708422904}\nEpoch: 4.111784248583154, Logs: {'loss': 0.05, 'grad_norm': 0.1548907607793808, 'learning_rate': 1.1779515246286162e-05, 'epoch': 4.111784248583154}\nEpoch: 4.115692788743404, Logs: {'loss': 0.0509, 'grad_norm': 0.16727544367313385, 'learning_rate': 1.1771696637998438e-05, 'epoch': 4.115692788743404}\nEpoch: 4.119601328903655, Logs: {'loss': 0.0396, 'grad_norm': 0.15999747812747955, 'learning_rate': 1.1763878029710711e-05, 'epoch': 4.119601328903655}\nEpoch: 4.123509869063905, Logs: {'loss': 0.0367, 'grad_norm': 0.12637543678283691, 'learning_rate': 1.1756059421422988e-05, 'epoch': 4.123509869063905}\nEpoch: 4.127418409224155, Logs: {'loss': 0.0412, 'grad_norm': 2.6485185623168945, 'learning_rate': 1.1748240813135263e-05, 'epoch': 4.127418409224155}\nEpoch: 4.131326949384405, Logs: {'loss': 0.0421, 'grad_norm': 0.15358532965183258, 'learning_rate': 1.174042220484754e-05, 'epoch': 4.131326949384405}\nEpoch: 4.135235489544655, Logs: {'loss': 0.041, 'grad_norm': 0.2044518142938614, 'learning_rate': 1.1732603596559813e-05, 'epoch': 4.135235489544655}\nEpoch: 4.139144029704905, Logs: {'loss': 0.049, 'grad_norm': 0.462379515171051, 'learning_rate': 1.1724784988272089e-05, 'epoch': 4.139144029704905}\nEpoch: 4.143052569865155, Logs: {'loss': 0.0466, 'grad_norm': 0.2427399456501007, 'learning_rate': 1.1716966379984364e-05, 'epoch': 4.143052569865155}\nEpoch: 4.146961110025406, Logs: {'loss': 0.0438, 'grad_norm': 0.13258925080299377, 'learning_rate': 1.1709147771696639e-05, 'epoch': 4.146961110025406}\nEpoch: 4.150869650185656, Logs: {'loss': 0.0397, 'grad_norm': 0.1995832324028015, 'learning_rate': 1.1701329163408914e-05, 'epoch': 4.150869650185656}\nEpoch: 4.154778190345906, Logs: {'loss': 0.0425, 'grad_norm': 0.10816814750432968, 'learning_rate': 1.169351055512119e-05, 'epoch': 4.154778190345906}\nEpoch: 4.158686730506156, Logs: {'loss': 0.044, 'grad_norm': 0.13724392652511597, 'learning_rate': 1.1685691946833463e-05, 'epoch': 4.158686730506156}\nEpoch: 4.162595270666406, Logs: {'loss': 0.0431, 'grad_norm': 0.17489667236804962, 'learning_rate': 1.167787333854574e-05, 'epoch': 4.162595270666406}\nEpoch: 4.166503810826656, Logs: {'loss': 0.0462, 'grad_norm': 0.22626163065433502, 'learning_rate': 1.1670054730258017e-05, 'epoch': 4.166503810826656}\nEpoch: 4.170412350986906, Logs: {'loss': 0.04, 'grad_norm': 0.12070359289646149, 'learning_rate': 1.166223612197029e-05, 'epoch': 4.170412350986906}\nEpoch: 4.174320891147157, Logs: {'loss': 0.045, 'grad_norm': 0.2102891206741333, 'learning_rate': 1.1654417513682565e-05, 'epoch': 4.174320891147157}\nEpoch: 4.178229431307407, Logs: {'loss': 0.0409, 'grad_norm': 0.3631370961666107, 'learning_rate': 1.1646598905394841e-05, 'epoch': 4.178229431307407}\nEpoch: 4.1821379714676565, Logs: {'loss': 0.044, 'grad_norm': 0.1611548811197281, 'learning_rate': 1.1638780297107114e-05, 'epoch': 4.1821379714676565}\nEpoch: 4.186046511627907, Logs: {'loss': 0.0465, 'grad_norm': 0.2044692188501358, 'learning_rate': 1.1630961688819391e-05, 'epoch': 4.186046511627907}\nEpoch: 4.189955051788157, Logs: {'loss': 0.0441, 'grad_norm': 0.11357387900352478, 'learning_rate': 1.1623143080531667e-05, 'epoch': 4.189955051788157}\nEpoch: 4.193863591948407, Logs: {'loss': 0.0427, 'grad_norm': 0.2459518313407898, 'learning_rate': 1.161532447224394e-05, 'epoch': 4.193863591948407}\nEpoch: 4.197772132108657, Logs: {'loss': 0.0413, 'grad_norm': 0.2918992042541504, 'learning_rate': 1.1607505863956217e-05, 'epoch': 4.197772132108657}\nEpoch: 4.201680672268908, Logs: {'loss': 0.0424, 'grad_norm': 0.2643497586250305, 'learning_rate': 1.1599687255668492e-05, 'epoch': 4.201680672268908}\nEpoch: 4.205589212429158, Logs: {'loss': 0.0422, 'grad_norm': 0.2015983760356903, 'learning_rate': 1.1591868647380767e-05, 'epoch': 4.205589212429158}\nEpoch: 4.2094977525894075, Logs: {'loss': 0.0396, 'grad_norm': 0.15174958109855652, 'learning_rate': 1.1584050039093042e-05, 'epoch': 4.2094977525894075}\nEpoch: 4.213406292749658, Logs: {'loss': 0.0404, 'grad_norm': 0.2698170840740204, 'learning_rate': 1.1576231430805318e-05, 'epoch': 4.213406292749658}\nEpoch: 4.217314832909908, Logs: {'loss': 0.0475, 'grad_norm': 0.12659522891044617, 'learning_rate': 1.1568412822517592e-05, 'epoch': 4.217314832909908}\nEpoch: 4.221223373070158, Logs: {'loss': 0.0421, 'grad_norm': 0.14260798692703247, 'learning_rate': 1.1560594214229868e-05, 'epoch': 4.221223373070158}\nEpoch: 4.225131913230409, Logs: {'loss': 0.0382, 'grad_norm': 0.1249101534485817, 'learning_rate': 1.1552775605942143e-05, 'epoch': 4.225131913230409}\nEpoch: 4.229040453390659, Logs: {'loss': 0.0423, 'grad_norm': 0.12644606828689575, 'learning_rate': 1.1544956997654418e-05, 'epoch': 4.229040453390659}\nEpoch: 4.232948993550909, Logs: {'loss': 0.0399, 'grad_norm': 0.14935588836669922, 'learning_rate': 1.1537138389366693e-05, 'epoch': 4.232948993550909}\nEpoch: 4.2368575337111585, Logs: {'loss': 0.0455, 'grad_norm': 0.07919733226299286, 'learning_rate': 1.152931978107897e-05, 'epoch': 4.2368575337111585}\nEpoch: 4.240766073871409, Logs: {'loss': 0.042, 'grad_norm': 0.15829375386238098, 'learning_rate': 1.1521501172791246e-05, 'epoch': 4.240766073871409}\nEpoch: 4.244674614031659, Logs: {'loss': 0.0423, 'grad_norm': 0.3932701647281647, 'learning_rate': 1.1513682564503519e-05, 'epoch': 4.244674614031659}\nEpoch: 4.248583154191909, Logs: {'loss': 0.0522, 'grad_norm': 0.21029148995876312, 'learning_rate': 1.1505863956215796e-05, 'epoch': 4.248583154191909}\nEpoch: 4.25249169435216, Logs: {'loss': 0.0417, 'grad_norm': 0.15127331018447876, 'learning_rate': 1.149804534792807e-05, 'epoch': 4.25249169435216}\nEpoch: 4.25640023451241, Logs: {'loss': 0.0407, 'grad_norm': 0.4236327111721039, 'learning_rate': 1.1490226739640344e-05, 'epoch': 4.25640023451241}\nEpoch: 4.26030877467266, Logs: {'loss': 0.0393, 'grad_norm': 0.15512005984783173, 'learning_rate': 1.148240813135262e-05, 'epoch': 4.26030877467266}\nEpoch: 4.26421731483291, Logs: {'loss': 0.0465, 'grad_norm': 0.07488498091697693, 'learning_rate': 1.1474589523064897e-05, 'epoch': 4.26421731483291}\nEpoch: 4.26812585499316, Logs: {'loss': 0.0481, 'grad_norm': 0.15415501594543457, 'learning_rate': 1.146677091477717e-05, 'epoch': 4.26812585499316}\nEpoch: 4.27203439515341, Logs: {'loss': 0.0386, 'grad_norm': 0.3645993173122406, 'learning_rate': 1.1458952306489446e-05, 'epoch': 4.27203439515341}\nEpoch: 4.27594293531366, Logs: {'loss': 0.0428, 'grad_norm': 0.1189403161406517, 'learning_rate': 1.1451133698201721e-05, 'epoch': 4.27594293531366}\nEpoch: 4.279851475473911, Logs: {'loss': 0.0428, 'grad_norm': 0.07161183655261993, 'learning_rate': 1.1443315089913996e-05, 'epoch': 4.279851475473911}\nEpoch: 4.283760015634161, Logs: {'loss': 0.0431, 'grad_norm': 0.21379117667675018, 'learning_rate': 1.1435496481626271e-05, 'epoch': 4.283760015634161}\nEpoch: 4.2876685557944105, Logs: {'loss': 0.0465, 'grad_norm': 0.2065909504890442, 'learning_rate': 1.1427677873338548e-05, 'epoch': 4.2876685557944105}\nEpoch: 4.291577095954661, Logs: {'loss': 0.0414, 'grad_norm': 0.07482057064771652, 'learning_rate': 1.141985926505082e-05, 'epoch': 4.291577095954661}\nEpoch: 4.295485636114911, Logs: {'loss': 0.0427, 'grad_norm': 0.1489870846271515, 'learning_rate': 1.1412040656763097e-05, 'epoch': 4.295485636114911}\nEpoch: 4.299394176275161, Logs: {'loss': 0.0449, 'grad_norm': 0.28921860456466675, 'learning_rate': 1.1404222048475372e-05, 'epoch': 4.299394176275161}\nEpoch: 4.303302716435411, Logs: {'loss': 0.0366, 'grad_norm': 0.2356443703174591, 'learning_rate': 1.1396403440187647e-05, 'epoch': 4.303302716435411}\nEpoch: 4.307211256595662, Logs: {'loss': 0.0465, 'grad_norm': 0.31972721219062805, 'learning_rate': 1.1388584831899922e-05, 'epoch': 4.307211256595662}\nEpoch: 4.311119796755912, Logs: {'loss': 0.0403, 'grad_norm': 0.2536433935165405, 'learning_rate': 1.1380766223612198e-05, 'epoch': 4.311119796755912}\nEpoch: 4.3150283369161615, Logs: {'loss': 0.0398, 'grad_norm': 0.10960277169942856, 'learning_rate': 1.1372947615324472e-05, 'epoch': 4.3150283369161615}\nEpoch: 4.318936877076412, Logs: {'loss': 0.0414, 'grad_norm': 0.19331006705760956, 'learning_rate': 1.1365129007036748e-05, 'epoch': 4.318936877076412}\nEpoch: 4.322845417236662, Logs: {'loss': 0.0422, 'grad_norm': 0.09687172621488571, 'learning_rate': 1.1357310398749025e-05, 'epoch': 4.322845417236662}\nEpoch: 4.326753957396912, Logs: {'loss': 0.0409, 'grad_norm': 0.24907433986663818, 'learning_rate': 1.1349491790461298e-05, 'epoch': 4.326753957396912}\nEpoch: 4.330662497557163, Logs: {'loss': 0.0441, 'grad_norm': 0.15645404160022736, 'learning_rate': 1.1341673182173575e-05, 'epoch': 4.330662497557163}\nEpoch: 4.334571037717413, Logs: {'loss': 0.0437, 'grad_norm': 0.18400542438030243, 'learning_rate': 1.133385457388585e-05, 'epoch': 4.334571037717413}\nEpoch: 4.338479577877663, Logs: {'loss': 0.0432, 'grad_norm': 0.20788784325122833, 'learning_rate': 1.1326035965598126e-05, 'epoch': 4.338479577877663}\nEpoch: 4.3423881180379125, Logs: {'loss': 0.0439, 'grad_norm': 0.13752232491970062, 'learning_rate': 1.1318217357310399e-05, 'epoch': 4.3423881180379125}\nEpoch: 4.346296658198163, Logs: {'loss': 0.039, 'grad_norm': 0.1494394987821579, 'learning_rate': 1.1310398749022676e-05, 'epoch': 4.346296658198163}\nEpoch: 4.350205198358413, Logs: {'loss': 0.045, 'grad_norm': 0.17454436421394348, 'learning_rate': 1.130258014073495e-05, 'epoch': 4.350205198358413}\nEpoch: 4.354113738518663, Logs: {'loss': 0.0388, 'grad_norm': 0.15473903715610504, 'learning_rate': 1.1294761532447225e-05, 'epoch': 4.354113738518663}\nEpoch: 4.358022278678914, Logs: {'loss': 0.0493, 'grad_norm': 0.1260230839252472, 'learning_rate': 1.12869429241595e-05, 'epoch': 4.358022278678914}\nEpoch: 4.361930818839164, Logs: {'loss': 0.0484, 'grad_norm': 0.19499491155147552, 'learning_rate': 1.1279124315871777e-05, 'epoch': 4.361930818839164}\nEpoch: 4.3658393589994136, Logs: {'loss': 0.0429, 'grad_norm': 0.21823656558990479, 'learning_rate': 1.127130570758405e-05, 'epoch': 4.3658393589994136}\nEpoch: 4.369747899159664, Logs: {'loss': 0.0457, 'grad_norm': 0.38852906227111816, 'learning_rate': 1.1263487099296327e-05, 'epoch': 4.369747899159664}\nEpoch: 4.373656439319914, Logs: {'loss': 0.0484, 'grad_norm': 0.1226084902882576, 'learning_rate': 1.1255668491008601e-05, 'epoch': 4.373656439319914}\nEpoch: 4.377564979480164, Logs: {'loss': 0.0383, 'grad_norm': 0.08908414840698242, 'learning_rate': 1.1247849882720876e-05, 'epoch': 4.377564979480164}\nEpoch: 4.381473519640414, Logs: {'loss': 0.0418, 'grad_norm': 0.13562405109405518, 'learning_rate': 1.1240031274433151e-05, 'epoch': 4.381473519640414}\nEpoch: 4.385382059800665, Logs: {'loss': 0.0412, 'grad_norm': 0.12830129265785217, 'learning_rate': 1.1232212666145428e-05, 'epoch': 4.385382059800665}\nEpoch: 4.389290599960915, Logs: {'loss': 0.0442, 'grad_norm': 0.10514416545629501, 'learning_rate': 1.1224394057857701e-05, 'epoch': 4.389290599960915}\nEpoch: 4.3931991401211645, Logs: {'loss': 0.0439, 'grad_norm': 0.09787565469741821, 'learning_rate': 1.1216575449569977e-05, 'epoch': 4.3931991401211645}\nEpoch: 4.397107680281415, Logs: {'loss': 0.0498, 'grad_norm': 0.10198016464710236, 'learning_rate': 1.1208756841282254e-05, 'epoch': 4.397107680281415}\nEpoch: 4.401016220441665, Logs: {'loss': 0.0356, 'grad_norm': 0.10609636455774307, 'learning_rate': 1.1200938232994527e-05, 'epoch': 4.401016220441665}\nEpoch: 4.404924760601915, Logs: {'loss': 0.0457, 'grad_norm': 0.21345046162605286, 'learning_rate': 1.1193119624706804e-05, 'epoch': 4.404924760601915}\nEpoch: 4.408833300762165, Logs: {'loss': 0.0416, 'grad_norm': 0.23104174435138702, 'learning_rate': 1.1185301016419079e-05, 'epoch': 4.408833300762165}\nEpoch: 4.412741840922416, Logs: {'loss': 0.0474, 'grad_norm': 0.0920545905828476, 'learning_rate': 1.1177482408131353e-05, 'epoch': 4.412741840922416}\nEpoch: 4.416650381082666, Logs: {'loss': 0.0458, 'grad_norm': 0.2586698830127716, 'learning_rate': 1.1169663799843628e-05, 'epoch': 4.416650381082666}\nEpoch: 4.4205589212429155, Logs: {'loss': 0.0414, 'grad_norm': 0.2980177104473114, 'learning_rate': 1.1161845191555905e-05, 'epoch': 4.4205589212429155}\nEpoch: 4.424467461403166, Logs: {'loss': 0.0495, 'grad_norm': 0.24967169761657715, 'learning_rate': 1.1154026583268178e-05, 'epoch': 4.424467461403166}\nEpoch: 4.428376001563416, Logs: {'loss': 0.043, 'grad_norm': 0.12738582491874695, 'learning_rate': 1.1146207974980455e-05, 'epoch': 4.428376001563416}\nEpoch: 4.432284541723666, Logs: {'loss': 0.0455, 'grad_norm': 0.19234265387058258, 'learning_rate': 1.113838936669273e-05, 'epoch': 4.432284541723666}\nEpoch: 4.436193081883916, Logs: {'loss': 0.0452, 'grad_norm': 0.20773907005786896, 'learning_rate': 1.1130570758405004e-05, 'epoch': 4.436193081883916}\nEpoch: 4.440101622044167, Logs: {'loss': 0.0437, 'grad_norm': 0.08221425116062164, 'learning_rate': 1.112275215011728e-05, 'epoch': 4.440101622044167}\nEpoch: 4.444010162204417, Logs: {'loss': 0.044, 'grad_norm': 0.15000590682029724, 'learning_rate': 1.1114933541829556e-05, 'epoch': 4.444010162204417}\nEpoch: 4.4479187023646665, Logs: {'loss': 0.0458, 'grad_norm': 0.20118866860866547, 'learning_rate': 1.1107114933541832e-05, 'epoch': 4.4479187023646665}\nEpoch: 4.451827242524917, Logs: {'loss': 0.0434, 'grad_norm': 0.1486838012933731, 'learning_rate': 1.1099296325254106e-05, 'epoch': 4.451827242524917}\nEpoch: 4.455735782685167, Logs: {'loss': 0.0396, 'grad_norm': 0.09693807363510132, 'learning_rate': 1.109147771696638e-05, 'epoch': 4.455735782685167}\nEpoch: 4.459644322845417, Logs: {'loss': 0.0388, 'grad_norm': 0.16660727560520172, 'learning_rate': 1.1083659108678657e-05, 'epoch': 4.459644322845417}\nEpoch: 4.463552863005668, Logs: {'loss': 0.0443, 'grad_norm': 0.12233119457960129, 'learning_rate': 1.107584050039093e-05, 'epoch': 4.463552863005668}\nEpoch: 4.467461403165918, Logs: {'loss': 0.0405, 'grad_norm': 0.3390578329563141, 'learning_rate': 1.1068021892103207e-05, 'epoch': 4.467461403165918}\nEpoch: 4.4713699433261676, Logs: {'loss': 0.0495, 'grad_norm': 0.09534391760826111, 'learning_rate': 1.1060203283815483e-05, 'epoch': 4.4713699433261676}\nEpoch: 4.475278483486417, Logs: {'loss': 0.0502, 'grad_norm': 0.2543826401233673, 'learning_rate': 1.1052384675527756e-05, 'epoch': 4.475278483486417}\nEpoch: 4.479187023646668, Logs: {'loss': 0.0399, 'grad_norm': 0.1387452930212021, 'learning_rate': 1.1044566067240033e-05, 'epoch': 4.479187023646668}\nEpoch: 4.483095563806918, Logs: {'loss': 0.0459, 'grad_norm': 0.32659485936164856, 'learning_rate': 1.1036747458952308e-05, 'epoch': 4.483095563806918}\nEpoch: 4.487004103967168, Logs: {'loss': 0.0426, 'grad_norm': 0.1731121987104416, 'learning_rate': 1.1028928850664583e-05, 'epoch': 4.487004103967168}\nEpoch: 4.490912644127419, Logs: {'loss': 0.0451, 'grad_norm': 0.231248140335083, 'learning_rate': 1.1021110242376858e-05, 'epoch': 4.490912644127419}\nEpoch: 4.494821184287669, Logs: {'loss': 0.0458, 'grad_norm': 0.27419859170913696, 'learning_rate': 1.1013291634089134e-05, 'epoch': 4.494821184287669}\nEpoch: 4.4987297244479185, Logs: {'loss': 0.0432, 'grad_norm': 0.09132228791713715, 'learning_rate': 1.1005473025801407e-05, 'epoch': 4.4987297244479185}\nEpoch: 4.502638264608169, Logs: {'loss': 0.0442, 'grad_norm': 0.346888929605484, 'learning_rate': 1.0997654417513684e-05, 'epoch': 4.502638264608169}\nEpoch: 4.506546804768419, Logs: {'loss': 0.0465, 'grad_norm': 0.1746961623430252, 'learning_rate': 1.0989835809225959e-05, 'epoch': 4.506546804768419}\nEpoch: 4.510455344928669, Logs: {'loss': 0.0449, 'grad_norm': 0.13450570404529572, 'learning_rate': 1.0982017200938234e-05, 'epoch': 4.510455344928669}\nEpoch: 4.514363885088919, Logs: {'loss': 0.0435, 'grad_norm': 0.24268555641174316, 'learning_rate': 1.0974198592650508e-05, 'epoch': 4.514363885088919}\nEpoch: 4.51827242524917, Logs: {'loss': 0.0419, 'grad_norm': 0.12967929244041443, 'learning_rate': 1.0966379984362785e-05, 'epoch': 4.51827242524917}\nEpoch: 4.52218096540942, Logs: {'loss': 0.0372, 'grad_norm': 0.26602229475975037, 'learning_rate': 1.0958561376075058e-05, 'epoch': 4.52218096540942}\nEpoch: 4.5260895055696695, Logs: {'loss': 0.0459, 'grad_norm': 0.24223975837230682, 'learning_rate': 1.0950742767787335e-05, 'epoch': 4.5260895055696695}\nEpoch: 4.52999804572992, Logs: {'loss': 0.0413, 'grad_norm': 0.15036480128765106, 'learning_rate': 1.0942924159499611e-05, 'epoch': 4.52999804572992}\nEpoch: 4.53390658589017, Logs: {'loss': 0.0434, 'grad_norm': 0.09841682016849518, 'learning_rate': 1.0935105551211884e-05, 'epoch': 4.53390658589017}\nEpoch: 4.53781512605042, Logs: {'loss': 0.0379, 'grad_norm': 0.1457986682653427, 'learning_rate': 1.092728694292416e-05, 'epoch': 4.53781512605042}\nEpoch: 4.54172366621067, Logs: {'loss': 0.045, 'grad_norm': 0.26384082436561584, 'learning_rate': 1.0919468334636436e-05, 'epoch': 4.54172366621067}\nEpoch: 4.545632206370921, Logs: {'loss': 0.0402, 'grad_norm': 0.258017897605896, 'learning_rate': 1.0911649726348709e-05, 'epoch': 4.545632206370921}\nEpoch: 4.549540746531171, Logs: {'loss': 0.0367, 'grad_norm': 0.18261341750621796, 'learning_rate': 1.0903831118060986e-05, 'epoch': 4.549540746531171}\nEpoch: 4.5534492866914205, Logs: {'loss': 0.0456, 'grad_norm': 0.10507091134786606, 'learning_rate': 1.0896012509773262e-05, 'epoch': 4.5534492866914205}\nEpoch: 4.557357826851671, Logs: {'loss': 0.0417, 'grad_norm': 0.12791797518730164, 'learning_rate': 1.0888193901485537e-05, 'epoch': 4.557357826851671}\nEpoch: 4.561266367011921, Logs: {'loss': 0.0409, 'grad_norm': 0.17997950315475464, 'learning_rate': 1.0880375293197812e-05, 'epoch': 4.561266367011921}\nEpoch: 4.565174907172171, Logs: {'loss': 0.0505, 'grad_norm': 0.2585894763469696, 'learning_rate': 1.0872556684910087e-05, 'epoch': 4.565174907172171}\nEpoch: 4.569083447332421, Logs: {'loss': 0.039, 'grad_norm': 0.3422064781188965, 'learning_rate': 1.0864738076622363e-05, 'epoch': 4.569083447332421}\nEpoch: 4.572991987492672, Logs: {'loss': 0.0402, 'grad_norm': 0.24037019908428192, 'learning_rate': 1.0856919468334637e-05, 'epoch': 4.572991987492672}\nEpoch: 4.5769005276529215, Logs: {'loss': 0.0432, 'grad_norm': 0.3387781083583832, 'learning_rate': 1.0849100860046913e-05, 'epoch': 4.5769005276529215}\nEpoch: 4.580809067813171, Logs: {'loss': 0.041, 'grad_norm': 0.21858009696006775, 'learning_rate': 1.0841282251759188e-05, 'epoch': 4.580809067813171}\nEpoch: 4.584717607973422, Logs: {'loss': 0.0429, 'grad_norm': 0.0747416615486145, 'learning_rate': 1.0833463643471463e-05, 'epoch': 4.584717607973422}\nEpoch: 4.588626148133672, Logs: {'loss': 0.0524, 'grad_norm': 0.14547668397426605, 'learning_rate': 1.0825645035183738e-05, 'epoch': 4.588626148133672}\nEpoch: 4.592534688293922, Logs: {'loss': 0.0431, 'grad_norm': 0.18903225660324097, 'learning_rate': 1.0817826426896014e-05, 'epoch': 4.592534688293922}\nEpoch: 4.596443228454173, Logs: {'loss': 0.0419, 'grad_norm': 0.2696308195590973, 'learning_rate': 1.0810007818608287e-05, 'epoch': 4.596443228454173}\nEpoch: 4.600351768614423, Logs: {'loss': 0.039, 'grad_norm': 0.08414016664028168, 'learning_rate': 1.0802189210320564e-05, 'epoch': 4.600351768614423}\nEpoch: 4.6042603087746725, Logs: {'loss': 0.0442, 'grad_norm': 0.14464744925498962, 'learning_rate': 1.079437060203284e-05, 'epoch': 4.6042603087746725}\nEpoch: 4.608168848934923, Logs: {'loss': 0.0445, 'grad_norm': 0.12042830884456635, 'learning_rate': 1.0786551993745114e-05, 'epoch': 4.608168848934923}\nEpoch: 4.612077389095173, Logs: {'loss': 0.0416, 'grad_norm': 0.1652485430240631, 'learning_rate': 1.077873338545739e-05, 'epoch': 4.612077389095173}\nEpoch: 4.615985929255423, Logs: {'loss': 0.0447, 'grad_norm': 0.1719914674758911, 'learning_rate': 1.0770914777169665e-05, 'epoch': 4.615985929255423}\nEpoch: 4.619894469415673, Logs: {'loss': 0.0456, 'grad_norm': 0.0947422906756401, 'learning_rate': 1.0763096168881938e-05, 'epoch': 4.619894469415673}\nEpoch: 4.623803009575924, Logs: {'loss': 0.0474, 'grad_norm': 0.24897129833698273, 'learning_rate': 1.0755277560594215e-05, 'epoch': 4.623803009575924}\nEpoch: 4.627711549736174, Logs: {'loss': 0.0474, 'grad_norm': 0.11105182766914368, 'learning_rate': 1.0747458952306491e-05, 'epoch': 4.627711549736174}\nEpoch: 4.6316200898964235, Logs: {'loss': 0.0425, 'grad_norm': 0.09286808967590332, 'learning_rate': 1.0739640344018765e-05, 'epoch': 4.6316200898964235}\nEpoch: 4.635528630056674, Logs: {'loss': 0.0457, 'grad_norm': 0.18451480567455292, 'learning_rate': 1.0731821735731041e-05, 'epoch': 4.635528630056674}\nEpoch: 4.639437170216924, Logs: {'loss': 0.0411, 'grad_norm': 0.10914769023656845, 'learning_rate': 1.0724003127443316e-05, 'epoch': 4.639437170216924}\nEpoch: 4.643345710377174, Logs: {'loss': 0.0409, 'grad_norm': 0.09702642261981964, 'learning_rate': 1.0716184519155591e-05, 'epoch': 4.643345710377174}\nEpoch: 4.647254250537424, Logs: {'loss': 0.0478, 'grad_norm': 0.5804429054260254, 'learning_rate': 1.0708365910867866e-05, 'epoch': 4.647254250537424}\nEpoch: 4.651162790697675, Logs: {'loss': 0.0494, 'grad_norm': 0.12996073067188263, 'learning_rate': 1.0700547302580142e-05, 'epoch': 4.651162790697675}\nEpoch: 4.655071330857925, Logs: {'loss': 0.0408, 'grad_norm': 0.17970800399780273, 'learning_rate': 1.0692728694292416e-05, 'epoch': 4.655071330857925}\nEpoch: 4.6589798710181745, Logs: {'loss': 0.0418, 'grad_norm': 0.08882912993431091, 'learning_rate': 1.0684910086004692e-05, 'epoch': 4.6589798710181745}\nEpoch: 4.662888411178425, Logs: {'loss': 0.0406, 'grad_norm': 0.16995754837989807, 'learning_rate': 1.0677091477716967e-05, 'epoch': 4.662888411178425}\nEpoch: 4.666796951338675, Logs: {'loss': 0.0438, 'grad_norm': 0.3408859074115753, 'learning_rate': 1.0669272869429243e-05, 'epoch': 4.666796951338675}\nEpoch: 4.670705491498925, Logs: {'loss': 0.0423, 'grad_norm': 0.21530242264270782, 'learning_rate': 1.0661454261141517e-05, 'epoch': 4.670705491498925}\nEpoch: 4.674614031659175, Logs: {'loss': 0.0423, 'grad_norm': 0.16753040254116058, 'learning_rate': 1.0653635652853793e-05, 'epoch': 4.674614031659175}\nEpoch: 4.678522571819426, Logs: {'loss': 0.0443, 'grad_norm': 0.12277072668075562, 'learning_rate': 1.064581704456607e-05, 'epoch': 4.678522571819426}\nEpoch: 4.6824311119796755, Logs: {'loss': 0.0382, 'grad_norm': 0.12361886352300644, 'learning_rate': 1.0637998436278343e-05, 'epoch': 4.6824311119796755}\nEpoch: 4.686339652139925, Logs: {'loss': 0.0429, 'grad_norm': 0.12052281200885773, 'learning_rate': 1.063017982799062e-05, 'epoch': 4.686339652139925}\nEpoch: 4.690248192300176, Logs: {'loss': 0.0438, 'grad_norm': 0.2058892399072647, 'learning_rate': 1.0622361219702894e-05, 'epoch': 4.690248192300176}\nEpoch: 4.694156732460426, Logs: {'loss': 0.0439, 'grad_norm': 0.08552710711956024, 'learning_rate': 1.0614542611415168e-05, 'epoch': 4.694156732460426}\nEpoch: 4.698065272620676, Logs: {'loss': 0.046, 'grad_norm': 0.09648634493350983, 'learning_rate': 1.0606724003127444e-05, 'epoch': 4.698065272620676}\nEpoch: 4.701973812780926, Logs: {'loss': 0.0456, 'grad_norm': 0.23456917703151703, 'learning_rate': 1.059890539483972e-05, 'epoch': 4.701973812780926}\nEpoch: 4.705882352941177, Logs: {'loss': 0.0413, 'grad_norm': 0.37696418166160583, 'learning_rate': 1.0591086786551994e-05, 'epoch': 4.705882352941177}\nEpoch: 4.7097908931014265, Logs: {'loss': 0.0418, 'grad_norm': 0.3249134123325348, 'learning_rate': 1.058326817826427e-05, 'epoch': 4.7097908931014265}\nEpoch: 4.713699433261676, Logs: {'loss': 0.0467, 'grad_norm': 0.2292451113462448, 'learning_rate': 1.0575449569976545e-05, 'epoch': 4.713699433261676}\nEpoch: 4.717607973421927, Logs: {'loss': 0.0434, 'grad_norm': 0.1842021644115448, 'learning_rate': 1.056763096168882e-05, 'epoch': 4.717607973421927}\nEpoch: 4.721516513582177, Logs: {'loss': 0.0436, 'grad_norm': 0.3011424243450165, 'learning_rate': 1.0559812353401095e-05, 'epoch': 4.721516513582177}\nEpoch: 4.725425053742427, Logs: {'loss': 0.0424, 'grad_norm': 0.11896347254514694, 'learning_rate': 1.0551993745113372e-05, 'epoch': 4.725425053742427}\nEpoch: 4.729333593902678, Logs: {'loss': 0.0389, 'grad_norm': 0.14983396232128143, 'learning_rate': 1.0544175136825645e-05, 'epoch': 4.729333593902678}\nEpoch: 4.733242134062928, Logs: {'loss': 0.0409, 'grad_norm': 0.2753620743751526, 'learning_rate': 1.0536356528537921e-05, 'epoch': 4.733242134062928}\nEpoch: 4.7371506742231775, Logs: {'loss': 0.0464, 'grad_norm': 0.24536395072937012, 'learning_rate': 1.0528537920250196e-05, 'epoch': 4.7371506742231775}\nEpoch: 4.741059214383428, Logs: {'loss': 0.0422, 'grad_norm': 0.28214356303215027, 'learning_rate': 1.0520719311962471e-05, 'epoch': 4.741059214383428}\nEpoch: 4.744967754543678, Logs: {'loss': 0.0437, 'grad_norm': 0.16005998849868774, 'learning_rate': 1.0512900703674746e-05, 'epoch': 4.744967754543678}\nEpoch: 4.748876294703928, Logs: {'loss': 0.0398, 'grad_norm': 0.14724451303482056, 'learning_rate': 1.0505082095387022e-05, 'epoch': 4.748876294703928}\nEpoch: 4.752784834864178, Logs: {'loss': 0.041, 'grad_norm': 0.15648357570171356, 'learning_rate': 1.0497263487099296e-05, 'epoch': 4.752784834864178}\nEpoch: 4.756693375024429, Logs: {'loss': 0.0428, 'grad_norm': 0.1945551186800003, 'learning_rate': 1.0489444878811572e-05, 'epoch': 4.756693375024429}\nEpoch: 4.760601915184679, Logs: {'loss': 0.0461, 'grad_norm': 0.10956612229347229, 'learning_rate': 1.0481626270523849e-05, 'epoch': 4.760601915184679}\nEpoch: 4.7645104553449285, Logs: {'loss': 0.0464, 'grad_norm': 0.23216089606285095, 'learning_rate': 1.0473807662236122e-05, 'epoch': 4.7645104553449285}\nEpoch: 4.768418995505179, Logs: {'loss': 0.0408, 'grad_norm': 0.35237982869148254, 'learning_rate': 1.0465989053948398e-05, 'epoch': 4.768418995505179}\nEpoch: 4.772327535665429, Logs: {'loss': 0.0418, 'grad_norm': 0.07969855517148972, 'learning_rate': 1.0458170445660673e-05, 'epoch': 4.772327535665429}\nEpoch: 4.776236075825679, Logs: {'loss': 0.0453, 'grad_norm': 0.17785049974918365, 'learning_rate': 1.045035183737295e-05, 'epoch': 4.776236075825679}\nEpoch: 4.780144615985929, Logs: {'loss': 0.0416, 'grad_norm': 0.25869235396385193, 'learning_rate': 1.0442533229085223e-05, 'epoch': 4.780144615985929}\nEpoch: 4.78405315614618, Logs: {'loss': 0.0396, 'grad_norm': 0.1117440015077591, 'learning_rate': 1.04347146207975e-05, 'epoch': 4.78405315614618}\nEpoch: 4.7879616963064295, Logs: {'loss': 0.0453, 'grad_norm': 0.2005905658006668, 'learning_rate': 1.0426896012509774e-05, 'epoch': 4.7879616963064295}\nEpoch: 4.791870236466679, Logs: {'loss': 0.0425, 'grad_norm': 0.13186584413051605, 'learning_rate': 1.041907740422205e-05, 'epoch': 4.791870236466679}\nEpoch: 4.999804572991987, Logs: {'eval_loss': 0.042739879339933395, 'eval_bleu': 3.086843099274389e-06, 'eval_accuracy': 0.944829945269742, 'eval_runtime': 349.1349, 'eval_samples_per_second': 58.613, 'eval_steps_per_second': 3.663, 'epoch': 4.999804572991987}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5.002931405120187, Logs: {'loss': 0.0469, 'grad_norm': 0.25489482283592224, 'learning_rate': 9.997654417513683e-06, 'epoch': 5.002931405120187}\nEpoch: 5.006839945280438, Logs: {'loss': 0.0433, 'grad_norm': 0.12055768072605133, 'learning_rate': 9.989835809225958e-06, 'epoch': 5.006839945280438}\nEpoch: 5.010748485440688, Logs: {'loss': 0.0472, 'grad_norm': 0.17062582075595856, 'learning_rate': 9.982017200938235e-06, 'epoch': 5.010748485440688}\nEpoch: 5.014657025600938, Logs: {'loss': 0.0502, 'grad_norm': 0.09296009689569473, 'learning_rate': 9.97419859265051e-06, 'epoch': 5.014657025600938}\nEpoch: 5.018565565761188, Logs: {'loss': 0.0356, 'grad_norm': 0.22560979425907135, 'learning_rate': 9.966379984362784e-06, 'epoch': 5.018565565761188}\nEpoch: 5.022474105921439, Logs: {'loss': 0.0418, 'grad_norm': 0.09843330085277557, 'learning_rate': 9.95856137607506e-06, 'epoch': 5.022474105921439}\nEpoch: 5.0263826460816885, Logs: {'loss': 0.041, 'grad_norm': 0.10891970247030258, 'learning_rate': 9.950742767787334e-06, 'epoch': 5.0263826460816885}\nEpoch: 5.030291186241938, Logs: {'loss': 0.0453, 'grad_norm': 0.1706646978855133, 'learning_rate': 9.942924159499609e-06, 'epoch': 5.030291186241938}\nEpoch: 5.034199726402189, Logs: {'loss': 0.041, 'grad_norm': 0.33518221974372864, 'learning_rate': 9.935105551211886e-06, 'epoch': 5.034199726402189}\nEpoch: 5.038108266562439, Logs: {'loss': 0.0429, 'grad_norm': 0.23868204653263092, 'learning_rate': 9.92728694292416e-06, 'epoch': 5.038108266562439}\nEpoch: 5.042016806722689, Logs: {'loss': 0.0441, 'grad_norm': 0.1477622091770172, 'learning_rate': 9.919468334636435e-06, 'epoch': 5.042016806722689}\nEpoch: 5.045925346882939, Logs: {'loss': 0.0391, 'grad_norm': 0.3924592435359955, 'learning_rate': 9.91164972634871e-06, 'epoch': 5.045925346882939}\nEpoch: 5.04983388704319, Logs: {'loss': 0.0427, 'grad_norm': 0.1769547164440155, 'learning_rate': 9.903831118060985e-06, 'epoch': 5.04983388704319}\nEpoch: 5.0537424272034395, Logs: {'loss': 0.0459, 'grad_norm': 0.11991768330335617, 'learning_rate': 9.89601250977326e-06, 'epoch': 5.0537424272034395}\nEpoch: 5.057650967363689, Logs: {'loss': 0.0447, 'grad_norm': 0.15971656143665314, 'learning_rate': 9.888193901485536e-06, 'epoch': 5.057650967363689}\nEpoch: 5.06155950752394, Logs: {'loss': 0.041, 'grad_norm': 0.09068365395069122, 'learning_rate': 9.880375293197811e-06, 'epoch': 5.06155950752394}\nEpoch: 5.06546804768419, Logs: {'loss': 0.0411, 'grad_norm': 0.1102372482419014, 'learning_rate': 9.872556684910088e-06, 'epoch': 5.06546804768419}\nEpoch: 5.06937658784444, Logs: {'loss': 0.0434, 'grad_norm': 0.10279756784439087, 'learning_rate': 9.864738076622363e-06, 'epoch': 5.06937658784444}\nEpoch: 5.073285128004691, Logs: {'loss': 0.0443, 'grad_norm': 0.17042437195777893, 'learning_rate': 9.856919468334638e-06, 'epoch': 5.073285128004691}\nEpoch: 5.0771936681649406, Logs: {'loss': 0.0442, 'grad_norm': 0.3985145390033722, 'learning_rate': 9.849100860046913e-06, 'epoch': 5.0771936681649406}\nEpoch: 5.08110220832519, Logs: {'loss': 0.0434, 'grad_norm': 0.17251689732074738, 'learning_rate': 9.841282251759187e-06, 'epoch': 5.08110220832519}\nEpoch: 5.08501074848544, Logs: {'loss': 0.0418, 'grad_norm': 0.40346354246139526, 'learning_rate': 9.833463643471462e-06, 'epoch': 5.08501074848544}\nEpoch: 5.088919288645691, Logs: {'loss': 0.0411, 'grad_norm': 0.19635534286499023, 'learning_rate': 9.825645035183739e-06, 'epoch': 5.088919288645691}\nEpoch: 5.092827828805941, Logs: {'loss': 0.0457, 'grad_norm': 0.23168902099132538, 'learning_rate': 9.817826426896014e-06, 'epoch': 5.092827828805941}\nEpoch: 5.096736368966191, Logs: {'loss': 0.0367, 'grad_norm': 0.13819387555122375, 'learning_rate': 9.810007818608289e-06, 'epoch': 5.096736368966191}\nEpoch: 5.100644909126442, Logs: {'loss': 0.043, 'grad_norm': 0.22707884013652802, 'learning_rate': 9.802189210320563e-06, 'epoch': 5.100644909126442}\nEpoch: 5.1045534492866915, Logs: {'loss': 0.0389, 'grad_norm': 0.350999116897583, 'learning_rate': 9.794370602032838e-06, 'epoch': 5.1045534492866915}\nEpoch: 5.108461989446941, Logs: {'loss': 0.0444, 'grad_norm': 0.3317936062812805, 'learning_rate': 9.786551993745113e-06, 'epoch': 5.108461989446941}\nEpoch: 5.112370529607191, Logs: {'loss': 0.0391, 'grad_norm': 0.13293269276618958, 'learning_rate': 9.77873338545739e-06, 'epoch': 5.112370529607191}\nEpoch: 5.116279069767442, Logs: {'loss': 0.0356, 'grad_norm': 0.10820991545915604, 'learning_rate': 9.770914777169665e-06, 'epoch': 5.116279069767442}\nEpoch: 5.120187609927692, Logs: {'loss': 0.0398, 'grad_norm': 0.16148333251476288, 'learning_rate': 9.763096168881941e-06, 'epoch': 5.120187609927692}\nEpoch: 5.124096150087942, Logs: {'loss': 0.0415, 'grad_norm': 0.07578645646572113, 'learning_rate': 9.755277560594216e-06, 'epoch': 5.124096150087942}\nEpoch: 5.128004690248193, Logs: {'loss': 0.0439, 'grad_norm': 0.39196741580963135, 'learning_rate': 9.74745895230649e-06, 'epoch': 5.128004690248193}\nEpoch: 5.1319132304084425, Logs: {'loss': 0.0396, 'grad_norm': 0.2738596498966217, 'learning_rate': 9.739640344018766e-06, 'epoch': 5.1319132304084425}\nEpoch: 5.135821770568692, Logs: {'loss': 0.0402, 'grad_norm': 0.6684297323226929, 'learning_rate': 9.73182173573104e-06, 'epoch': 5.135821770568692}\nEpoch: 5.139730310728943, Logs: {'loss': 0.0406, 'grad_norm': 0.19406525790691376, 'learning_rate': 9.724003127443315e-06, 'epoch': 5.139730310728943}\nEpoch: 5.143638850889193, Logs: {'loss': 0.0391, 'grad_norm': 0.3723505735397339, 'learning_rate': 9.716184519155592e-06, 'epoch': 5.143638850889193}\nEpoch: 5.147547391049443, Logs: {'loss': 0.0435, 'grad_norm': 0.24861817061901093, 'learning_rate': 9.708365910867867e-06, 'epoch': 5.147547391049443}\nEpoch: 5.151455931209693, Logs: {'loss': 0.0409, 'grad_norm': 0.25614142417907715, 'learning_rate': 9.700547302580142e-06, 'epoch': 5.151455931209693}\nEpoch: 5.155364471369944, Logs: {'loss': 0.0372, 'grad_norm': 0.2692204713821411, 'learning_rate': 9.692728694292417e-06, 'epoch': 5.155364471369944}\nEpoch: 5.1592730115301935, Logs: {'loss': 0.0514, 'grad_norm': 0.1635834276676178, 'learning_rate': 9.684910086004691e-06, 'epoch': 5.1592730115301935}\nEpoch: 5.163181551690443, Logs: {'loss': 0.0414, 'grad_norm': 0.18619678914546967, 'learning_rate': 9.677091477716966e-06, 'epoch': 5.163181551690443}\nEpoch: 5.167090091850694, Logs: {'loss': 0.0445, 'grad_norm': 0.10637099295854568, 'learning_rate': 9.669272869429243e-06, 'epoch': 5.167090091850694}\nEpoch: 5.170998632010944, Logs: {'loss': 0.0426, 'grad_norm': 0.5423063635826111, 'learning_rate': 9.661454261141518e-06, 'epoch': 5.170998632010944}\nEpoch: 5.174907172171194, Logs: {'loss': 0.0394, 'grad_norm': 0.34114038944244385, 'learning_rate': 9.653635652853793e-06, 'epoch': 5.174907172171194}\nEpoch: 5.178815712331444, Logs: {'loss': 0.0395, 'grad_norm': 0.11225824058055878, 'learning_rate': 9.645817044566067e-06, 'epoch': 5.178815712331444}\nEpoch: 5.1827242524916945, Logs: {'loss': 0.0421, 'grad_norm': 0.18534214794635773, 'learning_rate': 9.637998436278342e-06, 'epoch': 5.1827242524916945}\nEpoch: 5.186632792651944, Logs: {'loss': 0.0405, 'grad_norm': 0.14363577961921692, 'learning_rate': 9.630179827990619e-06, 'epoch': 5.186632792651944}\nEpoch: 5.190541332812194, Logs: {'loss': 0.041, 'grad_norm': 0.2457382082939148, 'learning_rate': 9.622361219702894e-06, 'epoch': 5.190541332812194}\nEpoch: 5.194449872972445, Logs: {'loss': 0.0448, 'grad_norm': 0.09380955249071121, 'learning_rate': 9.614542611415169e-06, 'epoch': 5.194449872972445}\nEpoch: 5.198358413132695, Logs: {'loss': 0.0403, 'grad_norm': 0.10859779268503189, 'learning_rate': 9.606724003127445e-06, 'epoch': 5.198358413132695}\nEpoch: 5.202266953292945, Logs: {'loss': 0.043, 'grad_norm': 0.12144365161657333, 'learning_rate': 9.59890539483972e-06, 'epoch': 5.202266953292945}\nEpoch: 5.206175493453196, Logs: {'loss': 0.0416, 'grad_norm': 0.22622348368167877, 'learning_rate': 9.591086786551995e-06, 'epoch': 5.206175493453196}\nEpoch: 5.2100840336134455, Logs: {'loss': 0.0385, 'grad_norm': 0.13945351541042328, 'learning_rate': 9.58326817826427e-06, 'epoch': 5.2100840336134455}\nEpoch: 5.213992573773695, Logs: {'loss': 0.0369, 'grad_norm': 0.44557780027389526, 'learning_rate': 9.575449569976545e-06, 'epoch': 5.213992573773695}\nEpoch: 5.217901113933945, Logs: {'loss': 0.0406, 'grad_norm': 0.1916055679321289, 'learning_rate': 9.56763096168882e-06, 'epoch': 5.217901113933945}\nEpoch: 5.221809654094196, Logs: {'loss': 0.0421, 'grad_norm': 0.15763375163078308, 'learning_rate': 9.559812353401096e-06, 'epoch': 5.221809654094196}\nEpoch: 5.225718194254446, Logs: {'loss': 0.0472, 'grad_norm': 0.0867166817188263, 'learning_rate': 9.551993745113371e-06, 'epoch': 5.225718194254446}\nEpoch: 5.229626734414696, Logs: {'loss': 0.0414, 'grad_norm': 0.32158905267715454, 'learning_rate': 9.544175136825646e-06, 'epoch': 5.229626734414696}\nEpoch: 5.233535274574947, Logs: {'loss': 0.0446, 'grad_norm': 0.09093222767114639, 'learning_rate': 9.53635652853792e-06, 'epoch': 5.233535274574947}\nEpoch: 5.2374438147351965, Logs: {'loss': 0.0465, 'grad_norm': 0.2868044674396515, 'learning_rate': 9.528537920250196e-06, 'epoch': 5.2374438147351965}\nEpoch: 5.241352354895446, Logs: {'loss': 0.043, 'grad_norm': 0.13354964554309845, 'learning_rate': 9.520719311962472e-06, 'epoch': 5.241352354895446}\nEpoch: 5.245260895055697, Logs: {'loss': 0.0421, 'grad_norm': 0.10836343467235565, 'learning_rate': 9.512900703674747e-06, 'epoch': 5.245260895055697}\nEpoch: 5.249169435215947, Logs: {'loss': 0.0486, 'grad_norm': 0.1469082087278366, 'learning_rate': 9.505082095387022e-06, 'epoch': 5.249169435215947}\nEpoch: 5.253077975376197, Logs: {'loss': 0.0412, 'grad_norm': 0.13007599115371704, 'learning_rate': 9.497263487099297e-06, 'epoch': 5.253077975376197}\nEpoch: 5.256986515536447, Logs: {'loss': 0.0403, 'grad_norm': 0.15940949320793152, 'learning_rate': 9.489444878811572e-06, 'epoch': 5.256986515536447}\nEpoch: 5.260895055696698, Logs: {'loss': 0.0393, 'grad_norm': 0.263552188873291, 'learning_rate': 9.481626270523846e-06, 'epoch': 5.260895055696698}\nEpoch: 5.2648035958569475, Logs: {'loss': 0.0456, 'grad_norm': 0.14759531617164612, 'learning_rate': 9.473807662236123e-06, 'epoch': 5.2648035958569475}\nEpoch: 5.268712136017197, Logs: {'loss': 0.0398, 'grad_norm': 0.1932821273803711, 'learning_rate': 9.465989053948398e-06, 'epoch': 5.268712136017197}\nEpoch: 5.272620676177448, Logs: {'loss': 0.0402, 'grad_norm': 0.1225939691066742, 'learning_rate': 9.458170445660673e-06, 'epoch': 5.272620676177448}\nEpoch: 5.276529216337698, Logs: {'loss': 0.0426, 'grad_norm': 0.11184778809547424, 'learning_rate': 9.45035183737295e-06, 'epoch': 5.276529216337698}\nEpoch: 5.280437756497948, Logs: {'loss': 0.043, 'grad_norm': 0.13778990507125854, 'learning_rate': 9.442533229085224e-06, 'epoch': 5.280437756497948}\nEpoch: 5.284346296658198, Logs: {'loss': 0.0403, 'grad_norm': 0.26471060514450073, 'learning_rate': 9.434714620797499e-06, 'epoch': 5.284346296658198}\nEpoch: 5.2882548368184485, Logs: {'loss': 0.0471, 'grad_norm': 0.29503780603408813, 'learning_rate': 9.426896012509774e-06, 'epoch': 5.2882548368184485}\nEpoch: 5.292163376978698, Logs: {'loss': 0.0407, 'grad_norm': 0.6125341653823853, 'learning_rate': 9.419077404222049e-06, 'epoch': 5.292163376978698}\nEpoch: 5.296071917138948, Logs: {'loss': 0.0459, 'grad_norm': 0.21410390734672546, 'learning_rate': 9.411258795934325e-06, 'epoch': 5.296071917138948}\nEpoch: 5.299980457299199, Logs: {'loss': 0.0402, 'grad_norm': 0.28722915053367615, 'learning_rate': 9.4034401876466e-06, 'epoch': 5.299980457299199}\nEpoch: 5.303888997459449, Logs: {'loss': 0.0439, 'grad_norm': 0.10812270641326904, 'learning_rate': 9.395621579358875e-06, 'epoch': 5.303888997459449}\nEpoch: 5.307797537619699, Logs: {'loss': 0.0384, 'grad_norm': 0.49057984352111816, 'learning_rate': 9.38780297107115e-06, 'epoch': 5.307797537619699}\nEpoch: 5.311706077779949, Logs: {'loss': 0.039, 'grad_norm': 0.19648922979831696, 'learning_rate': 9.379984362783425e-06, 'epoch': 5.311706077779949}\nEpoch: 5.3156146179401995, Logs: {'loss': 0.0388, 'grad_norm': 0.3101264238357544, 'learning_rate': 9.3721657544957e-06, 'epoch': 5.3156146179401995}\nEpoch: 5.319523158100449, Logs: {'loss': 0.0464, 'grad_norm': 0.1990983784198761, 'learning_rate': 9.364347146207976e-06, 'epoch': 5.319523158100449}\nEpoch: 5.323431698260699, Logs: {'loss': 0.042, 'grad_norm': 0.0733417347073555, 'learning_rate': 9.356528537920251e-06, 'epoch': 5.323431698260699}\nEpoch: 5.32734023842095, Logs: {'loss': 0.0356, 'grad_norm': 0.30085331201553345, 'learning_rate': 9.348709929632526e-06, 'epoch': 5.32734023842095}\nEpoch: 5.3312487785812, Logs: {'loss': 0.042, 'grad_norm': 0.09880749136209488, 'learning_rate': 9.3408913213448e-06, 'epoch': 5.3312487785812}\nEpoch: 5.33515731874145, Logs: {'loss': 0.0416, 'grad_norm': 0.28571656346321106, 'learning_rate': 9.333072713057076e-06, 'epoch': 5.33515731874145}\nEpoch: 5.339065858901701, Logs: {'loss': 0.0411, 'grad_norm': 0.2105080932378769, 'learning_rate': 9.325254104769352e-06, 'epoch': 5.339065858901701}\nEpoch: 5.3429743990619505, Logs: {'loss': 0.0381, 'grad_norm': 0.25788185000419617, 'learning_rate': 9.317435496481627e-06, 'epoch': 5.3429743990619505}\nEpoch: 5.3468829392222, Logs: {'loss': 0.0442, 'grad_norm': 0.3383041322231293, 'learning_rate': 9.309616888193902e-06, 'epoch': 5.3468829392222}\nEpoch: 5.35079147938245, Logs: {'loss': 0.0459, 'grad_norm': 0.24745821952819824, 'learning_rate': 9.301798279906179e-06, 'epoch': 5.35079147938245}\nEpoch: 5.354700019542701, Logs: {'loss': 0.0378, 'grad_norm': 0.15273380279541016, 'learning_rate': 9.293979671618453e-06, 'epoch': 5.354700019542701}\nEpoch: 5.358608559702951, Logs: {'loss': 0.0422, 'grad_norm': 0.5101533532142639, 'learning_rate': 9.286161063330728e-06, 'epoch': 5.358608559702951}\nEpoch: 5.362517099863201, Logs: {'loss': 0.0492, 'grad_norm': 0.08844802528619766, 'learning_rate': 9.278342455043003e-06, 'epoch': 5.362517099863201}\nEpoch: 5.366425640023452, Logs: {'loss': 0.0471, 'grad_norm': 0.21950915455818176, 'learning_rate': 9.270523846755278e-06, 'epoch': 5.366425640023452}\nEpoch: 5.3703341801837015, Logs: {'loss': 0.0435, 'grad_norm': 0.2885951101779938, 'learning_rate': 9.262705238467553e-06, 'epoch': 5.3703341801837015}\nEpoch: 5.374242720343951, Logs: {'loss': 0.0409, 'grad_norm': 0.12303613126277924, 'learning_rate': 9.25488663017983e-06, 'epoch': 5.374242720343951}\nEpoch: 5.378151260504202, Logs: {'loss': 0.0451, 'grad_norm': 0.15676718950271606, 'learning_rate': 9.247068021892104e-06, 'epoch': 5.378151260504202}\nEpoch: 5.382059800664452, Logs: {'loss': 0.0416, 'grad_norm': 0.1523411124944687, 'learning_rate': 9.23924941360438e-06, 'epoch': 5.382059800664452}\nEpoch: 5.385968340824702, Logs: {'loss': 0.0447, 'grad_norm': 0.24872620403766632, 'learning_rate': 9.231430805316654e-06, 'epoch': 5.385968340824702}\nEpoch: 5.389876880984952, Logs: {'loss': 0.0433, 'grad_norm': 0.2951103746891022, 'learning_rate': 9.223612197028929e-06, 'epoch': 5.389876880984952}\nEpoch: 5.3937854211452025, Logs: {'loss': 0.0486, 'grad_norm': 0.417959600687027, 'learning_rate': 9.215793588741205e-06, 'epoch': 5.3937854211452025}\nEpoch: 5.397693961305452, Logs: {'loss': 0.0391, 'grad_norm': 0.1159963309764862, 'learning_rate': 9.20797498045348e-06, 'epoch': 5.397693961305452}\nEpoch: 5.401602501465702, Logs: {'loss': 0.041, 'grad_norm': 0.1301499307155609, 'learning_rate': 9.200156372165755e-06, 'epoch': 5.401602501465702}\nEpoch: 5.405511041625953, Logs: {'loss': 0.0451, 'grad_norm': 0.13175809383392334, 'learning_rate': 9.19233776387803e-06, 'epoch': 5.405511041625953}\nEpoch: 5.409419581786203, Logs: {'loss': 0.0412, 'grad_norm': 0.10814984142780304, 'learning_rate': 9.184519155590305e-06, 'epoch': 5.409419581786203}\nEpoch: 5.413328121946453, Logs: {'loss': 0.0397, 'grad_norm': 0.11344993859529495, 'learning_rate': 9.17670054730258e-06, 'epoch': 5.413328121946453}\nEpoch: 5.417236662106703, Logs: {'loss': 0.0428, 'grad_norm': 0.44663506746292114, 'learning_rate': 9.168881939014856e-06, 'epoch': 5.417236662106703}\nEpoch: 5.4211452022669535, Logs: {'loss': 0.0394, 'grad_norm': 0.1479923129081726, 'learning_rate': 9.161063330727131e-06, 'epoch': 5.4211452022669535}\nEpoch: 5.425053742427203, Logs: {'loss': 0.0483, 'grad_norm': 0.10684023052453995, 'learning_rate': 9.153244722439406e-06, 'epoch': 5.425053742427203}\nEpoch: 5.428962282587453, Logs: {'loss': 0.0403, 'grad_norm': 0.11419571191072464, 'learning_rate': 9.145426114151683e-06, 'epoch': 5.428962282587453}\nEpoch: 5.432870822747704, Logs: {'loss': 0.0407, 'grad_norm': 0.3213490843772888, 'learning_rate': 9.137607505863958e-06, 'epoch': 5.432870822747704}\nEpoch: 5.436779362907954, Logs: {'loss': 0.0396, 'grad_norm': 0.19731405377388, 'learning_rate': 9.129788897576232e-06, 'epoch': 5.436779362907954}\nEpoch: 5.440687903068204, Logs: {'loss': 0.0418, 'grad_norm': 0.18700453639030457, 'learning_rate': 9.121970289288507e-06, 'epoch': 5.440687903068204}\nEpoch: 5.444596443228454, Logs: {'loss': 0.0407, 'grad_norm': 0.0934225395321846, 'learning_rate': 9.114151681000782e-06, 'epoch': 5.444596443228454}\nEpoch: 5.4485049833887045, Logs: {'loss': 0.0405, 'grad_norm': 0.19586196541786194, 'learning_rate': 9.106333072713059e-06, 'epoch': 5.4485049833887045}\nEpoch: 5.452413523548954, Logs: {'loss': 0.0397, 'grad_norm': 0.07320908457040787, 'learning_rate': 9.098514464425334e-06, 'epoch': 5.452413523548954}\nEpoch: 5.456322063709204, Logs: {'loss': 0.0421, 'grad_norm': 0.16765503585338593, 'learning_rate': 9.090695856137608e-06, 'epoch': 5.456322063709204}\nEpoch: 5.460230603869455, Logs: {'loss': 0.0366, 'grad_norm': 0.15403740108013153, 'learning_rate': 9.082877247849883e-06, 'epoch': 5.460230603869455}\nEpoch: 5.464139144029705, Logs: {'loss': 0.0426, 'grad_norm': 0.08577824383974075, 'learning_rate': 9.075058639562158e-06, 'epoch': 5.464139144029705}\nEpoch: 5.468047684189955, Logs: {'loss': 0.0432, 'grad_norm': 0.1404232382774353, 'learning_rate': 9.067240031274433e-06, 'epoch': 5.468047684189955}\nEpoch: 5.471956224350206, Logs: {'loss': 0.0475, 'grad_norm': 0.13193342089653015, 'learning_rate': 9.05942142298671e-06, 'epoch': 5.471956224350206}\nEpoch: 5.4758647645104555, Logs: {'loss': 0.0486, 'grad_norm': 0.48256635665893555, 'learning_rate': 9.051602814698984e-06, 'epoch': 5.4758647645104555}\nEpoch: 5.479773304670705, Logs: {'loss': 0.0424, 'grad_norm': 0.10608552396297455, 'learning_rate': 9.04378420641126e-06, 'epoch': 5.479773304670705}\nEpoch: 5.483681844830956, Logs: {'loss': 0.039, 'grad_norm': 0.09266237169504166, 'learning_rate': 9.035965598123536e-06, 'epoch': 5.483681844830956}\nEpoch: 5.487590384991206, Logs: {'loss': 0.0476, 'grad_norm': 0.39319145679473877, 'learning_rate': 9.028146989835809e-06, 'epoch': 5.487590384991206}\nEpoch: 5.491498925151456, Logs: {'loss': 0.0439, 'grad_norm': 0.20492032170295715, 'learning_rate': 9.020328381548084e-06, 'epoch': 5.491498925151456}\nEpoch: 5.495407465311706, Logs: {'loss': 0.0453, 'grad_norm': 0.10341489315032959, 'learning_rate': 9.01250977326036e-06, 'epoch': 5.495407465311706}\nEpoch: 5.4993160054719565, Logs: {'loss': 0.0395, 'grad_norm': 0.10743667930364609, 'learning_rate': 9.004691164972635e-06, 'epoch': 5.4993160054719565}\nEpoch: 5.503224545632206, Logs: {'loss': 0.0454, 'grad_norm': 0.2916642427444458, 'learning_rate': 8.996872556684912e-06, 'epoch': 5.503224545632206}\nEpoch: 5.507133085792456, Logs: {'loss': 0.0409, 'grad_norm': 0.2011236846446991, 'learning_rate': 8.989053948397187e-06, 'epoch': 5.507133085792456}\nEpoch: 5.511041625952707, Logs: {'loss': 0.0399, 'grad_norm': 0.22824914753437042, 'learning_rate': 8.981235340109462e-06, 'epoch': 5.511041625952707}\nEpoch: 5.514950166112957, Logs: {'loss': 0.0441, 'grad_norm': 0.19957736134529114, 'learning_rate': 8.973416731821736e-06, 'epoch': 5.514950166112957}\nEpoch: 5.518858706273207, Logs: {'loss': 0.0418, 'grad_norm': 0.18564149737358093, 'learning_rate': 8.965598123534011e-06, 'epoch': 5.518858706273207}\nEpoch: 5.522767246433457, Logs: {'loss': 0.0418, 'grad_norm': 0.14158374071121216, 'learning_rate': 8.957779515246286e-06, 'epoch': 5.522767246433457}\nEpoch: 5.5266757865937075, Logs: {'loss': 0.0413, 'grad_norm': 0.13770334422588348, 'learning_rate': 8.949960906958563e-06, 'epoch': 5.5266757865937075}\nEpoch: 5.530584326753957, Logs: {'loss': 0.0385, 'grad_norm': 0.2330632358789444, 'learning_rate': 8.942142298670838e-06, 'epoch': 5.530584326753957}\nEpoch: 5.534492866914207, Logs: {'loss': 0.0475, 'grad_norm': 0.20472069084644318, 'learning_rate': 8.934323690383112e-06, 'epoch': 5.534492866914207}\nEpoch: 5.538401407074458, Logs: {'loss': 0.0423, 'grad_norm': 0.08462351560592651, 'learning_rate': 8.926505082095387e-06, 'epoch': 5.538401407074458}\nEpoch: 5.542309947234708, Logs: {'loss': 0.0427, 'grad_norm': 0.27900075912475586, 'learning_rate': 8.918686473807662e-06, 'epoch': 5.542309947234708}\nEpoch: 5.546218487394958, Logs: {'loss': 0.0474, 'grad_norm': 0.16192826628684998, 'learning_rate': 8.910867865519937e-06, 'epoch': 5.546218487394958}\nEpoch: 5.550127027555208, Logs: {'loss': 0.0374, 'grad_norm': 0.19687427580356598, 'learning_rate': 8.903049257232214e-06, 'epoch': 5.550127027555208}\nEpoch: 5.5540355677154585, Logs: {'loss': 0.0484, 'grad_norm': 0.10699591785669327, 'learning_rate': 8.895230648944489e-06, 'epoch': 5.5540355677154585}\nEpoch: 5.557944107875708, Logs: {'loss': 0.0429, 'grad_norm': 0.06835909187793732, 'learning_rate': 8.887412040656765e-06, 'epoch': 5.557944107875708}\nEpoch: 5.561852648035958, Logs: {'loss': 0.0455, 'grad_norm': 0.1752835214138031, 'learning_rate': 8.87959343236904e-06, 'epoch': 5.561852648035958}\nEpoch: 5.565761188196209, Logs: {'loss': 0.0385, 'grad_norm': 0.13405098021030426, 'learning_rate': 8.871774824081315e-06, 'epoch': 5.565761188196209}\nEpoch: 5.569669728356459, Logs: {'loss': 0.04, 'grad_norm': 0.12880095839500427, 'learning_rate': 8.86395621579359e-06, 'epoch': 5.569669728356459}\nEpoch: 5.573578268516709, Logs: {'loss': 0.0394, 'grad_norm': 0.17744289338588715, 'learning_rate': 8.856137607505865e-06, 'epoch': 5.573578268516709}\nEpoch: 5.577486808676959, Logs: {'loss': 0.0428, 'grad_norm': 0.18830356001853943, 'learning_rate': 8.84831899921814e-06, 'epoch': 5.577486808676959}\nEpoch: 5.5813953488372094, Logs: {'loss': 0.0459, 'grad_norm': 0.10109784454107285, 'learning_rate': 8.840500390930416e-06, 'epoch': 5.5813953488372094}\nEpoch: 5.585303888997459, Logs: {'loss': 0.0404, 'grad_norm': 0.26300254464149475, 'learning_rate': 8.83268178264269e-06, 'epoch': 5.585303888997459}\nEpoch: 5.589212429157709, Logs: {'loss': 0.0406, 'grad_norm': 0.07272230088710785, 'learning_rate': 8.824863174354966e-06, 'epoch': 5.589212429157709}\nEpoch: 5.59312096931796, Logs: {'loss': 0.0402, 'grad_norm': 0.2271607369184494, 'learning_rate': 8.81704456606724e-06, 'epoch': 5.59312096931796}\nEpoch: 5.59702950947821, Logs: {'loss': 0.0432, 'grad_norm': 0.23318378627300262, 'learning_rate': 8.809225957779515e-06, 'epoch': 5.59702950947821}\nEpoch: 5.60093804963846, Logs: {'loss': 0.0417, 'grad_norm': 0.12409523129463196, 'learning_rate': 8.80140734949179e-06, 'epoch': 5.60093804963846}\nEpoch: 5.6048465897987105, Logs: {'loss': 0.0456, 'grad_norm': 0.2564232051372528, 'learning_rate': 8.793588741204067e-06, 'epoch': 5.6048465897987105}\nEpoch: 5.60875512995896, Logs: {'loss': 0.0463, 'grad_norm': 0.3829514682292938, 'learning_rate': 8.785770132916342e-06, 'epoch': 5.60875512995896}\nEpoch: 5.61266367011921, Logs: {'loss': 0.0414, 'grad_norm': 0.09486702084541321, 'learning_rate': 8.777951524628617e-06, 'epoch': 5.61266367011921}\nEpoch: 5.616572210279461, Logs: {'loss': 0.041, 'grad_norm': 0.6145005822181702, 'learning_rate': 8.770132916340891e-06, 'epoch': 5.616572210279461}\nEpoch: 5.620480750439711, Logs: {'loss': 0.0538, 'grad_norm': 0.19045564532279968, 'learning_rate': 8.762314308053166e-06, 'epoch': 5.620480750439711}\nEpoch: 5.624389290599961, Logs: {'loss': 0.0432, 'grad_norm': 0.11516664177179337, 'learning_rate': 8.754495699765443e-06, 'epoch': 5.624389290599961}\nEpoch: 5.628297830760211, Logs: {'loss': 0.0468, 'grad_norm': 0.6292843818664551, 'learning_rate': 8.746677091477718e-06, 'epoch': 5.628297830760211}\n","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"Epoch: 6.999804572991987, Logs: {'eval_loss': 0.04102117195725441, 'eval_bleu': 0.0, 'eval_accuracy': 0.9455629397967162, 'eval_runtime': 349.5566, 'eval_samples_per_second': 58.543, 'eval_steps_per_second': 3.659, 'epoch': 6.999804572991987}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7.000195427008013, Logs: {'loss': 0.0401, 'grad_norm': 0.3389039635658264, 'learning_rate': 6.00312744331509e-06, 'epoch': 7.000195427008013}\nEpoch: 7.004103967168263, Logs: {'loss': 0.0409, 'grad_norm': 0.1340303272008896, 'learning_rate': 5.995308835027366e-06, 'epoch': 7.004103967168263}\nEpoch: 7.008012507328512, Logs: {'loss': 0.0405, 'grad_norm': 0.0989096388220787, 'learning_rate': 5.987490226739641e-06, 'epoch': 7.008012507328512}\nEpoch: 7.011921047488763, Logs: {'loss': 0.0407, 'grad_norm': 0.19655492901802063, 'learning_rate': 5.9796716184519165e-06, 'epoch': 7.011921047488763}\nEpoch: 7.015829587649013, Logs: {'loss': 0.0439, 'grad_norm': 0.109644815325737, 'learning_rate': 5.971853010164191e-06, 'epoch': 7.015829587649013}\nEpoch: 7.019738127809263, Logs: {'loss': 0.0415, 'grad_norm': 0.151803657412529, 'learning_rate': 5.964034401876466e-06, 'epoch': 7.019738127809263}\nEpoch: 7.023646667969514, Logs: {'loss': 0.0427, 'grad_norm': 0.16753795742988586, 'learning_rate': 5.956215793588742e-06, 'epoch': 7.023646667969514}\nEpoch: 7.027555208129764, Logs: {'loss': 0.0399, 'grad_norm': 0.13117606937885284, 'learning_rate': 5.948397185301017e-06, 'epoch': 7.027555208129764}\nEpoch: 7.0314637482900135, Logs: {'loss': 0.0371, 'grad_norm': 0.6553027629852295, 'learning_rate': 5.940578577013292e-06, 'epoch': 7.0314637482900135}\nEpoch: 7.035372288450263, Logs: {'loss': 0.0427, 'grad_norm': 0.5297874808311462, 'learning_rate': 5.932759968725567e-06, 'epoch': 7.035372288450263}\nEpoch: 7.039280828610514, Logs: {'loss': 0.0375, 'grad_norm': 0.08141250163316727, 'learning_rate': 5.924941360437842e-06, 'epoch': 7.039280828610514}\nEpoch: 7.043189368770764, Logs: {'loss': 0.0421, 'grad_norm': 0.414501428604126, 'learning_rate': 5.917122752150117e-06, 'epoch': 7.043189368770764}\nEpoch: 7.047097908931014, Logs: {'loss': 0.04, 'grad_norm': 0.20409083366394043, 'learning_rate': 5.909304143862394e-06, 'epoch': 7.047097908931014}\nEpoch: 7.051006449091265, Logs: {'loss': 0.0411, 'grad_norm': 0.1529131531715393, 'learning_rate': 5.901485535574668e-06, 'epoch': 7.051006449091265}\nEpoch: 7.054914989251515, Logs: {'loss': 0.0407, 'grad_norm': 0.18768835067749023, 'learning_rate': 5.8936669272869426e-06, 'epoch': 7.054914989251515}\nEpoch: 7.0588235294117645, Logs: {'loss': 0.0404, 'grad_norm': 0.08900073170661926, 'learning_rate': 5.885848318999219e-06, 'epoch': 7.0588235294117645}\nEpoch: 7.062732069572015, Logs: {'loss': 0.0389, 'grad_norm': 0.13938283920288086, 'learning_rate': 5.878029710711494e-06, 'epoch': 7.062732069572015}\nEpoch: 7.066640609732265, Logs: {'loss': 0.0412, 'grad_norm': 0.3174383342266083, 'learning_rate': 5.87021110242377e-06, 'epoch': 7.066640609732265}\nEpoch: 7.070549149892515, Logs: {'loss': 0.0412, 'grad_norm': 0.2806190252304077, 'learning_rate': 5.8623924941360446e-06, 'epoch': 7.070549149892515}\nEpoch: 7.074457690052765, Logs: {'loss': 0.0468, 'grad_norm': 0.293235719203949, 'learning_rate': 5.8545738858483194e-06, 'epoch': 7.074457690052765}\nEpoch: 7.078366230213016, Logs: {'loss': 0.0411, 'grad_norm': 0.23824401199817657, 'learning_rate': 5.846755277560595e-06, 'epoch': 7.078366230213016}\nEpoch: 7.082274770373266, Logs: {'loss': 0.0434, 'grad_norm': 0.11502456665039062, 'learning_rate': 5.83893666927287e-06, 'epoch': 7.082274770373266}\nEpoch: 7.0861833105335155, Logs: {'loss': 0.0409, 'grad_norm': 0.5392401218414307, 'learning_rate': 5.831118060985145e-06, 'epoch': 7.0861833105335155}\nEpoch: 7.090091850693766, Logs: {'loss': 0.0414, 'grad_norm': 0.13539013266563416, 'learning_rate': 5.823299452697421e-06, 'epoch': 7.090091850693766}\nEpoch: 7.094000390854016, Logs: {'loss': 0.0409, 'grad_norm': 0.18784606456756592, 'learning_rate': 5.8154808444096955e-06, 'epoch': 7.094000390854016}\nEpoch: 7.097908931014266, Logs: {'loss': 0.0452, 'grad_norm': 0.13388444483280182, 'learning_rate': 5.80766223612197e-06, 'epoch': 7.097908931014266}\nEpoch: 7.101817471174516, Logs: {'loss': 0.0423, 'grad_norm': 0.22163590788841248, 'learning_rate': 5.799843627834246e-06, 'epoch': 7.101817471174516}\nEpoch: 7.105726011334767, Logs: {'loss': 0.0406, 'grad_norm': 0.11096245050430298, 'learning_rate': 5.792025019546521e-06, 'epoch': 7.105726011334767}\nEpoch: 7.1096345514950166, Logs: {'loss': 0.0374, 'grad_norm': 0.11990136653184891, 'learning_rate': 5.784206411258796e-06, 'epoch': 7.1096345514950166}\nEpoch: 7.113543091655266, Logs: {'loss': 0.0431, 'grad_norm': 0.21459142863750458, 'learning_rate': 5.7763878029710715e-06, 'epoch': 7.113543091655266}\nEpoch: 7.117451631815517, Logs: {'loss': 0.0405, 'grad_norm': 0.31669050455093384, 'learning_rate': 5.768569194683346e-06, 'epoch': 7.117451631815517}\nEpoch: 7.121360171975767, Logs: {'loss': 0.0417, 'grad_norm': 0.4671350121498108, 'learning_rate': 5.760750586395623e-06, 'epoch': 7.121360171975767}\nEpoch: 7.125268712136017, Logs: {'loss': 0.0478, 'grad_norm': 0.1792692244052887, 'learning_rate': 5.752931978107898e-06, 'epoch': 7.125268712136017}\nEpoch: 7.129177252296268, Logs: {'loss': 0.0386, 'grad_norm': 0.08077332377433777, 'learning_rate': 5.745113369820172e-06, 'epoch': 7.129177252296268}\nEpoch: 7.133085792456518, Logs: {'loss': 0.0422, 'grad_norm': 0.21466858685016632, 'learning_rate': 5.737294761532448e-06, 'epoch': 7.133085792456518}\nEpoch: 7.1369943326167675, Logs: {'loss': 0.0383, 'grad_norm': 0.54944908618927, 'learning_rate': 5.729476153244723e-06, 'epoch': 7.1369943326167675}\nEpoch: 7.140902872777017, Logs: {'loss': 0.0424, 'grad_norm': 0.3131059408187866, 'learning_rate': 5.721657544956998e-06, 'epoch': 7.140902872777017}\nEpoch: 7.144811412937268, Logs: {'loss': 0.0402, 'grad_norm': 0.2028452455997467, 'learning_rate': 5.713838936669274e-06, 'epoch': 7.144811412937268}\nEpoch: 7.148719953097518, Logs: {'loss': 0.0452, 'grad_norm': 0.09771781414747238, 'learning_rate': 5.706020328381549e-06, 'epoch': 7.148719953097518}\nEpoch: 7.152628493257768, Logs: {'loss': 0.0393, 'grad_norm': 0.22221162915229797, 'learning_rate': 5.6982017200938235e-06, 'epoch': 7.152628493257768}\nEpoch: 7.156537033418019, Logs: {'loss': 0.0399, 'grad_norm': 0.09971889108419418, 'learning_rate': 5.690383111806099e-06, 'epoch': 7.156537033418019}\nEpoch: 7.160445573578269, Logs: {'loss': 0.0365, 'grad_norm': 0.15807612240314484, 'learning_rate': 5.682564503518374e-06, 'epoch': 7.160445573578269}\nEpoch: 7.1643541137385185, Logs: {'loss': 0.041, 'grad_norm': 0.21583357453346252, 'learning_rate': 5.674745895230649e-06, 'epoch': 7.1643541137385185}\nEpoch: 7.168262653898768, Logs: {'loss': 0.0415, 'grad_norm': 0.12545955181121826, 'learning_rate': 5.666927286942925e-06, 'epoch': 7.168262653898768}\nEpoch: 7.172171194059019, Logs: {'loss': 0.038, 'grad_norm': 0.3488251268863678, 'learning_rate': 5.6591086786551996e-06, 'epoch': 7.172171194059019}\nEpoch: 7.176079734219269, Logs: {'loss': 0.0393, 'grad_norm': 0.0931834876537323, 'learning_rate': 5.651290070367475e-06, 'epoch': 7.176079734219269}\nEpoch: 7.179988274379519, Logs: {'loss': 0.0396, 'grad_norm': 0.15070267021656036, 'learning_rate': 5.64347146207975e-06, 'epoch': 7.179988274379519}\nEpoch: 7.18389681453977, Logs: {'loss': 0.0422, 'grad_norm': 0.122825987637043, 'learning_rate': 5.635652853792025e-06, 'epoch': 7.18389681453977}\nEpoch: 7.18780535470002, Logs: {'loss': 0.0467, 'grad_norm': 0.3398248255252838, 'learning_rate': 5.627834245504301e-06, 'epoch': 7.18780535470002}\nEpoch: 7.1917138948602695, Logs: {'loss': 0.0414, 'grad_norm': 0.1894528865814209, 'learning_rate': 5.620015637216576e-06, 'epoch': 7.1917138948602695}\nEpoch: 7.19562243502052, Logs: {'loss': 0.0437, 'grad_norm': 0.1419372856616974, 'learning_rate': 5.6121970289288504e-06, 'epoch': 7.19562243502052}\nEpoch: 7.19953097518077, Logs: {'loss': 0.0403, 'grad_norm': 0.11116349697113037, 'learning_rate': 5.604378420641127e-06, 'epoch': 7.19953097518077}\nEpoch: 7.20343951534102, Logs: {'loss': 0.0421, 'grad_norm': 0.10418225079774857, 'learning_rate': 5.596559812353402e-06, 'epoch': 7.20343951534102}\nEpoch: 7.20734805550127, Logs: {'loss': 0.0369, 'grad_norm': 0.1767929494380951, 'learning_rate': 5.588741204065677e-06, 'epoch': 7.20734805550127}\nEpoch: 7.211256595661521, Logs: {'loss': 0.0398, 'grad_norm': 0.10265343636274338, 'learning_rate': 5.5809225957779525e-06, 'epoch': 7.211256595661521}\nEpoch: 7.2151651358217705, Logs: {'loss': 0.0413, 'grad_norm': 0.07002420723438263, 'learning_rate': 5.573103987490227e-06, 'epoch': 7.2151651358217705}\nEpoch: 7.21907367598202, Logs: {'loss': 0.0412, 'grad_norm': 0.17231035232543945, 'learning_rate': 5.565285379202502e-06, 'epoch': 7.21907367598202}\nEpoch: 7.222982216142271, Logs: {'loss': 0.0385, 'grad_norm': 0.23855873942375183, 'learning_rate': 5.557466770914778e-06, 'epoch': 7.222982216142271}\nEpoch: 7.226890756302521, Logs: {'loss': 0.0454, 'grad_norm': 0.32652825117111206, 'learning_rate': 5.549648162627053e-06, 'epoch': 7.226890756302521}\nEpoch: 7.230799296462771, Logs: {'loss': 0.0443, 'grad_norm': 0.3159116804599762, 'learning_rate': 5.5418295543393285e-06, 'epoch': 7.230799296462771}\nEpoch: 7.234707836623022, Logs: {'loss': 0.0431, 'grad_norm': 0.33642905950546265, 'learning_rate': 5.534010946051603e-06, 'epoch': 7.234707836623022}\nEpoch: 7.238616376783272, Logs: {'loss': 0.0405, 'grad_norm': 0.34627383947372437, 'learning_rate': 5.526192337763878e-06, 'epoch': 7.238616376783272}\nEpoch: 7.2425249169435215, Logs: {'loss': 0.0414, 'grad_norm': 0.12558819353580475, 'learning_rate': 5.518373729476154e-06, 'epoch': 7.2425249169435215}\nEpoch: 7.246433457103771, Logs: {'loss': 0.0421, 'grad_norm': 0.32432717084884644, 'learning_rate': 5.510555121188429e-06, 'epoch': 7.246433457103771}\nEpoch: 7.250341997264022, Logs: {'loss': 0.0446, 'grad_norm': 0.2515169382095337, 'learning_rate': 5.502736512900704e-06, 'epoch': 7.250341997264022}\nEpoch: 7.254250537424272, Logs: {'loss': 0.0447, 'grad_norm': 0.162044957280159, 'learning_rate': 5.494917904612979e-06, 'epoch': 7.254250537424272}\nEpoch: 7.258159077584522, Logs: {'loss': 0.0411, 'grad_norm': 0.11634577065706253, 'learning_rate': 5.487099296325254e-06, 'epoch': 7.258159077584522}\nEpoch: 7.262067617744773, Logs: {'loss': 0.0401, 'grad_norm': 0.1458873599767685, 'learning_rate': 5.479280688037529e-06, 'epoch': 7.262067617744773}\nEpoch: 7.265976157905023, Logs: {'loss': 0.0382, 'grad_norm': 0.10220272094011307, 'learning_rate': 5.471462079749806e-06, 'epoch': 7.265976157905023}\nEpoch: 7.2698846980652725, Logs: {'loss': 0.0438, 'grad_norm': 0.34931933879852295, 'learning_rate': 5.46364347146208e-06, 'epoch': 7.2698846980652725}\nEpoch: 7.273793238225522, Logs: {'loss': 0.0379, 'grad_norm': 0.2896750569343567, 'learning_rate': 5.4558248631743545e-06, 'epoch': 7.273793238225522}\nEpoch: 7.277701778385773, Logs: {'loss': 0.0404, 'grad_norm': 0.34840187430381775, 'learning_rate': 5.448006254886631e-06, 'epoch': 7.277701778385773}\nEpoch: 7.281610318546023, Logs: {'loss': 0.0474, 'grad_norm': 0.1075897142291069, 'learning_rate': 5.440187646598906e-06, 'epoch': 7.281610318546023}\nEpoch: 7.285518858706273, Logs: {'loss': 0.0432, 'grad_norm': 0.11228356510400772, 'learning_rate': 5.432369038311182e-06, 'epoch': 7.285518858706273}\nEpoch: 7.289427398866524, Logs: {'loss': 0.0398, 'grad_norm': 0.10575786978006363, 'learning_rate': 5.4245504300234565e-06, 'epoch': 7.289427398866524}\nEpoch: 7.293335939026774, Logs: {'loss': 0.0383, 'grad_norm': 0.0984141156077385, 'learning_rate': 5.416731821735731e-06, 'epoch': 7.293335939026774}\nEpoch: 7.2972444791870235, Logs: {'loss': 0.0431, 'grad_norm': 0.1960243284702301, 'learning_rate': 5.408913213448007e-06, 'epoch': 7.2972444791870235}\nEpoch: 7.301153019347273, Logs: {'loss': 0.0439, 'grad_norm': 0.26044169068336487, 'learning_rate': 5.401094605160282e-06, 'epoch': 7.301153019347273}\nEpoch: 7.305061559507524, Logs: {'loss': 0.0421, 'grad_norm': 0.13152940571308136, 'learning_rate': 5.393275996872557e-06, 'epoch': 7.305061559507524}\nEpoch: 7.308970099667774, Logs: {'loss': 0.0389, 'grad_norm': 0.2967362403869629, 'learning_rate': 5.3854573885848326e-06, 'epoch': 7.308970099667774}\nEpoch: 7.312878639828024, Logs: {'loss': 0.0381, 'grad_norm': 0.1474619209766388, 'learning_rate': 5.3776387802971074e-06, 'epoch': 7.312878639828024}\nEpoch: 7.316787179988275, Logs: {'loss': 0.0442, 'grad_norm': 0.6727670431137085, 'learning_rate': 5.369820172009382e-06, 'epoch': 7.316787179988275}\nEpoch: 7.3206957201485245, Logs: {'loss': 0.0437, 'grad_norm': 0.29120370745658875, 'learning_rate': 5.362001563721658e-06, 'epoch': 7.3206957201485245}\nEpoch: 7.324604260308774, Logs: {'loss': 0.0404, 'grad_norm': 0.08723144978284836, 'learning_rate': 5.354182955433933e-06, 'epoch': 7.324604260308774}\nEpoch: 7.328512800469025, Logs: {'loss': 0.0523, 'grad_norm': 0.1023968830704689, 'learning_rate': 5.346364347146208e-06, 'epoch': 7.328512800469025}\nEpoch: 7.332421340629275, Logs: {'loss': 0.0393, 'grad_norm': 0.6089016199111938, 'learning_rate': 5.3385457388584835e-06, 'epoch': 7.332421340629275}\nEpoch: 7.336329880789525, Logs: {'loss': 0.0412, 'grad_norm': 0.24189279973506927, 'learning_rate': 5.330727130570758e-06, 'epoch': 7.336329880789525}\nEpoch: 7.340238420949775, Logs: {'loss': 0.0378, 'grad_norm': 0.12071462720632553, 'learning_rate': 5.322908522283035e-06, 'epoch': 7.340238420949775}\nEpoch: 7.344146961110026, Logs: {'loss': 0.038, 'grad_norm': 0.11291246861219406, 'learning_rate': 5.31508991399531e-06, 'epoch': 7.344146961110026}\nEpoch: 7.3480555012702755, Logs: {'loss': 0.038, 'grad_norm': 0.08095881342887878, 'learning_rate': 5.307271305707584e-06, 'epoch': 7.3480555012702755}\nEpoch: 7.351964041430525, Logs: {'loss': 0.0428, 'grad_norm': 0.4282465875148773, 'learning_rate': 5.29945269741986e-06, 'epoch': 7.351964041430525}\nEpoch: 7.355872581590776, Logs: {'loss': 0.043, 'grad_norm': 0.24720630049705505, 'learning_rate': 5.291634089132135e-06, 'epoch': 7.355872581590776}\nEpoch: 7.359781121751026, Logs: {'loss': 0.0379, 'grad_norm': 0.19799672067165375, 'learning_rate': 5.28381548084441e-06, 'epoch': 7.359781121751026}\nEpoch: 7.363689661911276, Logs: {'loss': 0.0448, 'grad_norm': 0.08858323842287064, 'learning_rate': 5.275996872556686e-06, 'epoch': 7.363689661911276}\nEpoch: 7.367598202071527, Logs: {'loss': 0.0408, 'grad_norm': 0.14268535375595093, 'learning_rate': 5.268178264268961e-06, 'epoch': 7.367598202071527}\nEpoch: 7.371506742231777, Logs: {'loss': 0.0394, 'grad_norm': 0.1821826845407486, 'learning_rate': 5.2603596559812355e-06, 'epoch': 7.371506742231777}\nEpoch: 7.3754152823920265, Logs: {'loss': 0.0441, 'grad_norm': 0.1734704077243805, 'learning_rate': 5.252541047693511e-06, 'epoch': 7.3754152823920265}\nEpoch: 7.379323822552276, Logs: {'loss': 0.0386, 'grad_norm': 0.1934329867362976, 'learning_rate': 5.244722439405786e-06, 'epoch': 7.379323822552276}\nEpoch: 7.383232362712527, Logs: {'loss': 0.0456, 'grad_norm': 0.10994308441877365, 'learning_rate': 5.236903831118061e-06, 'epoch': 7.383232362712527}\nEpoch: 7.387140902872777, Logs: {'loss': 0.0402, 'grad_norm': 0.1782723218202591, 'learning_rate': 5.229085222830337e-06, 'epoch': 7.387140902872777}\nEpoch: 7.391049443033027, Logs: {'loss': 0.0435, 'grad_norm': 0.2928078770637512, 'learning_rate': 5.2212666145426115e-06, 'epoch': 7.391049443033027}\nEpoch: 7.394957983193278, Logs: {'loss': 0.0455, 'grad_norm': 0.21669739484786987, 'learning_rate': 5.213448006254887e-06, 'epoch': 7.394957983193278}\nEpoch: 7.398866523353528, Logs: {'loss': 0.0451, 'grad_norm': 0.16466468572616577, 'learning_rate': 5.205629397967162e-06, 'epoch': 7.398866523353528}\nEpoch: 7.4027750635137775, Logs: {'loss': 0.0372, 'grad_norm': 0.24501237273216248, 'learning_rate': 5.197810789679437e-06, 'epoch': 7.4027750635137775}\nEpoch: 7.406683603674027, Logs: {'loss': 0.0482, 'grad_norm': 0.17685961723327637, 'learning_rate': 5.1899921813917135e-06, 'epoch': 7.406683603674027}\nEpoch: 7.410592143834278, Logs: {'loss': 0.0436, 'grad_norm': 0.11694206297397614, 'learning_rate': 5.1821735731039876e-06, 'epoch': 7.410592143834278}\nEpoch: 7.414500683994528, Logs: {'loss': 0.052, 'grad_norm': 0.16658295691013336, 'learning_rate': 5.1743549648162624e-06, 'epoch': 7.414500683994528}\nEpoch: 7.418409224154778, Logs: {'loss': 0.0409, 'grad_norm': 0.13369201123714447, 'learning_rate': 5.166536356528539e-06, 'epoch': 7.418409224154778}\nEpoch: 7.422317764315029, Logs: {'loss': 0.0365, 'grad_norm': 0.09666158258914948, 'learning_rate': 5.158717748240814e-06, 'epoch': 7.422317764315029}\nEpoch: 7.4262263044752785, Logs: {'loss': 0.0474, 'grad_norm': 0.1010873094201088, 'learning_rate': 5.150899139953089e-06, 'epoch': 7.4262263044752785}\nEpoch: 7.430134844635528, Logs: {'loss': 0.0397, 'grad_norm': 0.07043496519327164, 'learning_rate': 5.1430805316653644e-06, 'epoch': 7.430134844635528}\nEpoch: 7.434043384795778, Logs: {'loss': 0.0454, 'grad_norm': 0.10700175166130066, 'learning_rate': 5.135261923377639e-06, 'epoch': 7.434043384795778}\nEpoch: 7.437951924956029, Logs: {'loss': 0.0461, 'grad_norm': 0.18676702678203583, 'learning_rate': 5.127443315089914e-06, 'epoch': 7.437951924956029}\nEpoch: 7.441860465116279, Logs: {'loss': 0.0394, 'grad_norm': 0.07073837518692017, 'learning_rate': 5.11962470680219e-06, 'epoch': 7.441860465116279}\nEpoch: 7.445769005276529, Logs: {'loss': 0.0375, 'grad_norm': 0.08520951867103577, 'learning_rate': 5.111806098514465e-06, 'epoch': 7.445769005276529}\nEpoch: 7.44967754543678, Logs: {'loss': 0.0404, 'grad_norm': 0.09879975765943527, 'learning_rate': 5.1039874902267405e-06, 'epoch': 7.44967754543678}\nEpoch: 7.4535860855970295, Logs: {'loss': 0.0418, 'grad_norm': 0.1756998896598816, 'learning_rate': 5.096168881939015e-06, 'epoch': 7.4535860855970295}\nEpoch: 7.457494625757279, Logs: {'loss': 0.039, 'grad_norm': 0.13739249110221863, 'learning_rate': 5.08835027365129e-06, 'epoch': 7.457494625757279}\nEpoch: 7.46140316591753, Logs: {'loss': 0.0372, 'grad_norm': 0.12358535081148148, 'learning_rate': 5.080531665363566e-06, 'epoch': 7.46140316591753}\nEpoch: 7.46531170607778, Logs: {'loss': 0.0367, 'grad_norm': 0.12917692959308624, 'learning_rate': 5.072713057075841e-06, 'epoch': 7.46531170607778}\nEpoch: 7.46922024623803, Logs: {'loss': 0.0383, 'grad_norm': 0.1106950119137764, 'learning_rate': 5.064894448788116e-06, 'epoch': 7.46922024623803}\nEpoch: 7.473128786398281, Logs: {'loss': 0.0362, 'grad_norm': 0.12265595048666, 'learning_rate': 5.057075840500391e-06, 'epoch': 7.473128786398281}\nEpoch: 7.477037326558531, Logs: {'loss': 0.0452, 'grad_norm': 0.1161426529288292, 'learning_rate': 5.049257232212666e-06, 'epoch': 7.477037326558531}\nEpoch: 7.4809458667187805, Logs: {'loss': 0.0426, 'grad_norm': 0.25151023268699646, 'learning_rate': 5.041438623924941e-06, 'epoch': 7.4809458667187805}\nEpoch: 7.48485440687903, Logs: {'loss': 0.0435, 'grad_norm': 0.23108205199241638, 'learning_rate': 5.033620015637218e-06, 'epoch': 7.48485440687903}\nEpoch: 7.488762947039281, Logs: {'loss': 0.0417, 'grad_norm': 0.20959872007369995, 'learning_rate': 5.025801407349492e-06, 'epoch': 7.488762947039281}\nEpoch: 7.492671487199531, Logs: {'loss': 0.0447, 'grad_norm': 0.16370418667793274, 'learning_rate': 5.0179827990617665e-06, 'epoch': 7.492671487199531}\nEpoch: 7.496580027359781, Logs: {'loss': 0.0429, 'grad_norm': 0.2102341204881668, 'learning_rate': 5.010164190774043e-06, 'epoch': 7.496580027359781}\nEpoch: 7.500488567520032, Logs: {'loss': 0.0458, 'grad_norm': 0.30354052782058716, 'learning_rate': 5.002345582486318e-06, 'epoch': 7.500488567520032}\nEpoch: 7.504397107680282, Logs: {'loss': 0.0398, 'grad_norm': 0.12646989524364471, 'learning_rate': 4.994526974198593e-06, 'epoch': 7.504397107680282}\nEpoch: 7.5083056478405314, Logs: {'loss': 0.046, 'grad_norm': 0.17378482222557068, 'learning_rate': 4.9867083659108685e-06, 'epoch': 7.5083056478405314}\nEpoch: 7.512214188000781, Logs: {'loss': 0.0419, 'grad_norm': 0.41735848784446716, 'learning_rate': 4.978889757623143e-06, 'epoch': 7.512214188000781}\nEpoch: 7.516122728161032, Logs: {'loss': 0.0452, 'grad_norm': 0.3582090735435486, 'learning_rate': 4.971071149335418e-06, 'epoch': 7.516122728161032}\nEpoch: 7.520031268321282, Logs: {'loss': 0.0425, 'grad_norm': 0.12363053858280182, 'learning_rate': 4.963252541047694e-06, 'epoch': 7.520031268321282}\nEpoch: 7.523939808481532, Logs: {'loss': 0.0411, 'grad_norm': 0.17111000418663025, 'learning_rate': 4.95543393275997e-06, 'epoch': 7.523939808481532}\nEpoch: 7.527848348641783, Logs: {'loss': 0.0443, 'grad_norm': 0.13137590885162354, 'learning_rate': 4.9476153244722446e-06, 'epoch': 7.527848348641783}\nEpoch: 7.5317568888020325, Logs: {'loss': 0.0481, 'grad_norm': 0.09086287766695023, 'learning_rate': 4.939796716184519e-06, 'epoch': 7.5317568888020325}\nEpoch: 7.535665428962282, Logs: {'loss': 0.0389, 'grad_norm': 0.11552802473306656, 'learning_rate': 4.931978107896795e-06, 'epoch': 7.535665428962282}\nEpoch: 7.539573969122532, Logs: {'loss': 0.0452, 'grad_norm': 0.19364552199840546, 'learning_rate': 4.92415949960907e-06, 'epoch': 7.539573969122532}\nEpoch: 7.543482509282783, Logs: {'loss': 0.0404, 'grad_norm': 0.3627701997756958, 'learning_rate': 4.916340891321345e-06, 'epoch': 7.543482509282783}\nEpoch: 7.547391049443033, Logs: {'loss': 0.0407, 'grad_norm': 0.07607059180736542, 'learning_rate': 4.908522283033621e-06, 'epoch': 7.547391049443033}\nEpoch: 7.551299589603283, Logs: {'loss': 0.038, 'grad_norm': 0.2077876776456833, 'learning_rate': 4.9007036747458954e-06, 'epoch': 7.551299589603283}\nEpoch: 7.555208129763534, Logs: {'loss': 0.0468, 'grad_norm': 0.22513660788536072, 'learning_rate': 4.89288506645817e-06, 'epoch': 7.555208129763534}\nEpoch: 7.5591166699237835, Logs: {'loss': 0.041, 'grad_norm': 0.2036518156528473, 'learning_rate': 4.885066458170446e-06, 'epoch': 7.5591166699237835}\nEpoch: 7.563025210084033, Logs: {'loss': 0.0417, 'grad_norm': 0.3192082345485687, 'learning_rate': 4.877247849882722e-06, 'epoch': 7.563025210084033}\nEpoch: 7.566933750244283, Logs: {'loss': 0.0417, 'grad_norm': 0.08835721015930176, 'learning_rate': 4.869429241594997e-06, 'epoch': 7.566933750244283}\nEpoch: 7.570842290404534, Logs: {'loss': 0.0405, 'grad_norm': 0.25453701615333557, 'learning_rate': 4.8616106333072715e-06, 'epoch': 7.570842290404534}\nEpoch: 7.574750830564784, Logs: {'loss': 0.0476, 'grad_norm': 0.2186194509267807, 'learning_rate': 4.853792025019547e-06, 'epoch': 7.574750830564784}\nEpoch: 7.578659370725034, Logs: {'loss': 0.0412, 'grad_norm': 0.1073991060256958, 'learning_rate': 4.845973416731822e-06, 'epoch': 7.578659370725034}\nEpoch: 7.582567910885285, Logs: {'loss': 0.0423, 'grad_norm': 0.17155030369758606, 'learning_rate': 4.838154808444097e-06, 'epoch': 7.582567910885285}\nEpoch: 7.5864764510455345, Logs: {'loss': 0.0406, 'grad_norm': 0.099831223487854, 'learning_rate': 4.830336200156373e-06, 'epoch': 7.5864764510455345}\nEpoch: 7.590384991205784, Logs: {'loss': 0.0367, 'grad_norm': 0.11805617064237595, 'learning_rate': 4.8225175918686475e-06, 'epoch': 7.590384991205784}\nEpoch: 7.594293531366035, Logs: {'loss': 0.0414, 'grad_norm': 0.14813962578773499, 'learning_rate': 4.814698983580923e-06, 'epoch': 7.594293531366035}\nEpoch: 7.598202071526285, Logs: {'loss': 0.0396, 'grad_norm': 0.2957436740398407, 'learning_rate': 4.806880375293198e-06, 'epoch': 7.598202071526285}\nEpoch: 7.602110611686535, Logs: {'loss': 0.0473, 'grad_norm': 0.21233238279819489, 'learning_rate': 4.799061767005474e-06, 'epoch': 7.602110611686535}\nEpoch: 7.606019151846786, Logs: {'loss': 0.0414, 'grad_norm': 0.34341198205947876, 'learning_rate': 4.791243158717749e-06, 'epoch': 7.606019151846786}\nEpoch: 7.609927692007036, Logs: {'loss': 0.0531, 'grad_norm': 0.11337510496377945, 'learning_rate': 4.7834245504300235e-06, 'epoch': 7.609927692007036}\nEpoch: 7.6138362321672854, Logs: {'loss': 0.0411, 'grad_norm': 0.5810359716415405, 'learning_rate': 4.775605942142299e-06, 'epoch': 7.6138362321672854}\nEpoch: 7.617744772327535, Logs: {'loss': 0.0399, 'grad_norm': 0.17090152204036713, 'learning_rate': 4.767787333854574e-06, 'epoch': 7.617744772327535}\nEpoch: 7.621653312487786, Logs: {'loss': 0.0408, 'grad_norm': 0.217790886759758, 'learning_rate': 4.75996872556685e-06, 'epoch': 7.621653312487786}\nEpoch: 7.625561852648036, Logs: {'loss': 0.0434, 'grad_norm': 0.24765318632125854, 'learning_rate': 4.752150117279125e-06, 'epoch': 7.625561852648036}\nEpoch: 7.629470392808286, Logs: {'loss': 0.0444, 'grad_norm': 0.138325035572052, 'learning_rate': 4.7443315089913995e-06, 'epoch': 7.629470392808286}\nEpoch: 7.633378932968537, Logs: {'loss': 0.0428, 'grad_norm': 0.20760315656661987, 'learning_rate': 4.736512900703675e-06, 'epoch': 7.633378932968537}\nEpoch: 7.6372874731287865, Logs: {'loss': 0.0445, 'grad_norm': 0.09157264232635498, 'learning_rate': 4.72869429241595e-06, 'epoch': 7.6372874731287865}\nEpoch: 7.641196013289036, Logs: {'loss': 0.046, 'grad_norm': 0.18837256729602814, 'learning_rate': 4.720875684128226e-06, 'epoch': 7.641196013289036}\nEpoch: 7.645104553449286, Logs: {'loss': 0.0424, 'grad_norm': 0.1067662313580513, 'learning_rate': 4.713057075840501e-06, 'epoch': 7.645104553449286}\nEpoch: 7.649013093609537, Logs: {'loss': 0.0394, 'grad_norm': 0.2012084275484085, 'learning_rate': 4.705238467552776e-06, 'epoch': 7.649013093609537}\nEpoch: 7.652921633769787, Logs: {'loss': 0.046, 'grad_norm': 0.5397312045097351, 'learning_rate': 4.697419859265051e-06, 'epoch': 7.652921633769787}\nEpoch: 7.656830173930037, Logs: {'loss': 0.0438, 'grad_norm': 0.2100776582956314, 'learning_rate': 4.689601250977326e-06, 'epoch': 7.656830173930037}\nEpoch: 7.660738714090288, Logs: {'loss': 0.0454, 'grad_norm': 0.09672385454177856, 'learning_rate': 4.681782642689602e-06, 'epoch': 7.660738714090288}\nEpoch: 7.6646472542505375, Logs: {'loss': 0.0419, 'grad_norm': 0.13355553150177002, 'learning_rate': 4.673964034401877e-06, 'epoch': 7.6646472542505375}\nEpoch: 7.668555794410787, Logs: {'loss': 0.037, 'grad_norm': 0.2621747553348541, 'learning_rate': 4.666145426114152e-06, 'epoch': 7.668555794410787}\nEpoch: 7.672464334571037, Logs: {'loss': 0.0416, 'grad_norm': 0.15643173456192017, 'learning_rate': 4.658326817826427e-06, 'epoch': 7.672464334571037}\nEpoch: 7.676372874731288, Logs: {'loss': 0.042, 'grad_norm': 0.205941841006279, 'learning_rate': 4.650508209538703e-06, 'epoch': 7.676372874731288}\nEpoch: 7.680281414891538, Logs: {'loss': 0.0385, 'grad_norm': 0.2920638620853424, 'learning_rate': 4.642689601250978e-06, 'epoch': 7.680281414891538}\nEpoch: 7.684189955051788, Logs: {'loss': 0.0419, 'grad_norm': 0.11280439794063568, 'learning_rate': 4.634870992963253e-06, 'epoch': 7.684189955051788}\nEpoch: 7.688098495212039, Logs: {'loss': 0.04, 'grad_norm': 0.10578656196594238, 'learning_rate': 4.6270523846755285e-06, 'epoch': 7.688098495212039}\nEpoch: 7.6920070353722885, Logs: {'loss': 0.0377, 'grad_norm': 0.2594155967235565, 'learning_rate': 4.619233776387803e-06, 'epoch': 7.6920070353722885}\nEpoch: 7.695915575532538, Logs: {'loss': 0.0392, 'grad_norm': 0.11632334440946579, 'learning_rate': 4.611415168100078e-06, 'epoch': 7.695915575532538}\nEpoch: 7.699824115692789, Logs: {'loss': 0.0428, 'grad_norm': 0.2602822780609131, 'learning_rate': 4.603596559812354e-06, 'epoch': 7.699824115692789}\nEpoch: 7.703732655853039, Logs: {'loss': 0.0389, 'grad_norm': 0.06326732784509659, 'learning_rate': 4.59577795152463e-06, 'epoch': 7.703732655853039}\nEpoch: 7.707641196013289, Logs: {'loss': 0.0366, 'grad_norm': 0.16609932482242584, 'learning_rate': 4.5879593432369045e-06, 'epoch': 7.707641196013289}\nEpoch: 7.71154973617354, Logs: {'loss': 0.0419, 'grad_norm': 0.22344231605529785, 'learning_rate': 4.580140734949179e-06, 'epoch': 7.71154973617354}\nEpoch: 7.7154582763337896, Logs: {'loss': 0.0416, 'grad_norm': 0.12711471319198608, 'learning_rate': 4.572322126661455e-06, 'epoch': 7.7154582763337896}\nEpoch: 7.719366816494039, Logs: {'loss': 0.0383, 'grad_norm': 0.12402834743261337, 'learning_rate': 4.56450351837373e-06, 'epoch': 7.719366816494039}\nEpoch: 7.723275356654289, Logs: {'loss': 0.0426, 'grad_norm': 0.09905309230089188, 'learning_rate': 4.556684910086005e-06, 'epoch': 7.723275356654289}\nEpoch: 7.72718389681454, Logs: {'loss': 0.0404, 'grad_norm': 0.08993630111217499, 'learning_rate': 4.5488663017982805e-06, 'epoch': 7.72718389681454}\nEpoch: 7.73109243697479, Logs: {'loss': 0.0421, 'grad_norm': 0.1081121489405632, 'learning_rate': 4.541047693510555e-06, 'epoch': 7.73109243697479}\nEpoch: 7.73500097713504, Logs: {'loss': 0.0393, 'grad_norm': 0.3797449469566345, 'learning_rate': 4.53322908522283e-06, 'epoch': 7.73500097713504}\nEpoch: 7.738909517295291, Logs: {'loss': 0.0406, 'grad_norm': 0.47837743163108826, 'learning_rate': 4.525410476935106e-06, 'epoch': 7.738909517295291}\nEpoch: 7.7428180574555405, Logs: {'loss': 0.0394, 'grad_norm': 0.23841652274131775, 'learning_rate': 4.517591868647382e-06, 'epoch': 7.7428180574555405}\nEpoch: 7.74672659761579, Logs: {'loss': 0.0388, 'grad_norm': 0.16352304816246033, 'learning_rate': 4.5097732603596565e-06, 'epoch': 7.74672659761579}\nEpoch: 7.75063513777604, Logs: {'loss': 0.0482, 'grad_norm': 0.20724813640117645, 'learning_rate': 4.501954652071931e-06, 'epoch': 7.75063513777604}\nEpoch: 7.754543677936291, Logs: {'loss': 0.0415, 'grad_norm': 0.07567485421895981, 'learning_rate': 4.494136043784207e-06, 'epoch': 7.754543677936291}\nEpoch: 7.758452218096541, Logs: {'loss': 0.0442, 'grad_norm': 0.3160535395145416, 'learning_rate': 4.486317435496482e-06, 'epoch': 7.758452218096541}\nEpoch: 7.762360758256791, Logs: {'loss': 0.0388, 'grad_norm': 0.06519687920808792, 'learning_rate': 4.478498827208757e-06, 'epoch': 7.762360758256791}\nEpoch: 7.766269298417042, Logs: {'loss': 0.0397, 'grad_norm': 0.08628664910793304, 'learning_rate': 4.4706802189210326e-06, 'epoch': 7.766269298417042}\nEpoch: 7.7701778385772915, Logs: {'loss': 0.048, 'grad_norm': 0.08974762260913849, 'learning_rate': 4.462861610633307e-06, 'epoch': 7.7701778385772915}\nEpoch: 7.774086378737541, Logs: {'loss': 0.0425, 'grad_norm': 0.2503361105918884, 'learning_rate': 4.455043002345582e-06, 'epoch': 7.774086378737541}\nEpoch: 7.777994918897791, Logs: {'loss': 0.038, 'grad_norm': 0.1568797528743744, 'learning_rate': 4.447224394057858e-06, 'epoch': 7.777994918897791}\nEpoch: 7.781903459058042, Logs: {'loss': 0.0403, 'grad_norm': 0.08305343240499496, 'learning_rate': 4.439405785770134e-06, 'epoch': 7.781903459058042}\nEpoch: 7.785811999218292, Logs: {'loss': 0.0381, 'grad_norm': 0.15867826342582703, 'learning_rate': 4.431587177482409e-06, 'epoch': 7.785811999218292}\nEpoch: 7.789720539378542, Logs: {'loss': 0.0459, 'grad_norm': 0.12711189687252045, 'learning_rate': 4.4237685691946834e-06, 'epoch': 7.789720539378542}\nEpoch: 7.793629079538793, Logs: {'loss': 0.0369, 'grad_norm': 0.2959423363208771, 'learning_rate': 4.415949960906959e-06, 'epoch': 7.793629079538793}\nEpoch: 7.7975376196990425, Logs: {'loss': 0.0381, 'grad_norm': 0.08387432247400284, 'learning_rate': 4.408131352619234e-06, 'epoch': 7.7975376196990425}\nEpoch: 7.801446159859292, Logs: {'loss': 0.045, 'grad_norm': 0.17863048613071442, 'learning_rate': 4.400312744331509e-06, 'epoch': 7.801446159859292}\nEpoch: 7.805354700019542, Logs: {'loss': 0.0392, 'grad_norm': 0.11981140077114105, 'learning_rate': 4.392494136043785e-06, 'epoch': 7.805354700019542}\nEpoch: 7.809263240179793, Logs: {'loss': 0.04, 'grad_norm': 0.11179319769144058, 'learning_rate': 4.3846755277560595e-06, 'epoch': 7.809263240179793}\nEpoch: 7.813171780340043, Logs: {'loss': 0.035, 'grad_norm': 0.2461814284324646, 'learning_rate': 4.376856919468335e-06, 'epoch': 7.813171780340043}\nEpoch: 7.817080320500293, Logs: {'loss': 0.042, 'grad_norm': 0.23316071927547455, 'learning_rate': 4.36903831118061e-06, 'epoch': 7.817080320500293}\nEpoch: 7.8209888606605436, Logs: {'loss': 0.0455, 'grad_norm': 0.10622374713420868, 'learning_rate': 4.361219702892886e-06, 'epoch': 7.8209888606605436}\nEpoch: 7.824897400820793, Logs: {'loss': 0.0425, 'grad_norm': 0.12179495394229889, 'learning_rate': 4.353401094605161e-06, 'epoch': 7.824897400820793}\nEpoch: 7.828805940981043, Logs: {'loss': 0.0443, 'grad_norm': 0.23589883744716644, 'learning_rate': 4.3455824863174355e-06, 'epoch': 7.828805940981043}\nEpoch: 7.832714481141294, Logs: {'loss': 0.036, 'grad_norm': 0.15229777991771698, 'learning_rate': 4.337763878029711e-06, 'epoch': 7.832714481141294}\nEpoch: 7.836623021301544, Logs: {'loss': 0.0422, 'grad_norm': 0.09709173440933228, 'learning_rate': 4.329945269741986e-06, 'epoch': 7.836623021301544}\nEpoch: 7.840531561461794, Logs: {'loss': 0.0404, 'grad_norm': 0.1260053813457489, 'learning_rate': 4.322126661454262e-06, 'epoch': 7.840531561461794}\nEpoch: 7.844440101622045, Logs: {'loss': 0.0386, 'grad_norm': 0.08287567645311356, 'learning_rate': 4.314308053166537e-06, 'epoch': 7.844440101622045}\nEpoch: 7.8483486417822945, Logs: {'loss': 0.0407, 'grad_norm': 0.1452045887708664, 'learning_rate': 4.3064894448788115e-06, 'epoch': 7.8483486417822945}\nEpoch: 7.852257181942544, Logs: {'loss': 0.041, 'grad_norm': 0.27520751953125, 'learning_rate': 4.298670836591087e-06, 'epoch': 7.852257181942544}\nEpoch: 7.856165722102794, Logs: {'loss': 0.0424, 'grad_norm': 0.09291262179613113, 'learning_rate': 4.290852228303362e-06, 'epoch': 7.856165722102794}\nEpoch: 7.860074262263045, Logs: {'loss': 0.0481, 'grad_norm': 0.1857329159975052, 'learning_rate': 4.283033620015638e-06, 'epoch': 7.860074262263045}\nEpoch: 7.863982802423295, Logs: {'loss': 0.042, 'grad_norm': 0.13644513487815857, 'learning_rate': 4.275215011727913e-06, 'epoch': 7.863982802423295}\nEpoch: 7.867891342583545, Logs: {'loss': 0.0393, 'grad_norm': 0.14158889651298523, 'learning_rate': 4.267396403440188e-06, 'epoch': 7.867891342583545}\nEpoch: 7.871799882743796, Logs: {'loss': 0.0415, 'grad_norm': 0.24212069809436798, 'learning_rate': 4.259577795152463e-06, 'epoch': 7.871799882743796}\nEpoch: 7.8757084229040455, Logs: {'loss': 0.0473, 'grad_norm': 0.1861463189125061, 'learning_rate': 4.251759186864738e-06, 'epoch': 7.8757084229040455}\nEpoch: 7.879616963064295, Logs: {'loss': 0.0404, 'grad_norm': 0.28519296646118164, 'learning_rate': 4.243940578577014e-06, 'epoch': 7.879616963064295}\nEpoch: 7.883525503224545, Logs: {'loss': 0.0403, 'grad_norm': 0.17468789219856262, 'learning_rate': 4.236121970289289e-06, 'epoch': 7.883525503224545}\nEpoch: 7.887434043384796, Logs: {'loss': 0.0445, 'grad_norm': 0.3860166072845459, 'learning_rate': 4.228303362001564e-06, 'epoch': 7.887434043384796}\nEpoch: 7.891342583545046, Logs: {'loss': 0.0402, 'grad_norm': 0.22459538280963898, 'learning_rate': 4.220484753713839e-06, 'epoch': 7.891342583545046}\nEpoch: 7.895251123705296, Logs: {'loss': 0.0444, 'grad_norm': 0.14344771206378937, 'learning_rate': 4.212666145426115e-06, 'epoch': 7.895251123705296}\nEpoch: 7.899159663865547, Logs: {'loss': 0.0419, 'grad_norm': 0.13852053880691528, 'learning_rate': 4.20484753713839e-06, 'epoch': 7.899159663865547}\nEpoch: 7.9030682040257965, Logs: {'loss': 0.0427, 'grad_norm': 0.1405656337738037, 'learning_rate': 4.197028928850665e-06, 'epoch': 7.9030682040257965}\nEpoch: 7.906976744186046, Logs: {'loss': 0.0406, 'grad_norm': 0.15741589665412903, 'learning_rate': 4.1892103205629404e-06, 'epoch': 7.906976744186046}\nEpoch: 7.910885284346296, Logs: {'loss': 0.0474, 'grad_norm': 0.18510235846042633, 'learning_rate': 4.181391712275215e-06, 'epoch': 7.910885284346296}\nEpoch: 7.914793824506547, Logs: {'loss': 0.0415, 'grad_norm': 0.16121774911880493, 'learning_rate': 4.17357310398749e-06, 'epoch': 7.914793824506547}\nEpoch: 7.918702364666797, Logs: {'loss': 0.0424, 'grad_norm': 0.2387634664773941, 'learning_rate': 4.165754495699766e-06, 'epoch': 7.918702364666797}\nEpoch: 7.922610904827047, Logs: {'loss': 0.042, 'grad_norm': 0.22862333059310913, 'learning_rate': 4.157935887412042e-06, 'epoch': 7.922610904827047}\nEpoch: 7.9265194449872975, Logs: {'loss': 0.0443, 'grad_norm': 0.15745826065540314, 'learning_rate': 4.1501172791243165e-06, 'epoch': 7.9265194449872975}\nEpoch: 7.930427985147547, Logs: {'loss': 0.0386, 'grad_norm': 0.15612751245498657, 'learning_rate': 4.142298670836591e-06, 'epoch': 7.930427985147547}\nEpoch: 7.934336525307797, Logs: {'loss': 0.0412, 'grad_norm': 0.2402796894311905, 'learning_rate': 4.134480062548867e-06, 'epoch': 7.934336525307797}\nEpoch: 7.938245065468047, Logs: {'loss': 0.0385, 'grad_norm': 0.31004294753074646, 'learning_rate': 4.126661454261142e-06, 'epoch': 7.938245065468047}\nEpoch: 7.942153605628298, Logs: {'loss': 0.0408, 'grad_norm': 0.13458825647830963, 'learning_rate': 4.118842845973417e-06, 'epoch': 7.942153605628298}\nEpoch: 7.946062145788548, Logs: {'loss': 0.0393, 'grad_norm': 0.34107640385627747, 'learning_rate': 4.1110242376856925e-06, 'epoch': 7.946062145788548}\nEpoch: 7.949970685948798, Logs: {'loss': 0.0423, 'grad_norm': 0.33911609649658203, 'learning_rate': 4.103205629397967e-06, 'epoch': 7.949970685948798}\nEpoch: 7.9538792261090485, Logs: {'loss': 0.0447, 'grad_norm': 0.31781333684921265, 'learning_rate': 4.095387021110242e-06, 'epoch': 7.9538792261090485}\nEpoch: 7.957787766269298, Logs: {'loss': 0.0444, 'grad_norm': 0.20163707435131073, 'learning_rate': 4.087568412822518e-06, 'epoch': 7.957787766269298}\nEpoch: 7.961696306429548, Logs: {'loss': 0.0397, 'grad_norm': 0.10888244956731796, 'learning_rate': 4.079749804534794e-06, 'epoch': 7.961696306429548}\nEpoch: 7.965604846589799, Logs: {'loss': 0.0411, 'grad_norm': 0.23513081669807434, 'learning_rate': 4.0719311962470685e-06, 'epoch': 7.965604846589799}\nEpoch: 7.969513386750049, Logs: {'loss': 0.0461, 'grad_norm': 0.13377390801906586, 'learning_rate': 4.064112587959343e-06, 'epoch': 7.969513386750049}\nEpoch: 7.973421926910299, Logs: {'loss': 0.038, 'grad_norm': 0.09149321913719177, 'learning_rate': 4.056293979671619e-06, 'epoch': 7.973421926910299}\nEpoch: 7.97733046707055, Logs: {'loss': 0.0402, 'grad_norm': 0.17939014732837677, 'learning_rate': 4.048475371383894e-06, 'epoch': 7.97733046707055}\nEpoch: 7.9812390072307995, Logs: {'loss': 0.0445, 'grad_norm': 0.3222425580024719, 'learning_rate': 4.040656763096169e-06, 'epoch': 7.9812390072307995}\nEpoch: 7.985147547391049, Logs: {'loss': 0.0464, 'grad_norm': 0.16784095764160156, 'learning_rate': 4.0328381548084445e-06, 'epoch': 7.985147547391049}\nEpoch: 7.989056087551299, Logs: {'loss': 0.037, 'grad_norm': 0.08581217378377914, 'learning_rate': 4.025019546520719e-06, 'epoch': 7.989056087551299}\nEpoch: 7.99296462771155, Logs: {'loss': 0.0393, 'grad_norm': 0.0715259313583374, 'learning_rate': 4.017200938232994e-06, 'epoch': 7.99296462771155}\nEpoch: 7.9968731678718, Logs: {'loss': 0.0412, 'grad_norm': 0.08965065330266953, 'learning_rate': 4.00938232994527e-06, 'epoch': 7.9968731678718}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8.0, Logs: {'eval_loss': 0.04048850014805794, 'eval_bleu': 0.0, 'eval_accuracy': 0.9455629397967162, 'eval_runtime': 348.8835, 'eval_samples_per_second': 58.656, 'eval_steps_per_second': 3.666, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8.00078170803205, Logs: {'loss': 0.0389, 'grad_norm': 0.29725438356399536, 'learning_rate': 4.002345582486317e-06, 'epoch': 8.00078170803205}\nEpoch: 8.0046902481923, Logs: {'loss': 0.0496, 'grad_norm': 0.18161101639270782, 'learning_rate': 3.994526974198593e-06, 'epoch': 8.0046902481923}\nEpoch: 8.00859878835255, Logs: {'loss': 0.0513, 'grad_norm': 0.16988755762577057, 'learning_rate': 3.986708365910868e-06, 'epoch': 8.00859878835255}\nEpoch: 8.0125073285128, Logs: {'loss': 0.0422, 'grad_norm': 0.19423429667949677, 'learning_rate': 3.978889757623144e-06, 'epoch': 8.0125073285128}\nEpoch: 8.01641586867305, Logs: {'loss': 0.0427, 'grad_norm': 1.2838157415390015, 'learning_rate': 3.9710711493354185e-06, 'epoch': 8.01641586867305}\nEpoch: 8.0203244088333, Logs: {'loss': 0.0401, 'grad_norm': 0.14458313584327698, 'learning_rate': 3.963252541047694e-06, 'epoch': 8.0203244088333}\nEpoch: 8.024232948993552, Logs: {'loss': 0.0412, 'grad_norm': 0.10659250617027283, 'learning_rate': 3.955433932759969e-06, 'epoch': 8.024232948993552}\nEpoch: 8.028141489153802, Logs: {'loss': 0.0415, 'grad_norm': 0.2034100443124771, 'learning_rate': 3.947615324472244e-06, 'epoch': 8.028141489153802}\nEpoch: 8.032050029314052, Logs: {'loss': 0.0411, 'grad_norm': 0.2541203796863556, 'learning_rate': 3.93979671618452e-06, 'epoch': 8.032050029314052}\nEpoch: 8.035958569474301, Logs: {'loss': 0.0343, 'grad_norm': 0.08560295403003693, 'learning_rate': 3.9319781078967945e-06, 'epoch': 8.035958569474301}\nEpoch: 8.039867109634551, Logs: {'loss': 0.0411, 'grad_norm': 0.2178734689950943, 'learning_rate': 3.92415949960907e-06, 'epoch': 8.039867109634551}\nEpoch: 8.043775649794801, Logs: {'loss': 0.042, 'grad_norm': 0.23567673563957214, 'learning_rate': 3.916340891321345e-06, 'epoch': 8.043775649794801}\nEpoch: 8.047684189955051, Logs: {'loss': 0.0425, 'grad_norm': 0.1651749461889267, 'learning_rate': 3.908522283033621e-06, 'epoch': 8.047684189955051}\nEpoch: 8.051592730115303, Logs: {'loss': 0.0421, 'grad_norm': 0.12594357132911682, 'learning_rate': 3.900703674745896e-06, 'epoch': 8.051592730115303}\nEpoch: 8.055501270275553, Logs: {'loss': 0.0435, 'grad_norm': 0.1270235925912857, 'learning_rate': 3.8928850664581706e-06, 'epoch': 8.055501270275553}\nEpoch: 8.059409810435803, Logs: {'loss': 0.0414, 'grad_norm': 0.22428004443645477, 'learning_rate': 3.885066458170446e-06, 'epoch': 8.059409810435803}\nEpoch: 8.063318350596052, Logs: {'loss': 0.0475, 'grad_norm': 0.25305017828941345, 'learning_rate': 3.877247849882721e-06, 'epoch': 8.063318350596052}\nEpoch: 8.067226890756302, Logs: {'loss': 0.043, 'grad_norm': 0.1270563006401062, 'learning_rate': 3.869429241594996e-06, 'epoch': 8.067226890756302}\nEpoch: 8.071135430916552, Logs: {'loss': 0.0395, 'grad_norm': 0.2099078744649887, 'learning_rate': 3.861610633307272e-06, 'epoch': 8.071135430916552}\nEpoch: 8.075043971076802, Logs: {'loss': 0.0438, 'grad_norm': 0.32796892523765564, 'learning_rate': 3.8537920250195474e-06, 'epoch': 8.075043971076802}\nEpoch: 8.078952511237054, Logs: {'loss': 0.0438, 'grad_norm': 0.12687340378761292, 'learning_rate': 3.845973416731822e-06, 'epoch': 8.078952511237054}\nEpoch: 8.082861051397304, Logs: {'loss': 0.0436, 'grad_norm': 0.1724933683872223, 'learning_rate': 3.838154808444097e-06, 'epoch': 8.082861051397304}\nEpoch: 8.086769591557553, Logs: {'loss': 0.0405, 'grad_norm': 0.0808851346373558, 'learning_rate': 3.830336200156373e-06, 'epoch': 8.086769591557553}\nEpoch: 8.090678131717803, Logs: {'loss': 0.0415, 'grad_norm': 0.27091678977012634, 'learning_rate': 3.822517591868648e-06, 'epoch': 8.090678131717803}\nEpoch: 8.094586671878053, Logs: {'loss': 0.0449, 'grad_norm': 0.27463507652282715, 'learning_rate': 3.814698983580923e-06, 'epoch': 8.094586671878053}\nEpoch: 8.098495212038303, Logs: {'loss': 0.0413, 'grad_norm': 0.11250757426023483, 'learning_rate': 3.8068803752931983e-06, 'epoch': 8.098495212038303}\nEpoch: 8.102403752198553, Logs: {'loss': 0.0452, 'grad_norm': 0.40423500537872314, 'learning_rate': 3.7990617670054736e-06, 'epoch': 8.102403752198553}\nEpoch: 8.106312292358805, Logs: {'loss': 0.046, 'grad_norm': 0.3298569917678833, 'learning_rate': 3.7912431587177485e-06, 'epoch': 8.106312292358805}\nEpoch: 8.110220832519055, Logs: {'loss': 0.0408, 'grad_norm': 0.11707186698913574, 'learning_rate': 3.7834245504300238e-06, 'epoch': 8.110220832519055}\nEpoch: 8.114129372679304, Logs: {'loss': 0.0412, 'grad_norm': 0.1448059231042862, 'learning_rate': 3.775605942142299e-06, 'epoch': 8.114129372679304}\nEpoch: 8.118037912839554, Logs: {'loss': 0.0442, 'grad_norm': 0.08952195197343826, 'learning_rate': 3.767787333854574e-06, 'epoch': 8.118037912839554}\nEpoch: 8.121946452999804, Logs: {'loss': 0.046, 'grad_norm': 0.24061219394207, 'learning_rate': 3.7599687255668492e-06, 'epoch': 8.121946452999804}\nEpoch: 8.125854993160054, Logs: {'loss': 0.04, 'grad_norm': 0.16012626886367798, 'learning_rate': 3.7521501172791245e-06, 'epoch': 8.125854993160054}\nEpoch: 8.129763533320304, Logs: {'loss': 0.0466, 'grad_norm': 0.1395454853773117, 'learning_rate': 3.7443315089914002e-06, 'epoch': 8.129763533320304}\nEpoch: 8.133672073480556, Logs: {'loss': 0.041, 'grad_norm': 0.44030335545539856, 'learning_rate': 3.736512900703675e-06, 'epoch': 8.133672073480556}\nEpoch: 8.137580613640806, Logs: {'loss': 0.0415, 'grad_norm': 0.22171133756637573, 'learning_rate': 3.7286942924159504e-06, 'epoch': 8.137580613640806}\nEpoch: 8.141489153801055, Logs: {'loss': 0.0387, 'grad_norm': 0.6724563837051392, 'learning_rate': 3.7208756841282257e-06, 'epoch': 8.141489153801055}\nEpoch: 8.145397693961305, Logs: {'loss': 0.0396, 'grad_norm': 0.2958666980266571, 'learning_rate': 3.7130570758405005e-06, 'epoch': 8.145397693961305}\nEpoch: 8.149306234121555, Logs: {'loss': 0.0384, 'grad_norm': 0.1710807830095291, 'learning_rate': 3.705238467552776e-06, 'epoch': 8.149306234121555}\nEpoch: 8.153214774281805, Logs: {'loss': 0.0456, 'grad_norm': 0.07282520085573196, 'learning_rate': 3.697419859265051e-06, 'epoch': 8.153214774281805}\nEpoch: 8.157123314442057, Logs: {'loss': 0.0388, 'grad_norm': 0.09044648706912994, 'learning_rate': 3.6896012509773264e-06, 'epoch': 8.157123314442057}\nEpoch: 8.161031854602307, Logs: {'loss': 0.0428, 'grad_norm': 0.24768759310245514, 'learning_rate': 3.6817826426896013e-06, 'epoch': 8.161031854602307}\nEpoch: 8.164940394762557, Logs: {'loss': 0.0424, 'grad_norm': 0.12746165692806244, 'learning_rate': 3.6739640344018766e-06, 'epoch': 8.164940394762557}\nEpoch: 8.168848934922806, Logs: {'loss': 0.0396, 'grad_norm': 0.26538529992103577, 'learning_rate': 3.6661454261141523e-06, 'epoch': 8.168848934922806}\nEpoch: 8.172757475083056, Logs: {'loss': 0.0456, 'grad_norm': 0.16158542037010193, 'learning_rate': 3.658326817826427e-06, 'epoch': 8.172757475083056}\nEpoch: 8.176666015243306, Logs: {'loss': 0.0381, 'grad_norm': 0.21849995851516724, 'learning_rate': 3.6505082095387024e-06, 'epoch': 8.176666015243306}\nEpoch: 8.180574555403556, Logs: {'loss': 0.0455, 'grad_norm': 0.17101190984249115, 'learning_rate': 3.6426896012509777e-06, 'epoch': 8.180574555403556}\nEpoch: 8.184483095563808, Logs: {'loss': 0.0384, 'grad_norm': 0.11335988342761993, 'learning_rate': 3.634870992963253e-06, 'epoch': 8.184483095563808}\nEpoch: 8.188391635724058, Logs: {'loss': 0.0413, 'grad_norm': 0.4648241400718689, 'learning_rate': 3.627052384675528e-06, 'epoch': 8.188391635724058}\nEpoch: 8.192300175884307, Logs: {'loss': 0.0406, 'grad_norm': 0.11827582120895386, 'learning_rate': 3.619233776387803e-06, 'epoch': 8.192300175884307}\nEpoch: 8.196208716044557, Logs: {'loss': 0.0503, 'grad_norm': 0.31789711117744446, 'learning_rate': 3.6114151681000784e-06, 'epoch': 8.196208716044557}\nEpoch: 8.200117256204807, Logs: {'loss': 0.0381, 'grad_norm': 0.0868806466460228, 'learning_rate': 3.6035965598123533e-06, 'epoch': 8.200117256204807}\nEpoch: 8.204025796365057, Logs: {'loss': 0.042, 'grad_norm': 0.21092341840267181, 'learning_rate': 3.595777951524629e-06, 'epoch': 8.204025796365057}\nEpoch: 8.207934336525307, Logs: {'loss': 0.0417, 'grad_norm': 0.10196653008460999, 'learning_rate': 3.5879593432369043e-06, 'epoch': 8.207934336525307}\nEpoch: 8.211842876685559, Logs: {'loss': 0.0399, 'grad_norm': 0.28508758544921875, 'learning_rate': 3.5801407349491796e-06, 'epoch': 8.211842876685559}\nEpoch: 8.215751416845809, Logs: {'loss': 0.0429, 'grad_norm': 0.1986769288778305, 'learning_rate': 3.5723221266614545e-06, 'epoch': 8.215751416845809}\nEpoch: 8.219659957006058, Logs: {'loss': 0.0447, 'grad_norm': 0.08317945897579193, 'learning_rate': 3.5645035183737298e-06, 'epoch': 8.219659957006058}\nEpoch: 8.223568497166308, Logs: {'loss': 0.0394, 'grad_norm': 0.09030008316040039, 'learning_rate': 3.556684910086005e-06, 'epoch': 8.223568497166308}\nEpoch: 8.227477037326558, Logs: {'loss': 0.0447, 'grad_norm': 0.13787604868412018, 'learning_rate': 3.54886630179828e-06, 'epoch': 8.227477037326558}\nEpoch: 8.231385577486808, Logs: {'loss': 0.0415, 'grad_norm': 0.10388125479221344, 'learning_rate': 3.541047693510555e-06, 'epoch': 8.231385577486808}\nEpoch: 8.235294117647058, Logs: {'loss': 0.0409, 'grad_norm': 0.19103406369686127, 'learning_rate': 3.5332290852228305e-06, 'epoch': 8.235294117647058}\nEpoch: 8.23920265780731, Logs: {'loss': 0.0384, 'grad_norm': 0.1899871975183487, 'learning_rate': 3.525410476935106e-06, 'epoch': 8.23920265780731}\nEpoch: 8.24311119796756, Logs: {'loss': 0.0431, 'grad_norm': 0.1156771332025528, 'learning_rate': 3.517591868647381e-06, 'epoch': 8.24311119796756}\nEpoch: 8.24701973812781, Logs: {'loss': 0.0432, 'grad_norm': 0.21898077428340912, 'learning_rate': 3.5097732603596564e-06, 'epoch': 8.24701973812781}\nEpoch: 8.25092827828806, Logs: {'loss': 0.0421, 'grad_norm': 0.1271435022354126, 'learning_rate': 3.5019546520719317e-06, 'epoch': 8.25092827828806}\nEpoch: 8.25483681844831, Logs: {'loss': 0.0406, 'grad_norm': 0.161212757229805, 'learning_rate': 3.4941360437842065e-06, 'epoch': 8.25483681844831}\nEpoch: 8.258745358608559, Logs: {'loss': 0.0407, 'grad_norm': 0.23487627506256104, 'learning_rate': 3.486317435496482e-06, 'epoch': 8.258745358608559}\nEpoch: 8.26265389876881, Logs: {'loss': 0.0367, 'grad_norm': 0.15726058185100555, 'learning_rate': 3.478498827208757e-06, 'epoch': 8.26265389876881}\nEpoch: 8.26656243892906, Logs: {'loss': 0.0405, 'grad_norm': 0.11133690923452377, 'learning_rate': 3.4706802189210324e-06, 'epoch': 8.26656243892906}\nEpoch: 8.27047097908931, Logs: {'loss': 0.0456, 'grad_norm': 0.15567339956760406, 'learning_rate': 3.4628616106333073e-06, 'epoch': 8.27047097908931}\nEpoch: 8.27437951924956, Logs: {'loss': 0.0459, 'grad_norm': 0.11451364308595657, 'learning_rate': 3.455043002345583e-06, 'epoch': 8.27437951924956}\nEpoch: 8.27828805940981, Logs: {'loss': 0.0391, 'grad_norm': 0.1256953626871109, 'learning_rate': 3.4472243940578583e-06, 'epoch': 8.27828805940981}\nEpoch: 8.28219659957006, Logs: {'loss': 0.0387, 'grad_norm': 0.16021108627319336, 'learning_rate': 3.439405785770133e-06, 'epoch': 8.28219659957006}\nEpoch: 8.28610513973031, Logs: {'loss': 0.0419, 'grad_norm': 0.08784759044647217, 'learning_rate': 3.4315871774824084e-06, 'epoch': 8.28610513973031}\nEpoch: 8.290013679890562, Logs: {'loss': 0.0406, 'grad_norm': 0.3334273099899292, 'learning_rate': 3.4237685691946837e-06, 'epoch': 8.290013679890562}\nEpoch: 8.293922220050812, Logs: {'loss': 0.0467, 'grad_norm': 0.18699729442596436, 'learning_rate': 3.415949960906959e-06, 'epoch': 8.293922220050812}\nEpoch: 8.297830760211061, Logs: {'loss': 0.0445, 'grad_norm': 0.17030857503414154, 'learning_rate': 3.408131352619234e-06, 'epoch': 8.297830760211061}\nEpoch: 8.301739300371311, Logs: {'loss': 0.0403, 'grad_norm': 0.17413465678691864, 'learning_rate': 3.400312744331509e-06, 'epoch': 8.301739300371311}\nEpoch: 8.305647840531561, Logs: {'loss': 0.0467, 'grad_norm': 0.24318550527095795, 'learning_rate': 3.3924941360437844e-06, 'epoch': 8.305647840531561}\nEpoch: 8.309556380691811, Logs: {'loss': 0.0423, 'grad_norm': 0.11901696771383286, 'learning_rate': 3.3846755277560593e-06, 'epoch': 8.309556380691811}\nEpoch: 8.313464920852061, Logs: {'loss': 0.0415, 'grad_norm': 0.1399717926979065, 'learning_rate': 3.376856919468335e-06, 'epoch': 8.313464920852061}\nEpoch: 8.317373461012313, Logs: {'loss': 0.0386, 'grad_norm': 0.12212834507226944, 'learning_rate': 3.3690383111806103e-06, 'epoch': 8.317373461012313}\nEpoch: 8.321282001172563, Logs: {'loss': 0.0372, 'grad_norm': 0.10189639776945114, 'learning_rate': 3.3612197028928856e-06, 'epoch': 8.321282001172563}\nEpoch: 8.325190541332812, Logs: {'loss': 0.0415, 'grad_norm': 0.12338964641094208, 'learning_rate': 3.3534010946051605e-06, 'epoch': 8.325190541332812}\nEpoch: 8.329099081493062, Logs: {'loss': 0.0398, 'grad_norm': 0.24690456688404083, 'learning_rate': 3.3455824863174358e-06, 'epoch': 8.329099081493062}\nEpoch: 8.333007621653312, Logs: {'loss': 0.038, 'grad_norm': 0.08035346120595932, 'learning_rate': 3.337763878029711e-06, 'epoch': 8.333007621653312}\nEpoch: 8.336916161813562, Logs: {'loss': 0.0375, 'grad_norm': 0.7465376853942871, 'learning_rate': 3.329945269741986e-06, 'epoch': 8.336916161813562}\nEpoch: 8.340824701973812, Logs: {'loss': 0.0432, 'grad_norm': 0.35154491662979126, 'learning_rate': 3.322126661454261e-06, 'epoch': 8.340824701973812}\nEpoch: 8.344733242134064, Logs: {'loss': 0.0395, 'grad_norm': 0.3075672686100006, 'learning_rate': 3.314308053166537e-06, 'epoch': 8.344733242134064}\nEpoch: 8.348641782294314, Logs: {'loss': 0.0428, 'grad_norm': 0.3031025528907776, 'learning_rate': 3.306489444878812e-06, 'epoch': 8.348641782294314}\nEpoch: 8.352550322454563, Logs: {'loss': 0.0404, 'grad_norm': 1.0117098093032837, 'learning_rate': 3.298670836591087e-06, 'epoch': 8.352550322454563}\nEpoch: 8.356458862614813, Logs: {'loss': 0.0379, 'grad_norm': 0.15501751005649567, 'learning_rate': 3.2908522283033624e-06, 'epoch': 8.356458862614813}\nEpoch: 8.360367402775063, Logs: {'loss': 0.0395, 'grad_norm': 0.12627141177654266, 'learning_rate': 3.2830336200156376e-06, 'epoch': 8.360367402775063}\nEpoch: 8.364275942935313, Logs: {'loss': 0.0391, 'grad_norm': 0.20657363533973694, 'learning_rate': 3.2752150117279125e-06, 'epoch': 8.364275942935313}\nEpoch: 8.368184483095563, Logs: {'loss': 0.0378, 'grad_norm': 0.1669565588235855, 'learning_rate': 3.267396403440188e-06, 'epoch': 8.368184483095563}\nEpoch: 8.372093023255815, Logs: {'loss': 0.0392, 'grad_norm': 0.49545928835868835, 'learning_rate': 3.259577795152463e-06, 'epoch': 8.372093023255815}\nEpoch: 8.376001563416064, Logs: {'loss': 0.0433, 'grad_norm': 0.2982783019542694, 'learning_rate': 3.2517591868647384e-06, 'epoch': 8.376001563416064}\nEpoch: 8.379910103576314, Logs: {'loss': 0.0414, 'grad_norm': 0.28505566716194153, 'learning_rate': 3.2439405785770132e-06, 'epoch': 8.379910103576314}\nEpoch: 8.383818643736564, Logs: {'loss': 0.0395, 'grad_norm': 0.22236265242099762, 'learning_rate': 3.236121970289289e-06, 'epoch': 8.383818643736564}\nEpoch: 8.387727183896814, Logs: {'loss': 0.0414, 'grad_norm': 0.22595514357089996, 'learning_rate': 3.2283033620015642e-06, 'epoch': 8.387727183896814}\nEpoch: 8.391635724057064, Logs: {'loss': 0.0431, 'grad_norm': 0.11978395283222198, 'learning_rate': 3.220484753713839e-06, 'epoch': 8.391635724057064}\nEpoch: 8.395544264217314, Logs: {'loss': 0.0444, 'grad_norm': 0.12349984794855118, 'learning_rate': 3.2126661454261144e-06, 'epoch': 8.395544264217314}\nEpoch: 8.399452804377566, Logs: {'loss': 0.0456, 'grad_norm': 0.07702162861824036, 'learning_rate': 3.2048475371383897e-06, 'epoch': 8.399452804377566}\nEpoch: 8.403361344537815, Logs: {'loss': 0.0453, 'grad_norm': 0.2248092144727707, 'learning_rate': 3.197028928850665e-06, 'epoch': 8.403361344537815}\nEpoch: 8.407269884698065, Logs: {'loss': 0.0385, 'grad_norm': 0.1966371238231659, 'learning_rate': 3.18921032056294e-06, 'epoch': 8.407269884698065}\nEpoch: 8.411178424858315, Logs: {'loss': 0.0452, 'grad_norm': 0.1908246874809265, 'learning_rate': 3.181391712275215e-06, 'epoch': 8.411178424858315}\nEpoch: 8.415086965018565, Logs: {'loss': 0.0389, 'grad_norm': 0.20825225114822388, 'learning_rate': 3.1735731039874904e-06, 'epoch': 8.415086965018565}\nEpoch: 8.418995505178815, Logs: {'loss': 0.0395, 'grad_norm': 0.10723578184843063, 'learning_rate': 3.1657544956997653e-06, 'epoch': 8.418995505178815}\nEpoch: 8.422904045339067, Logs: {'loss': 0.0419, 'grad_norm': 0.2709242105484009, 'learning_rate': 3.157935887412041e-06, 'epoch': 8.422904045339067}\nEpoch: 8.426812585499317, Logs: {'loss': 0.0421, 'grad_norm': 0.25458580255508423, 'learning_rate': 3.1501172791243163e-06, 'epoch': 8.426812585499317}\nEpoch: 8.430721125659566, Logs: {'loss': 0.0405, 'grad_norm': 0.15975415706634521, 'learning_rate': 3.1422986708365916e-06, 'epoch': 8.430721125659566}\nEpoch: 8.434629665819816, Logs: {'loss': 0.0393, 'grad_norm': 0.19694174826145172, 'learning_rate': 3.1344800625488665e-06, 'epoch': 8.434629665819816}\nEpoch: 8.438538205980066, Logs: {'loss': 0.0438, 'grad_norm': 0.19272395968437195, 'learning_rate': 3.1266614542611417e-06, 'epoch': 8.438538205980066}\nEpoch: 8.442446746140316, Logs: {'loss': 0.0423, 'grad_norm': 0.08937480300664902, 'learning_rate': 3.118842845973417e-06, 'epoch': 8.442446746140316}\nEpoch: 8.446355286300566, Logs: {'loss': 0.0399, 'grad_norm': 0.12851117551326752, 'learning_rate': 3.111024237685692e-06, 'epoch': 8.446355286300566}\nEpoch: 8.450263826460818, Logs: {'loss': 0.0425, 'grad_norm': 0.34413155913352966, 'learning_rate': 3.103205629397967e-06, 'epoch': 8.450263826460818}\nEpoch: 8.454172366621068, Logs: {'loss': 0.0444, 'grad_norm': 0.1052001491189003, 'learning_rate': 3.095387021110243e-06, 'epoch': 8.454172366621068}\nEpoch: 8.458080906781317, Logs: {'loss': 0.0366, 'grad_norm': 0.09531215578317642, 'learning_rate': 3.087568412822518e-06, 'epoch': 8.458080906781317}\nEpoch: 8.461989446941567, Logs: {'loss': 0.0391, 'grad_norm': 0.19634029269218445, 'learning_rate': 3.079749804534793e-06, 'epoch': 8.461989446941567}\nEpoch: 8.465897987101817, Logs: {'loss': 0.0443, 'grad_norm': 0.5245510935783386, 'learning_rate': 3.0719311962470683e-06, 'epoch': 8.465897987101817}\nEpoch: 8.469806527262067, Logs: {'loss': 0.0414, 'grad_norm': 0.1614280492067337, 'learning_rate': 3.0641125879593436e-06, 'epoch': 8.469806527262067}\nEpoch: 8.473715067422317, Logs: {'loss': 0.0431, 'grad_norm': 0.34184595942497253, 'learning_rate': 3.0562939796716185e-06, 'epoch': 8.473715067422317}\nEpoch: 8.477623607582569, Logs: {'loss': 0.0435, 'grad_norm': 0.09759023785591125, 'learning_rate': 3.048475371383894e-06, 'epoch': 8.477623607582569}\nEpoch: 8.481532147742818, Logs: {'loss': 0.038, 'grad_norm': 0.1134888306260109, 'learning_rate': 3.040656763096169e-06, 'epoch': 8.481532147742818}\nEpoch: 8.485440687903068, Logs: {'loss': 0.0387, 'grad_norm': 0.2551262080669403, 'learning_rate': 3.0328381548084444e-06, 'epoch': 8.485440687903068}\nEpoch: 8.489349228063318, Logs: {'loss': 0.0377, 'grad_norm': 0.08385152369737625, 'learning_rate': 3.0250195465207192e-06, 'epoch': 8.489349228063318}\nEpoch: 8.493257768223568, Logs: {'loss': 0.041, 'grad_norm': 0.10148307681083679, 'learning_rate': 3.017200938232995e-06, 'epoch': 8.493257768223568}\nEpoch: 8.497166308383818, Logs: {'loss': 0.0374, 'grad_norm': 0.5741026401519775, 'learning_rate': 3.0093823299452702e-06, 'epoch': 8.497166308383818}\nEpoch: 8.501074848544068, Logs: {'loss': 0.0388, 'grad_norm': 0.7008431553840637, 'learning_rate': 3.001563721657545e-06, 'epoch': 8.501074848544068}\nEpoch: 8.50498338870432, Logs: {'loss': 0.0406, 'grad_norm': 0.380694180727005, 'learning_rate': 2.9937451133698204e-06, 'epoch': 8.50498338870432}\nEpoch: 8.50889192886457, Logs: {'loss': 0.0394, 'grad_norm': 0.12283490598201752, 'learning_rate': 2.9859265050820957e-06, 'epoch': 8.50889192886457}\nEpoch: 8.51280046902482, Logs: {'loss': 0.0441, 'grad_norm': 0.10054129362106323, 'learning_rate': 2.978107896794371e-06, 'epoch': 8.51280046902482}\nEpoch: 8.51670900918507, Logs: {'loss': 0.0494, 'grad_norm': 0.16922585666179657, 'learning_rate': 2.970289288506646e-06, 'epoch': 8.51670900918507}\nEpoch: 8.52061754934532, Logs: {'loss': 0.0368, 'grad_norm': 0.39424195885658264, 'learning_rate': 2.962470680218921e-06, 'epoch': 8.52061754934532}\nEpoch: 8.524526089505569, Logs: {'loss': 0.0396, 'grad_norm': 0.12604638934135437, 'learning_rate': 2.954652071931197e-06, 'epoch': 8.524526089505569}\nEpoch: 8.52843462966582, Logs: {'loss': 0.0413, 'grad_norm': 0.09977378696203232, 'learning_rate': 2.9468334636434713e-06, 'epoch': 8.52843462966582}\nEpoch: 8.53234316982607, Logs: {'loss': 0.0395, 'grad_norm': 0.1062927171587944, 'learning_rate': 2.939014855355747e-06, 'epoch': 8.53234316982607}\nEpoch: 8.53625170998632, Logs: {'loss': 0.0412, 'grad_norm': 0.23481491208076477, 'learning_rate': 2.9311962470680223e-06, 'epoch': 8.53625170998632}\nEpoch: 8.54016025014657, Logs: {'loss': 0.0451, 'grad_norm': 0.1793864220380783, 'learning_rate': 2.9233776387802976e-06, 'epoch': 8.54016025014657}\nEpoch: 8.54406879030682, Logs: {'loss': 0.0459, 'grad_norm': 0.2770223915576935, 'learning_rate': 2.9155590304925724e-06, 'epoch': 8.54406879030682}\nEpoch: 8.54797733046707, Logs: {'loss': 0.04, 'grad_norm': 0.14965753257274628, 'learning_rate': 2.9077404222048477e-06, 'epoch': 8.54797733046707}\nEpoch: 8.55188587062732, Logs: {'loss': 0.0385, 'grad_norm': 0.12861748039722443, 'learning_rate': 2.899921813917123e-06, 'epoch': 8.55188587062732}\nEpoch: 8.555794410787572, Logs: {'loss': 0.0439, 'grad_norm': 0.12999606132507324, 'learning_rate': 2.892103205629398e-06, 'epoch': 8.555794410787572}\nEpoch: 8.559702950947822, Logs: {'loss': 0.0369, 'grad_norm': 0.1825937032699585, 'learning_rate': 2.884284597341673e-06, 'epoch': 8.559702950947822}\nEpoch: 8.563611491108071, Logs: {'loss': 0.0388, 'grad_norm': 0.4514830708503723, 'learning_rate': 2.876465989053949e-06, 'epoch': 8.563611491108071}\nEpoch: 8.567520031268321, Logs: {'loss': 0.0397, 'grad_norm': 0.27184396982192993, 'learning_rate': 2.868647380766224e-06, 'epoch': 8.567520031268321}\nEpoch: 8.571428571428571, Logs: {'loss': 0.0451, 'grad_norm': 0.1392529159784317, 'learning_rate': 2.860828772478499e-06, 'epoch': 8.571428571428571}\nEpoch: 8.575337111588821, Logs: {'loss': 0.0432, 'grad_norm': 0.20389331877231598, 'learning_rate': 2.8530101641907743e-06, 'epoch': 8.575337111588821}\nEpoch: 8.579245651749071, Logs: {'loss': 0.0413, 'grad_norm': 0.17527508735656738, 'learning_rate': 2.8451915559030496e-06, 'epoch': 8.579245651749071}\nEpoch: 8.583154191909323, Logs: {'loss': 0.0418, 'grad_norm': 0.43079566955566406, 'learning_rate': 2.8373729476153245e-06, 'epoch': 8.583154191909323}\nEpoch: 8.587062732069572, Logs: {'loss': 0.0391, 'grad_norm': 0.3469715714454651, 'learning_rate': 2.8295543393275998e-06, 'epoch': 8.587062732069572}\nEpoch: 8.590971272229822, Logs: {'loss': 0.0385, 'grad_norm': 0.17024947702884674, 'learning_rate': 2.821735731039875e-06, 'epoch': 8.590971272229822}\nEpoch: 8.594879812390072, Logs: {'loss': 0.0414, 'grad_norm': 0.11944694817066193, 'learning_rate': 2.8139171227521504e-06, 'epoch': 8.594879812390072}\nEpoch: 8.598788352550322, Logs: {'loss': 0.0427, 'grad_norm': 0.1418938785791397, 'learning_rate': 2.8060985144644252e-06, 'epoch': 8.598788352550322}\nEpoch: 8.602696892710572, Logs: {'loss': 0.044, 'grad_norm': 0.10492260009050369, 'learning_rate': 2.798279906176701e-06, 'epoch': 8.602696892710572}\nEpoch: 8.606605432870822, Logs: {'loss': 0.043, 'grad_norm': 0.18836568295955658, 'learning_rate': 2.7904612978889762e-06, 'epoch': 8.606605432870822}\nEpoch: 8.610513973031074, Logs: {'loss': 0.0404, 'grad_norm': 0.09718725830316544, 'learning_rate': 2.782642689601251e-06, 'epoch': 8.610513973031074}\nEpoch: 8.614422513191323, Logs: {'loss': 0.0417, 'grad_norm': 0.23506829142570496, 'learning_rate': 2.7748240813135264e-06, 'epoch': 8.614422513191323}\nEpoch: 8.618331053351573, Logs: {'loss': 0.0401, 'grad_norm': 0.08047676086425781, 'learning_rate': 2.7670054730258017e-06, 'epoch': 8.618331053351573}\nEpoch: 8.622239593511823, Logs: {'loss': 0.0421, 'grad_norm': 0.09146471321582794, 'learning_rate': 2.759186864738077e-06, 'epoch': 8.622239593511823}\nEpoch: 8.626148133672073, Logs: {'loss': 0.0385, 'grad_norm': 0.24716722965240479, 'learning_rate': 2.751368256450352e-06, 'epoch': 8.626148133672073}\nEpoch: 8.630056673832323, Logs: {'loss': 0.0415, 'grad_norm': 0.36065223813056946, 'learning_rate': 2.743549648162627e-06, 'epoch': 8.630056673832323}\nEpoch: 8.633965213992575, Logs: {'loss': 0.04, 'grad_norm': 0.3968515992164612, 'learning_rate': 2.735731039874903e-06, 'epoch': 8.633965213992575}\nEpoch: 8.637873754152825, Logs: {'loss': 0.0383, 'grad_norm': 0.2075035274028778, 'learning_rate': 2.7279124315871773e-06, 'epoch': 8.637873754152825}\nEpoch: 8.641782294313074, Logs: {'loss': 0.0413, 'grad_norm': 0.6961410045623779, 'learning_rate': 2.720093823299453e-06, 'epoch': 8.641782294313074}\nEpoch: 8.645690834473324, Logs: {'loss': 0.0424, 'grad_norm': 0.0981658324599266, 'learning_rate': 2.7122752150117283e-06, 'epoch': 8.645690834473324}\nEpoch: 8.649599374633574, Logs: {'loss': 0.0407, 'grad_norm': 0.17247648537158966, 'learning_rate': 2.7044566067240036e-06, 'epoch': 8.649599374633574}\nEpoch: 8.653507914793824, Logs: {'loss': 0.0407, 'grad_norm': 0.14524951577186584, 'learning_rate': 2.6966379984362784e-06, 'epoch': 8.653507914793824}\nEpoch: 8.657416454954074, Logs: {'loss': 0.0421, 'grad_norm': 0.20260052382946014, 'learning_rate': 2.6888193901485537e-06, 'epoch': 8.657416454954074}\nEpoch: 8.661324995114326, Logs: {'loss': 0.0406, 'grad_norm': 0.15290692448616028, 'learning_rate': 2.681000781860829e-06, 'epoch': 8.661324995114326}\nEpoch: 8.665233535274576, Logs: {'loss': 0.0431, 'grad_norm': 0.27269500494003296, 'learning_rate': 2.673182173573104e-06, 'epoch': 8.665233535274576}\nEpoch: 8.669142075434825, Logs: {'loss': 0.0358, 'grad_norm': 0.16570307314395905, 'learning_rate': 2.665363565285379e-06, 'epoch': 8.669142075434825}\nEpoch: 8.673050615595075, Logs: {'loss': 0.0389, 'grad_norm': 0.24410556256771088, 'learning_rate': 2.657544956997655e-06, 'epoch': 8.673050615595075}\nEpoch: 8.676959155755325, Logs: {'loss': 0.0424, 'grad_norm': 0.11773707717657089, 'learning_rate': 2.64972634870993e-06, 'epoch': 8.676959155755325}\nEpoch: 8.680867695915575, Logs: {'loss': 0.0415, 'grad_norm': 0.13096380233764648, 'learning_rate': 2.641907740422205e-06, 'epoch': 8.680867695915575}\nEpoch: 8.684776236075825, Logs: {'loss': 0.0375, 'grad_norm': 0.4320257902145386, 'learning_rate': 2.6340891321344803e-06, 'epoch': 8.684776236075825}\nEpoch: 8.688684776236077, Logs: {'loss': 0.0441, 'grad_norm': 0.11343879997730255, 'learning_rate': 2.6262705238467556e-06, 'epoch': 8.688684776236077}\nEpoch: 8.692593316396326, Logs: {'loss': 0.0425, 'grad_norm': 0.08731590956449509, 'learning_rate': 2.6184519155590305e-06, 'epoch': 8.692593316396326}\nEpoch: 8.696501856556576, Logs: {'loss': 0.0446, 'grad_norm': 0.15453383326530457, 'learning_rate': 2.6106333072713058e-06, 'epoch': 8.696501856556576}\nEpoch: 8.700410396716826, Logs: {'loss': 0.0407, 'grad_norm': 0.12387669831514359, 'learning_rate': 2.602814698983581e-06, 'epoch': 8.700410396716826}\nEpoch: 8.704318936877076, Logs: {'loss': 0.0401, 'grad_norm': 0.1502772867679596, 'learning_rate': 2.5949960906958568e-06, 'epoch': 8.704318936877076}\nEpoch: 8.708227477037326, Logs: {'loss': 0.0432, 'grad_norm': 0.13217854499816895, 'learning_rate': 2.5871774824081312e-06, 'epoch': 8.708227477037326}\nEpoch: 8.712136017197576, Logs: {'loss': 0.0419, 'grad_norm': 0.1685250699520111, 'learning_rate': 2.579358874120407e-06, 'epoch': 8.712136017197576}\nEpoch: 8.716044557357828, Logs: {'loss': 0.0414, 'grad_norm': 0.5207962989807129, 'learning_rate': 2.5715402658326822e-06, 'epoch': 8.716044557357828}\nEpoch: 8.719953097518077, Logs: {'loss': 0.0413, 'grad_norm': 0.14674262702465057, 'learning_rate': 2.563721657544957e-06, 'epoch': 8.719953097518077}\nEpoch: 8.723861637678327, Logs: {'loss': 0.0411, 'grad_norm': 0.10212064534425735, 'learning_rate': 2.5559030492572324e-06, 'epoch': 8.723861637678327}\nEpoch: 8.727770177838577, Logs: {'loss': 0.0415, 'grad_norm': 0.11166773736476898, 'learning_rate': 2.5480844409695077e-06, 'epoch': 8.727770177838577}\nEpoch: 8.731678717998827, Logs: {'loss': 0.0379, 'grad_norm': 0.16547442972660065, 'learning_rate': 2.540265832681783e-06, 'epoch': 8.731678717998827}\nEpoch: 8.735587258159077, Logs: {'loss': 0.0392, 'grad_norm': 0.21635030210018158, 'learning_rate': 2.532447224394058e-06, 'epoch': 8.735587258159077}\nEpoch: 8.739495798319329, Logs: {'loss': 0.0502, 'grad_norm': 0.39385613799095154, 'learning_rate': 2.524628616106333e-06, 'epoch': 8.739495798319329}\nEpoch: 8.743404338479579, Logs: {'loss': 0.0391, 'grad_norm': 0.165917307138443, 'learning_rate': 2.516810007818609e-06, 'epoch': 8.743404338479579}\nEpoch: 8.747312878639828, Logs: {'loss': 0.0469, 'grad_norm': 0.10614524036645889, 'learning_rate': 2.5089913995308833e-06, 'epoch': 8.747312878639828}\nEpoch: 8.751221418800078, Logs: {'loss': 0.0478, 'grad_norm': 0.18641780316829681, 'learning_rate': 2.501172791243159e-06, 'epoch': 8.751221418800078}\nEpoch: 8.755129958960328, Logs: {'loss': 0.0389, 'grad_norm': 0.2536960244178772, 'learning_rate': 2.4933541829554343e-06, 'epoch': 8.755129958960328}\nEpoch: 8.759038499120578, Logs: {'loss': 0.0377, 'grad_norm': 0.32477888464927673, 'learning_rate': 2.485535574667709e-06, 'epoch': 8.759038499120578}\nEpoch: 8.762947039280828, Logs: {'loss': 0.0443, 'grad_norm': 0.144021138548851, 'learning_rate': 2.477716966379985e-06, 'epoch': 8.762947039280828}\nEpoch: 8.766855579441078, Logs: {'loss': 0.0438, 'grad_norm': 0.17078828811645508, 'learning_rate': 2.4698983580922597e-06, 'epoch': 8.766855579441078}\nEpoch: 8.77076411960133, Logs: {'loss': 0.0379, 'grad_norm': 0.10283472388982773, 'learning_rate': 2.462079749804535e-06, 'epoch': 8.77076411960133}\nEpoch: 8.77467265976158, Logs: {'loss': 0.0395, 'grad_norm': 0.24918517470359802, 'learning_rate': 2.4542611415168103e-06, 'epoch': 8.77467265976158}\nEpoch: 8.77858119992183, Logs: {'loss': 0.0468, 'grad_norm': 0.10800996422767639, 'learning_rate': 2.446442533229085e-06, 'epoch': 8.77858119992183}\nEpoch: 8.78248974008208, Logs: {'loss': 0.0437, 'grad_norm': 0.09302501380443573, 'learning_rate': 2.438623924941361e-06, 'epoch': 8.78248974008208}\nEpoch: 8.786398280242329, Logs: {'loss': 0.04, 'grad_norm': 0.12135917693376541, 'learning_rate': 2.4308053166536357e-06, 'epoch': 8.786398280242329}\nEpoch: 8.790306820402579, Logs: {'loss': 0.0384, 'grad_norm': 0.08146781474351883, 'learning_rate': 2.422986708365911e-06, 'epoch': 8.790306820402579}\nEpoch: 8.79421536056283, Logs: {'loss': 0.0429, 'grad_norm': 0.10973597317934036, 'learning_rate': 2.4151681000781863e-06, 'epoch': 8.79421536056283}\nEpoch: 8.79812390072308, Logs: {'loss': 0.0394, 'grad_norm': 0.11016485840082169, 'learning_rate': 2.4073494917904616e-06, 'epoch': 8.79812390072308}\nEpoch: 8.80203244088333, Logs: {'loss': 0.0447, 'grad_norm': 0.2596417963504791, 'learning_rate': 2.399530883502737e-06, 'epoch': 8.80203244088333}\nEpoch: 8.80594098104358, Logs: {'loss': 0.04, 'grad_norm': 0.1319633275270462, 'learning_rate': 2.3917122752150118e-06, 'epoch': 8.80594098104358}\nEpoch: 8.80984952120383, Logs: {'loss': 0.0385, 'grad_norm': 0.2489042431116104, 'learning_rate': 2.383893666927287e-06, 'epoch': 8.80984952120383}\nEpoch: 8.81375806136408, Logs: {'loss': 0.0385, 'grad_norm': 0.1679404228925705, 'learning_rate': 2.3760750586395623e-06, 'epoch': 8.81375806136408}\nEpoch: 8.81766660152433, Logs: {'loss': 0.0401, 'grad_norm': 0.11258333176374435, 'learning_rate': 2.3682564503518376e-06, 'epoch': 8.81766660152433}\nEpoch: 8.821575141684582, Logs: {'loss': 0.0421, 'grad_norm': 0.16886961460113525, 'learning_rate': 2.360437842064113e-06, 'epoch': 8.821575141684582}\nEpoch: 8.825483681844831, Logs: {'loss': 0.0378, 'grad_norm': 0.14357028901576996, 'learning_rate': 2.352619233776388e-06, 'epoch': 8.825483681844831}\nEpoch: 8.829392222005081, Logs: {'loss': 0.0428, 'grad_norm': 0.6863136887550354, 'learning_rate': 2.344800625488663e-06, 'epoch': 8.829392222005081}\nEpoch: 8.833300762165331, Logs: {'loss': 0.0383, 'grad_norm': 0.1191687062382698, 'learning_rate': 2.3369820172009384e-06, 'epoch': 8.833300762165331}\nEpoch: 8.837209302325581, Logs: {'loss': 0.0426, 'grad_norm': 0.1696523129940033, 'learning_rate': 2.3291634089132137e-06, 'epoch': 8.837209302325581}\nEpoch: 8.841117842485831, Logs: {'loss': 0.0355, 'grad_norm': 0.18941627442836761, 'learning_rate': 2.321344800625489e-06, 'epoch': 8.841117842485831}\nEpoch: 8.84502638264608, Logs: {'loss': 0.0424, 'grad_norm': 0.11197983473539352, 'learning_rate': 2.3135261923377642e-06, 'epoch': 8.84502638264608}\nEpoch: 8.848934922806333, Logs: {'loss': 0.0426, 'grad_norm': 0.1488870531320572, 'learning_rate': 2.305707584050039e-06, 'epoch': 8.848934922806333}\nEpoch: 8.852843462966582, Logs: {'loss': 0.0425, 'grad_norm': 0.470033198595047, 'learning_rate': 2.297888975762315e-06, 'epoch': 8.852843462966582}\nEpoch: 8.856752003126832, Logs: {'loss': 0.0394, 'grad_norm': 0.3005056381225586, 'learning_rate': 2.2900703674745897e-06, 'epoch': 8.856752003126832}\nEpoch: 8.860660543287082, Logs: {'loss': 0.0462, 'grad_norm': 0.15015889704227448, 'learning_rate': 2.282251759186865e-06, 'epoch': 8.860660543287082}\nEpoch: 8.864569083447332, Logs: {'loss': 0.0433, 'grad_norm': 0.4029299020767212, 'learning_rate': 2.2744331508991403e-06, 'epoch': 8.864569083447332}\nEpoch: 8.868477623607582, Logs: {'loss': 0.0461, 'grad_norm': 0.10381042957305908, 'learning_rate': 2.266614542611415e-06, 'epoch': 8.868477623607582}\nEpoch: 8.872386163767832, Logs: {'loss': 0.0411, 'grad_norm': 0.15707150101661682, 'learning_rate': 2.258795934323691e-06, 'epoch': 8.872386163767832}\nEpoch: 8.876294703928084, Logs: {'loss': 0.0387, 'grad_norm': 0.06843149662017822, 'learning_rate': 2.2509773260359657e-06, 'epoch': 8.876294703928084}\nEpoch: 8.880203244088333, Logs: {'loss': 0.0417, 'grad_norm': 0.26539549231529236, 'learning_rate': 2.243158717748241e-06, 'epoch': 8.880203244088333}\nEpoch: 8.884111784248583, Logs: {'loss': 0.0416, 'grad_norm': 0.10716341435909271, 'learning_rate': 2.2353401094605163e-06, 'epoch': 8.884111784248583}\nEpoch: 8.888020324408833, Logs: {'loss': 0.0413, 'grad_norm': 0.11833556741476059, 'learning_rate': 2.227521501172791e-06, 'epoch': 8.888020324408833}\nEpoch: 8.891928864569083, Logs: {'loss': 0.0425, 'grad_norm': 0.14592309296131134, 'learning_rate': 2.219702892885067e-06, 'epoch': 8.891928864569083}\nEpoch: 8.895837404729333, Logs: {'loss': 0.0378, 'grad_norm': 0.14392511546611786, 'learning_rate': 2.2118842845973417e-06, 'epoch': 8.895837404729333}\nEpoch: 8.899745944889585, Logs: {'loss': 0.0439, 'grad_norm': 0.14654923975467682, 'learning_rate': 2.204065676309617e-06, 'epoch': 8.899745944889585}\nEpoch: 8.903654485049834, Logs: {'loss': 0.0439, 'grad_norm': 0.24904905259609222, 'learning_rate': 2.1962470680218923e-06, 'epoch': 8.903654485049834}\nEpoch: 8.907563025210084, Logs: {'loss': 0.0423, 'grad_norm': 0.08270679414272308, 'learning_rate': 2.1884284597341676e-06, 'epoch': 8.907563025210084}\nEpoch: 8.911471565370334, Logs: {'loss': 0.0434, 'grad_norm': 0.25369828939437866, 'learning_rate': 2.180609851446443e-06, 'epoch': 8.911471565370334}\nEpoch: 8.915380105530584, Logs: {'loss': 0.0361, 'grad_norm': 0.24780061841011047, 'learning_rate': 2.1727912431587177e-06, 'epoch': 8.915380105530584}\nEpoch: 8.919288645690834, Logs: {'loss': 0.041, 'grad_norm': 0.08330610394477844, 'learning_rate': 2.164972634870993e-06, 'epoch': 8.919288645690834}\nEpoch: 8.923197185851084, Logs: {'loss': 0.0385, 'grad_norm': 0.16602754592895508, 'learning_rate': 2.1571540265832683e-06, 'epoch': 8.923197185851084}\nEpoch: 8.927105726011336, Logs: {'loss': 0.0406, 'grad_norm': 0.1318197399377823, 'learning_rate': 2.1493354182955436e-06, 'epoch': 8.927105726011336}\nEpoch: 8.931014266171585, Logs: {'loss': 0.0397, 'grad_norm': 0.16493427753448486, 'learning_rate': 2.141516810007819e-06, 'epoch': 8.931014266171585}\nEpoch: 8.934922806331835, Logs: {'loss': 0.0455, 'grad_norm': 0.15380533039569855, 'learning_rate': 2.133698201720094e-06, 'epoch': 8.934922806331835}\nEpoch: 8.938831346492085, Logs: {'loss': 0.0399, 'grad_norm': 0.19816645979881287, 'learning_rate': 2.125879593432369e-06, 'epoch': 8.938831346492085}\nEpoch: 8.942739886652335, Logs: {'loss': 0.0409, 'grad_norm': 0.0671297088265419, 'learning_rate': 2.1180609851446444e-06, 'epoch': 8.942739886652335}\nEpoch: 8.946648426812585, Logs: {'loss': 0.0374, 'grad_norm': 0.13309712707996368, 'learning_rate': 2.1102423768569196e-06, 'epoch': 8.946648426812585}\nEpoch: 8.950556966972835, Logs: {'loss': 0.0433, 'grad_norm': 0.11039084941148758, 'learning_rate': 2.102423768569195e-06, 'epoch': 8.950556966972835}\nEpoch: 8.954465507133087, Logs: {'loss': 0.0397, 'grad_norm': 0.11170820891857147, 'learning_rate': 2.0946051602814702e-06, 'epoch': 8.954465507133087}\nEpoch: 8.958374047293336, Logs: {'loss': 0.0421, 'grad_norm': 0.09234434366226196, 'learning_rate': 2.086786551993745e-06, 'epoch': 8.958374047293336}\nEpoch: 8.962282587453586, Logs: {'loss': 0.0406, 'grad_norm': 0.39961761236190796, 'learning_rate': 2.078967943706021e-06, 'epoch': 8.962282587453586}\nEpoch: 8.966191127613836, Logs: {'loss': 0.0367, 'grad_norm': 0.07559279352426529, 'learning_rate': 2.0711493354182957e-06, 'epoch': 8.966191127613836}\nEpoch: 8.970099667774086, Logs: {'loss': 0.0412, 'grad_norm': 0.16143833100795746, 'learning_rate': 2.063330727130571e-06, 'epoch': 8.970099667774086}\nEpoch: 8.974008207934336, Logs: {'loss': 0.0427, 'grad_norm': 0.12742459774017334, 'learning_rate': 2.0555121188428462e-06, 'epoch': 8.974008207934336}\nEpoch: 8.977916748094586, Logs: {'loss': 0.0427, 'grad_norm': 0.07595780491828918, 'learning_rate': 2.047693510555121e-06, 'epoch': 8.977916748094586}\nEpoch: 8.981825288254838, Logs: {'loss': 0.0407, 'grad_norm': 0.10154318809509277, 'learning_rate': 2.039874902267397e-06, 'epoch': 8.981825288254838}\nEpoch: 8.985733828415087, Logs: {'loss': 0.0421, 'grad_norm': 0.11501990258693695, 'learning_rate': 2.0320562939796717e-06, 'epoch': 8.985733828415087}\nEpoch: 8.989642368575337, Logs: {'loss': 0.0379, 'grad_norm': 0.11336640268564224, 'learning_rate': 2.024237685691947e-06, 'epoch': 8.989642368575337}\nEpoch: 8.993550908735587, Logs: {'loss': 0.0389, 'grad_norm': 0.11901679635047913, 'learning_rate': 2.0164190774042223e-06, 'epoch': 8.993550908735587}\nEpoch: 8.997459448895837, Logs: {'loss': 0.0417, 'grad_norm': 0.19366073608398438, 'learning_rate': 2.008600469116497e-06, 'epoch': 8.997459448895837}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8.999804572991987, Logs: {'eval_loss': 0.0411706306040287, 'eval_bleu': 0.0, 'eval_accuracy': 0.9456118060985145, 'eval_runtime': 348.4833, 'eval_samples_per_second': 58.723, 'eval_steps_per_second': 3.67, 'epoch': 8.999804572991987}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9.001367989056087, Logs: {'loss': 0.036, 'grad_norm': 0.08481262624263763, 'learning_rate': 2.001563721657545e-06, 'epoch': 9.001367989056087}\nEpoch: 9.005276529216337, Logs: {'loss': 0.0417, 'grad_norm': 0.10203515738248825, 'learning_rate': 1.9937451133698206e-06, 'epoch': 9.005276529216337}\nEpoch: 9.009185069376588, Logs: {'loss': 0.0394, 'grad_norm': 0.1734808087348938, 'learning_rate': 1.9859265050820955e-06, 'epoch': 9.009185069376588}\nEpoch: 9.013093609536838, Logs: {'loss': 0.0424, 'grad_norm': 0.3041340708732605, 'learning_rate': 1.978107896794371e-06, 'epoch': 9.013093609536838}\nEpoch: 9.017002149697088, Logs: {'loss': 0.0344, 'grad_norm': 0.17183080315589905, 'learning_rate': 1.970289288506646e-06, 'epoch': 9.017002149697088}\nEpoch: 9.020910689857338, Logs: {'loss': 0.0402, 'grad_norm': 0.19109117984771729, 'learning_rate': 1.962470680218921e-06, 'epoch': 9.020910689857338}\nEpoch: 9.024819230017588, Logs: {'loss': 0.0369, 'grad_norm': 0.3314269483089447, 'learning_rate': 1.9546520719311967e-06, 'epoch': 9.024819230017588}\nEpoch: 9.028727770177838, Logs: {'loss': 0.0351, 'grad_norm': 0.25283685326576233, 'learning_rate': 1.9468334636434715e-06, 'epoch': 9.028727770177838}\nEpoch: 9.03263631033809, Logs: {'loss': 0.0441, 'grad_norm': 0.2724628448486328, 'learning_rate': 1.939014855355747e-06, 'epoch': 9.03263631033809}\nEpoch: 9.03654485049834, Logs: {'loss': 0.0376, 'grad_norm': 0.20603759586811066, 'learning_rate': 1.931196247068022e-06, 'epoch': 9.03654485049834}\nEpoch: 9.04045339065859, Logs: {'loss': 0.0436, 'grad_norm': 0.14066734910011292, 'learning_rate': 1.923377638780297e-06, 'epoch': 9.04045339065859}\nEpoch: 9.04436193081884, Logs: {'loss': 0.0403, 'grad_norm': 0.08555760234594345, 'learning_rate': 1.9155590304925727e-06, 'epoch': 9.04436193081884}\nEpoch: 9.048270470979089, Logs: {'loss': 0.0385, 'grad_norm': 0.08533608913421631, 'learning_rate': 1.9077404222048476e-06, 'epoch': 9.048270470979089}\nEpoch: 9.052179011139339, Logs: {'loss': 0.0413, 'grad_norm': 0.12708644568920135, 'learning_rate': 1.899921813917123e-06, 'epoch': 9.052179011139339}\nEpoch: 9.056087551299589, Logs: {'loss': 0.041, 'grad_norm': 0.12955807149410248, 'learning_rate': 1.8921032056293981e-06, 'epoch': 9.056087551299589}\nEpoch: 9.05999609145984, Logs: {'loss': 0.0379, 'grad_norm': 0.09964728355407715, 'learning_rate': 1.8842845973416734e-06, 'epoch': 9.05999609145984}\nEpoch: 9.06390463162009, Logs: {'loss': 0.0367, 'grad_norm': 0.1463032215833664, 'learning_rate': 1.8764659890539485e-06, 'epoch': 9.06390463162009}\nEpoch: 9.06781317178034, Logs: {'loss': 0.043, 'grad_norm': 0.2511753737926483, 'learning_rate': 1.8686473807662236e-06, 'epoch': 9.06781317178034}\nEpoch: 9.07172171194059, Logs: {'loss': 0.0395, 'grad_norm': 0.1437234878540039, 'learning_rate': 1.860828772478499e-06, 'epoch': 9.07172171194059}\nEpoch: 9.07563025210084, Logs: {'loss': 0.042, 'grad_norm': 0.19349004328250885, 'learning_rate': 1.8530101641907742e-06, 'epoch': 9.07563025210084}\nEpoch: 9.07953879226109, Logs: {'loss': 0.0409, 'grad_norm': 0.14902374148368835, 'learning_rate': 1.8451915559030495e-06, 'epoch': 9.07953879226109}\nEpoch: 9.08344733242134, Logs: {'loss': 0.043, 'grad_norm': 0.41499197483062744, 'learning_rate': 1.8373729476153245e-06, 'epoch': 9.08344733242134}\nEpoch: 9.087355872581591, Logs: {'loss': 0.0404, 'grad_norm': 0.10097990185022354, 'learning_rate': 1.8295543393276e-06, 'epoch': 9.087355872581591}\nEpoch: 9.091264412741841, Logs: {'loss': 0.04, 'grad_norm': 0.13421791791915894, 'learning_rate': 1.8217357310398751e-06, 'epoch': 9.091264412741841}\nEpoch: 9.095172952902091, Logs: {'loss': 0.037, 'grad_norm': 0.12662261724472046, 'learning_rate': 1.8139171227521502e-06, 'epoch': 9.095172952902091}\nEpoch: 9.099081493062341, Logs: {'loss': 0.0422, 'grad_norm': 0.3536645472049713, 'learning_rate': 1.8060985144644255e-06, 'epoch': 9.099081493062341}\nEpoch: 9.102990033222591, Logs: {'loss': 0.041, 'grad_norm': 0.21151591837406158, 'learning_rate': 1.7982799061767006e-06, 'epoch': 9.102990033222591}\nEpoch: 9.106898573382841, Logs: {'loss': 0.048, 'grad_norm': 0.06922013312578201, 'learning_rate': 1.790461297888976e-06, 'epoch': 9.106898573382841}\nEpoch: 9.11080711354309, Logs: {'loss': 0.0399, 'grad_norm': 0.13466788828372955, 'learning_rate': 1.7826426896012511e-06, 'epoch': 9.11080711354309}\nEpoch: 9.114715653703342, Logs: {'loss': 0.0369, 'grad_norm': 0.09031250327825546, 'learning_rate': 1.7748240813135264e-06, 'epoch': 9.114715653703342}\nEpoch: 9.118624193863592, Logs: {'loss': 0.0377, 'grad_norm': 0.11670896410942078, 'learning_rate': 1.7670054730258015e-06, 'epoch': 9.118624193863592}\nEpoch: 9.122532734023842, Logs: {'loss': 0.0394, 'grad_norm': 0.192318394780159, 'learning_rate': 1.7591868647380766e-06, 'epoch': 9.122532734023842}\nEpoch: 9.126441274184092, Logs: {'loss': 0.0382, 'grad_norm': 0.09337303042411804, 'learning_rate': 1.751368256450352e-06, 'epoch': 9.126441274184092}\nEpoch: 9.130349814344342, Logs: {'loss': 0.0371, 'grad_norm': 0.2527707815170288, 'learning_rate': 1.7435496481626272e-06, 'epoch': 9.130349814344342}\nEpoch: 9.134258354504592, Logs: {'loss': 0.0452, 'grad_norm': 0.0813353881239891, 'learning_rate': 1.7357310398749025e-06, 'epoch': 9.134258354504592}\nEpoch: 9.138166894664844, Logs: {'loss': 0.0362, 'grad_norm': 0.09055149555206299, 'learning_rate': 1.7279124315871775e-06, 'epoch': 9.138166894664844}\nEpoch: 9.142075434825093, Logs: {'loss': 0.046, 'grad_norm': 0.18196408450603485, 'learning_rate': 1.720093823299453e-06, 'epoch': 9.142075434825093}\nEpoch: 9.145983974985343, Logs: {'loss': 0.0407, 'grad_norm': 0.09816481918096542, 'learning_rate': 1.7122752150117281e-06, 'epoch': 9.145983974985343}\nEpoch: 9.149892515145593, Logs: {'loss': 0.0404, 'grad_norm': 0.11602263152599335, 'learning_rate': 1.7044566067240032e-06, 'epoch': 9.149892515145593}\nEpoch: 9.153801055305843, Logs: {'loss': 0.0391, 'grad_norm': 0.12040535360574722, 'learning_rate': 1.6966379984362785e-06, 'epoch': 9.153801055305843}\nEpoch: 9.157709595466093, Logs: {'loss': 0.0461, 'grad_norm': 0.4032939076423645, 'learning_rate': 1.6888193901485536e-06, 'epoch': 9.157709595466093}\nEpoch: 9.161618135626343, Logs: {'loss': 0.0422, 'grad_norm': 0.18863044679164886, 'learning_rate': 1.681000781860829e-06, 'epoch': 9.161618135626343}\nEpoch: 9.165526675786595, Logs: {'loss': 0.0416, 'grad_norm': 0.1663089245557785, 'learning_rate': 1.6731821735731041e-06, 'epoch': 9.165526675786595}\nEpoch: 9.169435215946844, Logs: {'loss': 0.0423, 'grad_norm': 0.12352742999792099, 'learning_rate': 1.6653635652853794e-06, 'epoch': 9.169435215946844}\nEpoch: 9.173343756107094, Logs: {'loss': 0.0402, 'grad_norm': 0.24608060717582703, 'learning_rate': 1.6575449569976545e-06, 'epoch': 9.173343756107094}\nEpoch: 9.177252296267344, Logs: {'loss': 0.0354, 'grad_norm': 0.11745039373636246, 'learning_rate': 1.6497263487099296e-06, 'epoch': 9.177252296267344}\nEpoch: 9.181160836427594, Logs: {'loss': 0.0414, 'grad_norm': 0.08058642596006393, 'learning_rate': 1.641907740422205e-06, 'epoch': 9.181160836427594}\nEpoch: 9.185069376587844, Logs: {'loss': 0.043, 'grad_norm': 0.11258979886770248, 'learning_rate': 1.6340891321344802e-06, 'epoch': 9.185069376587844}\nEpoch: 9.188977916748094, Logs: {'loss': 0.0442, 'grad_norm': 0.3662450611591339, 'learning_rate': 1.6262705238467554e-06, 'epoch': 9.188977916748094}\nEpoch: 9.192886456908345, Logs: {'loss': 0.0457, 'grad_norm': 0.25798630714416504, 'learning_rate': 1.6184519155590305e-06, 'epoch': 9.192886456908345}\nEpoch: 9.196794997068595, Logs: {'loss': 0.0442, 'grad_norm': 0.17850889265537262, 'learning_rate': 1.610633307271306e-06, 'epoch': 9.196794997068595}\nEpoch: 9.200703537228845, Logs: {'loss': 0.0441, 'grad_norm': 0.1143888533115387, 'learning_rate': 1.602814698983581e-06, 'epoch': 9.200703537228845}\nEpoch: 9.204612077389095, Logs: {'loss': 0.0491, 'grad_norm': 0.13650654256343842, 'learning_rate': 1.5949960906958562e-06, 'epoch': 9.204612077389095}\nEpoch: 9.208520617549345, Logs: {'loss': 0.0415, 'grad_norm': 0.6131267547607422, 'learning_rate': 1.5871774824081315e-06, 'epoch': 9.208520617549345}\nEpoch: 9.212429157709595, Logs: {'loss': 0.0376, 'grad_norm': 0.33323630690574646, 'learning_rate': 1.5793588741204065e-06, 'epoch': 9.212429157709595}\nEpoch: 9.216337697869845, Logs: {'loss': 0.0414, 'grad_norm': 0.12378176301717758, 'learning_rate': 1.571540265832682e-06, 'epoch': 9.216337697869845}\nEpoch: 9.220246238030096, Logs: {'loss': 0.0407, 'grad_norm': 0.24164573848247528, 'learning_rate': 1.5637216575449571e-06, 'epoch': 9.220246238030096}\nEpoch: 9.224154778190346, Logs: {'loss': 0.0389, 'grad_norm': 0.2753515839576721, 'learning_rate': 1.5559030492572324e-06, 'epoch': 9.224154778190346}\nEpoch: 9.228063318350596, Logs: {'loss': 0.0412, 'grad_norm': 0.13453827798366547, 'learning_rate': 1.5480844409695075e-06, 'epoch': 9.228063318350596}\nEpoch: 9.231971858510846, Logs: {'loss': 0.0406, 'grad_norm': 0.10107410699129105, 'learning_rate': 1.5402658326817826e-06, 'epoch': 9.231971858510846}\nEpoch: 9.235880398671096, Logs: {'loss': 0.0377, 'grad_norm': 0.10276593267917633, 'learning_rate': 1.532447224394058e-06, 'epoch': 9.235880398671096}\nEpoch: 9.239788938831346, Logs: {'loss': 0.0453, 'grad_norm': 0.07598070800304413, 'learning_rate': 1.5246286161063332e-06, 'epoch': 9.239788938831346}\nEpoch: 9.243697478991596, Logs: {'loss': 0.0404, 'grad_norm': 0.15095628798007965, 'learning_rate': 1.5168100078186084e-06, 'epoch': 9.243697478991596}\nEpoch: 9.247606019151847, Logs: {'loss': 0.0424, 'grad_norm': 0.1269848346710205, 'learning_rate': 1.5089913995308835e-06, 'epoch': 9.247606019151847}\nEpoch: 9.251514559312097, Logs: {'loss': 0.0389, 'grad_norm': 0.09534341841936111, 'learning_rate': 1.501172791243159e-06, 'epoch': 9.251514559312097}\nEpoch: 9.255423099472347, Logs: {'loss': 0.041, 'grad_norm': 0.2525416612625122, 'learning_rate': 1.493354182955434e-06, 'epoch': 9.255423099472347}\nEpoch: 9.259331639632597, Logs: {'loss': 0.0364, 'grad_norm': 0.06668722629547119, 'learning_rate': 1.4855355746677092e-06, 'epoch': 9.259331639632597}\nEpoch: 9.263240179792847, Logs: {'loss': 0.0426, 'grad_norm': 0.09999272227287292, 'learning_rate': 1.4777169663799845e-06, 'epoch': 9.263240179792847}\nEpoch: 9.267148719953097, Logs: {'loss': 0.0461, 'grad_norm': 0.14498260617256165, 'learning_rate': 1.4698983580922595e-06, 'epoch': 9.267148719953097}\nEpoch: 9.271057260113349, Logs: {'loss': 0.0487, 'grad_norm': 0.18414807319641113, 'learning_rate': 1.462079749804535e-06, 'epoch': 9.271057260113349}\nEpoch: 9.274965800273598, Logs: {'loss': 0.0406, 'grad_norm': 0.24953456223011017, 'learning_rate': 1.4542611415168101e-06, 'epoch': 9.274965800273598}\nEpoch: 9.278874340433848, Logs: {'loss': 0.0336, 'grad_norm': 0.15296000242233276, 'learning_rate': 1.4464425332290854e-06, 'epoch': 9.278874340433848}\nEpoch: 9.282782880594098, Logs: {'loss': 0.0428, 'grad_norm': 0.21165113151073456, 'learning_rate': 1.4386239249413605e-06, 'epoch': 9.282782880594098}\nEpoch: 9.286691420754348, Logs: {'loss': 0.0425, 'grad_norm': 0.1399061381816864, 'learning_rate': 1.4308053166536356e-06, 'epoch': 9.286691420754348}\nEpoch: 9.290599960914598, Logs: {'loss': 0.0425, 'grad_norm': 0.08195602148771286, 'learning_rate': 1.422986708365911e-06, 'epoch': 9.290599960914598}\nEpoch: 9.294508501074848, Logs: {'loss': 0.0391, 'grad_norm': 0.09590065479278564, 'learning_rate': 1.4151681000781861e-06, 'epoch': 9.294508501074848}\nEpoch: 9.2984170412351, Logs: {'loss': 0.0455, 'grad_norm': 0.1055416464805603, 'learning_rate': 1.4073494917904614e-06, 'epoch': 9.2984170412351}\nEpoch: 9.30232558139535, Logs: {'loss': 0.0427, 'grad_norm': 0.11280413717031479, 'learning_rate': 1.3995308835027365e-06, 'epoch': 9.30232558139535}\nEpoch: 9.3062341215556, Logs: {'loss': 0.0442, 'grad_norm': 0.8595694899559021, 'learning_rate': 1.391712275215012e-06, 'epoch': 9.3062341215556}\nEpoch: 9.31014266171585, Logs: {'loss': 0.0397, 'grad_norm': 0.09833759814500809, 'learning_rate': 1.383893666927287e-06, 'epoch': 9.31014266171585}\nEpoch: 9.314051201876099, Logs: {'loss': 0.0396, 'grad_norm': 0.5967749953269958, 'learning_rate': 1.3760750586395622e-06, 'epoch': 9.314051201876099}\nEpoch: 9.317959742036349, Logs: {'loss': 0.0464, 'grad_norm': 0.13488715887069702, 'learning_rate': 1.3682564503518375e-06, 'epoch': 9.317959742036349}\nEpoch: 9.321868282196599, Logs: {'loss': 0.0418, 'grad_norm': 0.10666632652282715, 'learning_rate': 1.3604378420641125e-06, 'epoch': 9.321868282196599}\nEpoch: 9.32577682235685, Logs: {'loss': 0.0462, 'grad_norm': 0.11701888591051102, 'learning_rate': 1.352619233776388e-06, 'epoch': 9.32577682235685}\nEpoch: 9.3296853625171, Logs: {'loss': 0.0456, 'grad_norm': 0.11388272047042847, 'learning_rate': 1.3448006254886631e-06, 'epoch': 9.3296853625171}\nEpoch: 9.33359390267735, Logs: {'loss': 0.0414, 'grad_norm': 0.11694999039173126, 'learning_rate': 1.3369820172009384e-06, 'epoch': 9.33359390267735}\nEpoch: 9.3375024428376, Logs: {'loss': 0.0356, 'grad_norm': 0.13261163234710693, 'learning_rate': 1.3291634089132135e-06, 'epoch': 9.3375024428376}\nEpoch: 9.34141098299785, Logs: {'loss': 0.0382, 'grad_norm': 0.14522013068199158, 'learning_rate': 1.3213448006254886e-06, 'epoch': 9.34141098299785}\nEpoch: 9.3453195231581, Logs: {'loss': 0.0439, 'grad_norm': 0.09603451937437057, 'learning_rate': 1.313526192337764e-06, 'epoch': 9.3453195231581}\nEpoch: 9.34922806331835, Logs: {'loss': 0.0433, 'grad_norm': 0.2532796859741211, 'learning_rate': 1.3057075840500391e-06, 'epoch': 9.34922806331835}\nEpoch: 9.353136603478601, Logs: {'loss': 0.0396, 'grad_norm': 0.8187544345855713, 'learning_rate': 1.2978889757623144e-06, 'epoch': 9.353136603478601}\nEpoch: 9.357045143638851, Logs: {'loss': 0.0398, 'grad_norm': 0.12759250402450562, 'learning_rate': 1.2900703674745895e-06, 'epoch': 9.357045143638851}\nEpoch: 9.360953683799101, Logs: {'loss': 0.04, 'grad_norm': 0.12350743263959885, 'learning_rate': 1.282251759186865e-06, 'epoch': 9.360953683799101}\nEpoch: 9.364862223959351, Logs: {'loss': 0.0374, 'grad_norm': 0.10129757970571518, 'learning_rate': 1.27443315089914e-06, 'epoch': 9.364862223959351}\nEpoch: 9.368770764119601, Logs: {'loss': 0.0447, 'grad_norm': 0.25659576058387756, 'learning_rate': 1.2666145426114152e-06, 'epoch': 9.368770764119601}\nEpoch: 9.37267930427985, Logs: {'loss': 0.0419, 'grad_norm': 0.38297125697135925, 'learning_rate': 1.2587959343236905e-06, 'epoch': 9.37267930427985}\nEpoch: 9.376587844440103, Logs: {'loss': 0.0402, 'grad_norm': 0.0920368954539299, 'learning_rate': 1.2509773260359655e-06, 'epoch': 9.376587844440103}\nEpoch: 9.380496384600352, Logs: {'loss': 0.0389, 'grad_norm': 0.11283016949892044, 'learning_rate': 1.243158717748241e-06, 'epoch': 9.380496384600352}\nEpoch: 9.384404924760602, Logs: {'loss': 0.0436, 'grad_norm': 0.26919469237327576, 'learning_rate': 1.2353401094605161e-06, 'epoch': 9.384404924760602}\nEpoch: 9.388313464920852, Logs: {'loss': 0.0395, 'grad_norm': 0.10924796015024185, 'learning_rate': 1.2275215011727914e-06, 'epoch': 9.388313464920852}\nEpoch: 9.392222005081102, Logs: {'loss': 0.0439, 'grad_norm': 0.09824846684932709, 'learning_rate': 1.2197028928850665e-06, 'epoch': 9.392222005081102}\nEpoch: 9.396130545241352, Logs: {'loss': 0.0452, 'grad_norm': 0.1971360743045807, 'learning_rate': 1.2118842845973418e-06, 'epoch': 9.396130545241352}\nEpoch: 9.400039085401602, Logs: {'loss': 0.0407, 'grad_norm': 0.14836986362934113, 'learning_rate': 1.204065676309617e-06, 'epoch': 9.400039085401602}\nEpoch: 9.403947625561853, Logs: {'loss': 0.0405, 'grad_norm': 0.14836518466472626, 'learning_rate': 1.1962470680218923e-06, 'epoch': 9.403947625561853}\nEpoch: 9.407856165722103, Logs: {'loss': 0.0375, 'grad_norm': 0.1212979108095169, 'learning_rate': 1.1884284597341674e-06, 'epoch': 9.407856165722103}\nEpoch: 9.411764705882353, Logs: {'loss': 0.0386, 'grad_norm': 0.18806616961956024, 'learning_rate': 1.1806098514464425e-06, 'epoch': 9.411764705882353}\nEpoch: 9.415673246042603, Logs: {'loss': 0.045, 'grad_norm': 0.17157059907913208, 'learning_rate': 1.1727912431587178e-06, 'epoch': 9.415673246042603}\nEpoch: 9.419581786202853, Logs: {'loss': 0.0426, 'grad_norm': 0.12504911422729492, 'learning_rate': 1.164972634870993e-06, 'epoch': 9.419581786202853}\nEpoch: 9.423490326363103, Logs: {'loss': 0.0379, 'grad_norm': 0.1908140480518341, 'learning_rate': 1.1571540265832684e-06, 'epoch': 9.423490326363103}\nEpoch: 9.427398866523353, Logs: {'loss': 0.0394, 'grad_norm': 0.1677761822938919, 'learning_rate': 1.1493354182955434e-06, 'epoch': 9.427398866523353}\nEpoch: 9.431307406683604, Logs: {'loss': 0.0442, 'grad_norm': 0.2067754715681076, 'learning_rate': 1.1415168100078187e-06, 'epoch': 9.431307406683604}\nEpoch: 9.435215946843854, Logs: {'loss': 0.0394, 'grad_norm': 0.24467192590236664, 'learning_rate': 1.133698201720094e-06, 'epoch': 9.435215946843854}\nEpoch: 9.439124487004104, Logs: {'loss': 0.0428, 'grad_norm': 0.6250377893447876, 'learning_rate': 1.1258795934323691e-06, 'epoch': 9.439124487004104}\nEpoch: 9.443033027164354, Logs: {'loss': 0.0488, 'grad_norm': 0.17980393767356873, 'learning_rate': 1.1180609851446444e-06, 'epoch': 9.443033027164354}\nEpoch: 9.446941567324604, Logs: {'loss': 0.0429, 'grad_norm': 0.17419302463531494, 'learning_rate': 1.1102423768569195e-06, 'epoch': 9.446941567324604}\nEpoch: 9.450850107484854, Logs: {'loss': 0.0418, 'grad_norm': 0.13853983581066132, 'learning_rate': 1.1024237685691948e-06, 'epoch': 9.450850107484854}\nEpoch: 9.454758647645104, Logs: {'loss': 0.0409, 'grad_norm': 0.09734422713518143, 'learning_rate': 1.09460516028147e-06, 'epoch': 9.454758647645104}\nEpoch: 9.458667187805355, Logs: {'loss': 0.04, 'grad_norm': 0.2866767346858978, 'learning_rate': 1.0867865519937453e-06, 'epoch': 9.458667187805355}\nEpoch: 9.462575727965605, Logs: {'loss': 0.0446, 'grad_norm': 0.15271873772144318, 'learning_rate': 1.0789679437060204e-06, 'epoch': 9.462575727965605}\nEpoch: 9.466484268125855, Logs: {'loss': 0.04, 'grad_norm': 0.29305383563041687, 'learning_rate': 1.0711493354182955e-06, 'epoch': 9.466484268125855}\nEpoch: 9.470392808286105, Logs: {'loss': 0.0446, 'grad_norm': 0.43036991357803345, 'learning_rate': 1.0633307271305708e-06, 'epoch': 9.470392808286105}\nEpoch: 9.474301348446355, Logs: {'loss': 0.0385, 'grad_norm': 0.5365086793899536, 'learning_rate': 1.055512118842846e-06, 'epoch': 9.474301348446355}\nEpoch: 9.478209888606605, Logs: {'loss': 0.0381, 'grad_norm': 0.2670759856700897, 'learning_rate': 1.0476935105551214e-06, 'epoch': 9.478209888606605}\nEpoch: 9.482118428766855, Logs: {'loss': 0.0441, 'grad_norm': 0.16935847699642181, 'learning_rate': 1.0398749022673964e-06, 'epoch': 9.482118428766855}\nEpoch: 9.486026968927106, Logs: {'loss': 0.041, 'grad_norm': 0.1977550983428955, 'learning_rate': 1.0320562939796717e-06, 'epoch': 9.486026968927106}\nEpoch: 9.489935509087356, Logs: {'loss': 0.0438, 'grad_norm': 0.25161853432655334, 'learning_rate': 1.024237685691947e-06, 'epoch': 9.489935509087356}\nEpoch: 9.493844049247606, Logs: {'loss': 0.0386, 'grad_norm': 0.18111173808574677, 'learning_rate': 1.016419077404222e-06, 'epoch': 9.493844049247606}\nEpoch: 9.497752589407856, Logs: {'loss': 0.045, 'grad_norm': 0.12548945844173431, 'learning_rate': 1.0086004691164974e-06, 'epoch': 9.497752589407856}\nEpoch: 9.501661129568106, Logs: {'loss': 0.0403, 'grad_norm': 0.2495199590921402, 'learning_rate': 1.0007818608287725e-06, 'epoch': 9.501661129568106}\nEpoch: 9.505569669728356, Logs: {'loss': 0.0395, 'grad_norm': 0.27451467514038086, 'learning_rate': 9.929632525410478e-07, 'epoch': 9.505569669728356}\nEpoch: 9.509478209888606, Logs: {'loss': 0.0385, 'grad_norm': 0.159363254904747, 'learning_rate': 9.85144644253323e-07, 'epoch': 9.509478209888606}\nEpoch: 9.513386750048857, Logs: {'loss': 0.043, 'grad_norm': 0.3118261396884918, 'learning_rate': 9.773260359655983e-07, 'epoch': 9.513386750048857}\nEpoch: 9.517295290209107, Logs: {'loss': 0.0385, 'grad_norm': 0.07591015100479126, 'learning_rate': 9.695074276778734e-07, 'epoch': 9.517295290209107}\nEpoch: 9.521203830369357, Logs: {'loss': 0.0452, 'grad_norm': 0.08579736202955246, 'learning_rate': 9.616888193901485e-07, 'epoch': 9.521203830369357}\nEpoch: 9.525112370529607, Logs: {'loss': 0.0359, 'grad_norm': 0.16658897697925568, 'learning_rate': 9.538702111024238e-07, 'epoch': 9.525112370529607}\nEpoch: 9.529020910689857, Logs: {'loss': 0.0405, 'grad_norm': 0.10870885848999023, 'learning_rate': 9.460516028146991e-07, 'epoch': 9.529020910689857}\nEpoch: 9.532929450850107, Logs: {'loss': 0.0434, 'grad_norm': 0.23825444281101227, 'learning_rate': 9.382329945269743e-07, 'epoch': 9.532929450850107}\nEpoch: 9.536837991010358, Logs: {'loss': 0.0425, 'grad_norm': 0.20577029883861542, 'learning_rate': 9.304143862392495e-07, 'epoch': 9.536837991010358}\nEpoch: 9.540746531170608, Logs: {'loss': 0.0398, 'grad_norm': 0.6419607996940613, 'learning_rate': 9.225957779515247e-07, 'epoch': 9.540746531170608}\nEpoch: 9.544655071330858, Logs: {'loss': 0.0412, 'grad_norm': 0.06251388788223267, 'learning_rate': 9.147771696638e-07, 'epoch': 9.544655071330858}\nEpoch: 9.548563611491108, Logs: {'loss': 0.0352, 'grad_norm': 0.16586831212043762, 'learning_rate': 9.069585613760751e-07, 'epoch': 9.548563611491108}\nEpoch: 9.552472151651358, Logs: {'loss': 0.0409, 'grad_norm': 0.09919344633817673, 'learning_rate': 8.991399530883503e-07, 'epoch': 9.552472151651358}\nEpoch: 9.556380691811608, Logs: {'loss': 0.0405, 'grad_norm': 0.17231997847557068, 'learning_rate': 8.913213448006256e-07, 'epoch': 9.556380691811608}\nEpoch: 9.560289231971858, Logs: {'loss': 0.0415, 'grad_norm': 0.22748616337776184, 'learning_rate': 8.835027365129008e-07, 'epoch': 9.560289231971858}\nEpoch: 9.56419777213211, Logs: {'loss': 0.0408, 'grad_norm': 0.08887802064418793, 'learning_rate': 8.75684128225176e-07, 'epoch': 9.56419777213211}\nEpoch: 9.56810631229236, Logs: {'loss': 0.0419, 'grad_norm': 0.15196870267391205, 'learning_rate': 8.678655199374512e-07, 'epoch': 9.56810631229236}\nEpoch: 9.57201485245261, Logs: {'loss': 0.0424, 'grad_norm': 0.08804190158843994, 'learning_rate': 8.600469116497265e-07, 'epoch': 9.57201485245261}\nEpoch: 9.575923392612859, Logs: {'loss': 0.0395, 'grad_norm': 0.2623791992664337, 'learning_rate': 8.522283033620016e-07, 'epoch': 9.575923392612859}\nEpoch: 9.579831932773109, Logs: {'loss': 0.0457, 'grad_norm': 0.1316479742527008, 'learning_rate': 8.444096950742768e-07, 'epoch': 9.579831932773109}\nEpoch: 9.583740472933359, Logs: {'loss': 0.04, 'grad_norm': 0.1451789289712906, 'learning_rate': 8.365910867865521e-07, 'epoch': 9.583740472933359}\nEpoch: 9.587649013093609, Logs: {'loss': 0.0477, 'grad_norm': 0.1402750313282013, 'learning_rate': 8.287724784988272e-07, 'epoch': 9.587649013093609}\nEpoch: 9.59155755325386, Logs: {'loss': 0.0464, 'grad_norm': 0.21018554270267487, 'learning_rate': 8.209538702111025e-07, 'epoch': 9.59155755325386}\nEpoch: 9.59546609341411, Logs: {'loss': 0.0504, 'grad_norm': 0.11690083891153336, 'learning_rate': 8.131352619233777e-07, 'epoch': 9.59546609341411}\nEpoch: 9.59937463357436, Logs: {'loss': 0.0406, 'grad_norm': 0.07612035423517227, 'learning_rate': 8.05316653635653e-07, 'epoch': 9.59937463357436}\nEpoch: 9.60328317373461, Logs: {'loss': 0.0431, 'grad_norm': 0.13719511032104492, 'learning_rate': 7.974980453479281e-07, 'epoch': 9.60328317373461}\nEpoch: 9.60719171389486, Logs: {'loss': 0.0424, 'grad_norm': 0.17521415650844574, 'learning_rate': 7.896794370602033e-07, 'epoch': 9.60719171389486}\nEpoch: 9.61110025405511, Logs: {'loss': 0.0414, 'grad_norm': 0.11958280205726624, 'learning_rate': 7.818608287724786e-07, 'epoch': 9.61110025405511}\nEpoch: 9.61500879421536, Logs: {'loss': 0.0426, 'grad_norm': 0.14531613886356354, 'learning_rate': 7.740422204847537e-07, 'epoch': 9.61500879421536}\nEpoch: 9.618917334375611, Logs: {'loss': 0.0393, 'grad_norm': 0.15217356383800507, 'learning_rate': 7.66223612197029e-07, 'epoch': 9.618917334375611}\nEpoch: 9.622825874535861, Logs: {'loss': 0.0464, 'grad_norm': 0.15538902580738068, 'learning_rate': 7.584050039093042e-07, 'epoch': 9.622825874535861}\nEpoch: 9.626734414696111, Logs: {'loss': 0.0391, 'grad_norm': 0.1618756800889969, 'learning_rate': 7.505863956215795e-07, 'epoch': 9.626734414696111}\nEpoch: 9.630642954856361, Logs: {'loss': 0.0387, 'grad_norm': 0.14279673993587494, 'learning_rate': 7.427677873338546e-07, 'epoch': 9.630642954856361}\nEpoch: 9.634551495016611, Logs: {'loss': 0.0382, 'grad_norm': 0.25711607933044434, 'learning_rate': 7.349491790461298e-07, 'epoch': 9.634551495016611}\nEpoch: 9.63846003517686, Logs: {'loss': 0.0441, 'grad_norm': 0.13440749049186707, 'learning_rate': 7.271305707584051e-07, 'epoch': 9.63846003517686}\nEpoch: 9.642368575337112, Logs: {'loss': 0.0406, 'grad_norm': 0.1758924126625061, 'learning_rate': 7.193119624706802e-07, 'epoch': 9.642368575337112}\nEpoch: 9.646277115497362, Logs: {'loss': 0.0372, 'grad_norm': 0.15926457941532135, 'learning_rate': 7.114933541829555e-07, 'epoch': 9.646277115497362}\nEpoch: 9.650185655657612, Logs: {'loss': 0.0447, 'grad_norm': 0.1631944477558136, 'learning_rate': 7.036747458952307e-07, 'epoch': 9.650185655657612}\nEpoch: 9.654094195817862, Logs: {'loss': 0.0394, 'grad_norm': 0.14329709112644196, 'learning_rate': 6.95856137607506e-07, 'epoch': 9.654094195817862}\nEpoch: 9.658002735978112, Logs: {'loss': 0.0425, 'grad_norm': 0.11578245460987091, 'learning_rate': 6.880375293197811e-07, 'epoch': 9.658002735978112}\nEpoch: 9.661911276138362, Logs: {'loss': 0.0421, 'grad_norm': 0.10555074363946915, 'learning_rate': 6.802189210320563e-07, 'epoch': 9.661911276138362}\nEpoch: 9.665819816298612, Logs: {'loss': 0.0399, 'grad_norm': 0.13393674790859222, 'learning_rate': 6.724003127443316e-07, 'epoch': 9.665819816298612}\nEpoch: 9.669728356458863, Logs: {'loss': 0.0403, 'grad_norm': 0.1453515589237213, 'learning_rate': 6.645817044566067e-07, 'epoch': 9.669728356458863}\nEpoch: 9.673636896619113, Logs: {'loss': 0.0379, 'grad_norm': 0.2663424611091614, 'learning_rate': 6.56763096168882e-07, 'epoch': 9.673636896619113}\nEpoch: 9.677545436779363, Logs: {'loss': 0.0401, 'grad_norm': 0.07560040056705475, 'learning_rate': 6.489444878811572e-07, 'epoch': 9.677545436779363}\nEpoch: 9.681453976939613, Logs: {'loss': 0.0457, 'grad_norm': 0.23915943503379822, 'learning_rate': 6.411258795934325e-07, 'epoch': 9.681453976939613}\nEpoch: 9.685362517099863, Logs: {'loss': 0.0409, 'grad_norm': 0.22616471350193024, 'learning_rate': 6.333072713057076e-07, 'epoch': 9.685362517099863}\nEpoch: 9.689271057260113, Logs: {'loss': 0.0388, 'grad_norm': 0.10244693607091904, 'learning_rate': 6.254886630179828e-07, 'epoch': 9.689271057260113}\nEpoch: 9.693179597420363, Logs: {'loss': 0.043, 'grad_norm': 0.2814302146434784, 'learning_rate': 6.176700547302581e-07, 'epoch': 9.693179597420363}\nEpoch: 9.697088137580614, Logs: {'loss': 0.0395, 'grad_norm': 0.20341669023036957, 'learning_rate': 6.098514464425332e-07, 'epoch': 9.697088137580614}\nEpoch: 9.700996677740864, Logs: {'loss': 0.0378, 'grad_norm': 0.17050093412399292, 'learning_rate': 6.020328381548085e-07, 'epoch': 9.700996677740864}\nEpoch: 9.704905217901114, Logs: {'loss': 0.0391, 'grad_norm': 0.10165442526340485, 'learning_rate': 5.942142298670837e-07, 'epoch': 9.704905217901114}\nEpoch: 9.708813758061364, Logs: {'loss': 0.0382, 'grad_norm': 0.28755083680152893, 'learning_rate': 5.863956215793589e-07, 'epoch': 9.708813758061364}\nEpoch: 9.712722298221614, Logs: {'loss': 0.0385, 'grad_norm': 0.1262805461883545, 'learning_rate': 5.785770132916342e-07, 'epoch': 9.712722298221614}\nEpoch: 9.716630838381864, Logs: {'loss': 0.0434, 'grad_norm': 0.16356831789016724, 'learning_rate': 5.707584050039094e-07, 'epoch': 9.716630838381864}\nEpoch: 9.720539378542114, Logs: {'loss': 0.0453, 'grad_norm': 0.2520122230052948, 'learning_rate': 5.629397967161846e-07, 'epoch': 9.720539378542114}\nEpoch: 9.724447918702365, Logs: {'loss': 0.0405, 'grad_norm': 0.19049644470214844, 'learning_rate': 5.551211884284597e-07, 'epoch': 9.724447918702365}\nEpoch: 9.728356458862615, Logs: {'loss': 0.0463, 'grad_norm': 0.1518944501876831, 'learning_rate': 5.47302580140735e-07, 'epoch': 9.728356458862615}\nEpoch: 9.732264999022865, Logs: {'loss': 0.042, 'grad_norm': 0.3724537789821625, 'learning_rate': 5.394839718530102e-07, 'epoch': 9.732264999022865}\nEpoch: 9.736173539183115, Logs: {'loss': 0.0405, 'grad_norm': 0.136206716299057, 'learning_rate': 5.316653635652854e-07, 'epoch': 9.736173539183115}\nEpoch: 9.740082079343365, Logs: {'loss': 0.0397, 'grad_norm': 0.25101083517074585, 'learning_rate': 5.238467552775607e-07, 'epoch': 9.740082079343365}\nEpoch: 9.743990619503615, Logs: {'loss': 0.0404, 'grad_norm': 0.28618094325065613, 'learning_rate': 5.160281469898359e-07, 'epoch': 9.743990619503615}\nEpoch: 9.747899159663866, Logs: {'loss': 0.0427, 'grad_norm': 0.17041409015655518, 'learning_rate': 5.08209538702111e-07, 'epoch': 9.747899159663866}\nEpoch: 9.751807699824116, Logs: {'loss': 0.04, 'grad_norm': 0.24779881536960602, 'learning_rate': 5.003909304143862e-07, 'epoch': 9.751807699824116}\nEpoch: 9.755716239984366, Logs: {'loss': 0.0411, 'grad_norm': 0.10922008752822876, 'learning_rate': 4.925723221266615e-07, 'epoch': 9.755716239984366}\nEpoch: 9.759624780144616, Logs: {'loss': 0.0411, 'grad_norm': 0.07263711094856262, 'learning_rate': 4.847537138389367e-07, 'epoch': 9.759624780144616}\nEpoch: 9.763533320304866, Logs: {'loss': 0.0441, 'grad_norm': 0.16799676418304443, 'learning_rate': 4.769351055512119e-07, 'epoch': 9.763533320304866}\nEpoch: 9.767441860465116, Logs: {'loss': 0.0432, 'grad_norm': 0.18665362894535065, 'learning_rate': 4.6911649726348713e-07, 'epoch': 9.767441860465116}\nEpoch: 9.771350400625366, Logs: {'loss': 0.0413, 'grad_norm': 0.26420751214027405, 'learning_rate': 4.6129788897576236e-07, 'epoch': 9.771350400625366}\nEpoch: 9.775258940785616, Logs: {'loss': 0.0397, 'grad_norm': 0.19459232687950134, 'learning_rate': 4.5347928068803755e-07, 'epoch': 9.775258940785616}\nEpoch: 9.779167480945867, Logs: {'loss': 0.0435, 'grad_norm': 0.520967960357666, 'learning_rate': 4.456606724003128e-07, 'epoch': 9.779167480945867}\nEpoch: 9.783076021106117, Logs: {'loss': 0.0374, 'grad_norm': 0.17139682173728943, 'learning_rate': 4.37842064112588e-07, 'epoch': 9.783076021106117}\nEpoch: 9.786984561266367, Logs: {'loss': 0.0431, 'grad_norm': 0.12851938605308533, 'learning_rate': 4.3002345582486326e-07, 'epoch': 9.786984561266367}\nEpoch: 9.790893101426617, Logs: {'loss': 0.0419, 'grad_norm': 0.20768314599990845, 'learning_rate': 4.222048475371384e-07, 'epoch': 9.790893101426617}\nEpoch: 9.794801641586867, Logs: {'loss': 0.0382, 'grad_norm': 0.5407365560531616, 'learning_rate': 4.143862392494136e-07, 'epoch': 9.794801641586867}\nEpoch: 9.798710181747117, Logs: {'loss': 0.0441, 'grad_norm': 0.14177298545837402, 'learning_rate': 4.0656763096168886e-07, 'epoch': 9.798710181747117}\nEpoch: 9.802618721907368, Logs: {'loss': 0.0403, 'grad_norm': 0.47483956813812256, 'learning_rate': 3.9874902267396405e-07, 'epoch': 9.802618721907368}\nEpoch: 9.806527262067618, Logs: {'loss': 0.0386, 'grad_norm': 0.07788128405809402, 'learning_rate': 3.909304143862393e-07, 'epoch': 9.806527262067618}\nEpoch: 9.810435802227868, Logs: {'loss': 0.0399, 'grad_norm': 0.09708744287490845, 'learning_rate': 3.831118060985145e-07, 'epoch': 9.810435802227868}\nEpoch: 9.814344342388118, Logs: {'loss': 0.0435, 'grad_norm': 0.12635010480880737, 'learning_rate': 3.7529319781078975e-07, 'epoch': 9.814344342388118}\nEpoch: 9.818252882548368, Logs: {'loss': 0.0399, 'grad_norm': 0.17227064073085785, 'learning_rate': 3.674745895230649e-07, 'epoch': 9.818252882548368}\nEpoch: 9.822161422708618, Logs: {'loss': 0.0388, 'grad_norm': 0.13914868235588074, 'learning_rate': 3.596559812353401e-07, 'epoch': 9.822161422708618}\nEpoch: 9.826069962868868, Logs: {'loss': 0.0375, 'grad_norm': 0.13578034937381744, 'learning_rate': 3.5183737294761536e-07, 'epoch': 9.826069962868868}\nEpoch: 9.82997850302912, Logs: {'loss': 0.0375, 'grad_norm': 0.16111208498477936, 'learning_rate': 3.4401876465989054e-07, 'epoch': 9.82997850302912}\nEpoch: 9.83388704318937, Logs: {'loss': 0.0417, 'grad_norm': 0.13283580541610718, 'learning_rate': 3.362001563721658e-07, 'epoch': 9.83388704318937}\nEpoch: 9.83779558334962, Logs: {'loss': 0.0445, 'grad_norm': 0.13984179496765137, 'learning_rate': 3.28381548084441e-07, 'epoch': 9.83779558334962}\nEpoch: 9.841704123509869, Logs: {'loss': 0.0328, 'grad_norm': 0.1356716752052307, 'learning_rate': 3.2056293979671625e-07, 'epoch': 9.841704123509869}\nEpoch: 9.845612663670119, Logs: {'loss': 0.0437, 'grad_norm': 0.11467831581830978, 'learning_rate': 3.127443315089914e-07, 'epoch': 9.845612663670119}\nEpoch: 9.849521203830369, Logs: {'loss': 0.0402, 'grad_norm': 0.16902558505535126, 'learning_rate': 3.049257232212666e-07, 'epoch': 9.849521203830369}\nEpoch: 9.85342974399062, Logs: {'loss': 0.0408, 'grad_norm': 0.10767202824354172, 'learning_rate': 2.9710711493354186e-07, 'epoch': 9.85342974399062}\nEpoch: 9.85733828415087, Logs: {'loss': 0.0402, 'grad_norm': 0.10816878080368042, 'learning_rate': 2.892885066458171e-07, 'epoch': 9.85733828415087}\nEpoch: 9.86124682431112, Logs: {'loss': 0.0437, 'grad_norm': 0.2646406292915344, 'learning_rate': 2.814698983580923e-07, 'epoch': 9.86124682431112}\nEpoch: 9.86515536447137, Logs: {'loss': 0.0429, 'grad_norm': 0.1027098298072815, 'learning_rate': 2.736512900703675e-07, 'epoch': 9.86515536447137}\nEpoch: 9.86906390463162, Logs: {'loss': 0.042, 'grad_norm': 0.10350853949785233, 'learning_rate': 2.658326817826427e-07, 'epoch': 9.86906390463162}\nEpoch: 9.87297244479187, Logs: {'loss': 0.0429, 'grad_norm': 0.23419810831546783, 'learning_rate': 2.5801407349491793e-07, 'epoch': 9.87297244479187}\nEpoch: 9.87688098495212, Logs: {'loss': 0.0419, 'grad_norm': 0.23271948099136353, 'learning_rate': 2.501954652071931e-07, 'epoch': 9.87688098495212}\nEpoch: 9.88078952511237, Logs: {'loss': 0.0359, 'grad_norm': 0.15497946739196777, 'learning_rate': 2.4237685691946835e-07, 'epoch': 9.88078952511237}\nEpoch: 9.884698065272621, Logs: {'loss': 0.0449, 'grad_norm': 0.2697853744029999, 'learning_rate': 2.3455824863174356e-07, 'epoch': 9.884698065272621}\nEpoch: 9.888606605432871, Logs: {'loss': 0.0379, 'grad_norm': 0.18898820877075195, 'learning_rate': 2.2673964034401877e-07, 'epoch': 9.888606605432871}\nEpoch: 9.892515145593121, Logs: {'loss': 0.0409, 'grad_norm': 0.29630693793296814, 'learning_rate': 2.18921032056294e-07, 'epoch': 9.892515145593121}\nEpoch: 9.896423685753371, Logs: {'loss': 0.046, 'grad_norm': 0.14533931016921997, 'learning_rate': 2.111024237685692e-07, 'epoch': 9.896423685753371}\nEpoch: 9.90033222591362, Logs: {'loss': 0.0397, 'grad_norm': 0.23843128979206085, 'learning_rate': 2.0328381548084443e-07, 'epoch': 9.90033222591362}\nEpoch: 9.90424076607387, Logs: {'loss': 0.0367, 'grad_norm': 0.23561827838420868, 'learning_rate': 1.9546520719311964e-07, 'epoch': 9.90424076607387}\nEpoch: 9.908149306234122, Logs: {'loss': 0.0414, 'grad_norm': 0.14829812943935394, 'learning_rate': 1.8764659890539488e-07, 'epoch': 9.908149306234122}\nEpoch: 9.912057846394372, Logs: {'loss': 0.0445, 'grad_norm': 0.10153935104608536, 'learning_rate': 1.7982799061767006e-07, 'epoch': 9.912057846394372}\nEpoch: 9.915966386554622, Logs: {'loss': 0.0404, 'grad_norm': 0.10148664563894272, 'learning_rate': 1.7200938232994527e-07, 'epoch': 9.915966386554622}\nEpoch: 9.919874926714872, Logs: {'loss': 0.0404, 'grad_norm': 0.4012276828289032, 'learning_rate': 1.641907740422205e-07, 'epoch': 9.919874926714872}\nEpoch: 9.923783466875122, Logs: {'loss': 0.041, 'grad_norm': 0.13443806767463684, 'learning_rate': 1.563721657544957e-07, 'epoch': 9.923783466875122}\nEpoch: 9.927692007035372, Logs: {'loss': 0.0403, 'grad_norm': 0.12936778366565704, 'learning_rate': 1.4855355746677093e-07, 'epoch': 9.927692007035372}\nEpoch: 9.931600547195622, Logs: {'loss': 0.0391, 'grad_norm': 0.14448390901088715, 'learning_rate': 1.4073494917904614e-07, 'epoch': 9.931600547195622}\nEpoch: 9.935509087355873, Logs: {'loss': 0.0416, 'grad_norm': 0.2606179118156433, 'learning_rate': 1.3291634089132135e-07, 'epoch': 9.935509087355873}\nEpoch: 9.939417627516123, Logs: {'loss': 0.0403, 'grad_norm': 0.12436982989311218, 'learning_rate': 1.2509773260359656e-07, 'epoch': 9.939417627516123}\nEpoch: 9.943326167676373, Logs: {'loss': 0.0423, 'grad_norm': 0.17228654026985168, 'learning_rate': 1.1727912431587178e-07, 'epoch': 9.943326167676373}\nEpoch: 9.947234707836623, Logs: {'loss': 0.0446, 'grad_norm': 0.09781907498836517, 'learning_rate': 1.09460516028147e-07, 'epoch': 9.947234707836623}\nEpoch: 9.951143247996873, Logs: {'loss': 0.0448, 'grad_norm': 0.16397424042224884, 'learning_rate': 1.0164190774042222e-07, 'epoch': 9.951143247996873}\nEpoch: 9.955051788157123, Logs: {'loss': 0.0392, 'grad_norm': 0.6582856178283691, 'learning_rate': 9.382329945269744e-08, 'epoch': 9.955051788157123}\nEpoch: 9.958960328317373, Logs: {'loss': 0.042, 'grad_norm': 0.20232710242271423, 'learning_rate': 8.600469116497264e-08, 'epoch': 9.958960328317373}\nEpoch: 9.962868868477624, Logs: {'loss': 0.0445, 'grad_norm': 0.17242959141731262, 'learning_rate': 7.818608287724785e-08, 'epoch': 9.962868868477624}\nEpoch: 9.966777408637874, Logs: {'loss': 0.0407, 'grad_norm': 0.24202881753444672, 'learning_rate': 7.036747458952307e-08, 'epoch': 9.966777408637874}\nEpoch: 9.970685948798124, Logs: {'loss': 0.039, 'grad_norm': 0.09512704610824585, 'learning_rate': 6.254886630179828e-08, 'epoch': 9.970685948798124}\nEpoch: 9.974594488958374, Logs: {'loss': 0.0417, 'grad_norm': 0.14246313273906708, 'learning_rate': 5.47302580140735e-08, 'epoch': 9.974594488958374}\nEpoch: 9.978503029118624, Logs: {'loss': 0.0396, 'grad_norm': 0.10675736516714096, 'learning_rate': 4.691164972634872e-08, 'epoch': 9.978503029118624}\nEpoch: 9.982411569278874, Logs: {'loss': 0.0428, 'grad_norm': 0.17464753985404968, 'learning_rate': 3.909304143862392e-08, 'epoch': 9.982411569278874}\nEpoch: 9.986320109439124, Logs: {'loss': 0.0405, 'grad_norm': 0.13464424014091492, 'learning_rate': 3.127443315089914e-08, 'epoch': 9.986320109439124}\nEpoch: 9.990228649599375, Logs: {'loss': 0.0414, 'grad_norm': 0.20055848360061646, 'learning_rate': 2.345582486317436e-08, 'epoch': 9.990228649599375}\nEpoch: 9.994137189759625, Logs: {'loss': 0.0433, 'grad_norm': 0.23767907917499542, 'learning_rate': 1.563721657544957e-08, 'epoch': 9.994137189759625}\nEpoch: 9.998045729919875, Logs: {'loss': 0.0383, 'grad_norm': 0.1914224475622177, 'learning_rate': 7.818608287724785e-09, 'epoch': 9.998045729919875}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9.998045729919875, Logs: {'eval_loss': 0.04147132486104965, 'eval_bleu': 0.0, 'eval_accuracy': 0.9457584050039093, 'eval_runtime': 354.7961, 'eval_samples_per_second': 57.678, 'eval_steps_per_second': 3.605, 'epoch': 9.998045729919875}\nEpoch: 9.998045729919875, Logs: {'train_runtime': 8479.2766, 'train_samples_per_second': 96.539, 'train_steps_per_second': 3.017, 'total_flos': 1.3846041493241856e+16, 'train_loss': 0.06780480503150502, 'epoch': 9.998045729919875}\nTraining history saved to ./results_english_hindi/training_history.json\nModel and tokenizer saved to './results_english_hindi/'\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1279' max='1279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1279/1279 05:44]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Epoch: 9.998045729919875, Logs: {'eval_loss': 0.04147132486104965, 'eval_bleu': 0.0, 'eval_accuracy': 0.9457584050039093, 'eval_runtime': 346.4635, 'eval_samples_per_second': 59.065, 'eval_steps_per_second': 3.692, 'epoch': 9.998045729919875}\nEvaluation Results: {'eval_loss': 0.04147132486104965, 'eval_bleu': 0.0, 'eval_accuracy': 0.9457584050039093, 'eval_runtime': 346.4635, 'eval_samples_per_second': 59.065, 'eval_steps_per_second': 3.692, 'epoch': 9.998045729919875}\nEvaluation results saved to ./results_english_hindi/evaluation_results.json\nEnglish: This is a test sentence for translation.\nHindi:    \n","output_type":"stream"}],"execution_count":1}]}